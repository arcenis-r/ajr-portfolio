[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Green, D., Melzer, B. T., Parker, J. A., & Rojas, A. (2020). Accelerator or Brake? Cash for Clunkers, Household Liquidity, and Aggregate Demand. American Economic Journal: Economic Policy, 12(4), 178–211. https://www.aeaweb.org/articles?id=10.1257/pol.20170122\nHubener, E., Rojas, A., & Tseng, N. (2018). Tradeoffs in the expenditure patterns of families with children. Beyond the Numbers: Prices & Spending (U.S. Bureau of Labor Statistics), 7(11). https://www.bls.gov/opub/btn/volume-7/tradeoffs-in-the-expenditure-patterns-of-families-with-children.htm\nFoster, A.C., Rojas, A. (2018). Program participation and spending patterns of families receiving government means-tested assistance. Monthly Labor Review (U.S. Bureau of Labor Statistics). https://doi.org/10.21916/mlr.2018.3"
  },
  {
    "objectID": "publications.html#papers",
    "href": "publications.html#papers",
    "title": "Publications",
    "section": "",
    "text": "Green, D., Melzer, B. T., Parker, J. A., & Rojas, A. (2020). Accelerator or Brake? Cash for Clunkers, Household Liquidity, and Aggregate Demand. American Economic Journal: Economic Policy, 12(4), 178–211. https://www.aeaweb.org/articles?id=10.1257/pol.20170122\nHubener, E., Rojas, A., & Tseng, N. (2018). Tradeoffs in the expenditure patterns of families with children. Beyond the Numbers: Prices & Spending (U.S. Bureau of Labor Statistics), 7(11). https://www.bls.gov/opub/btn/volume-7/tradeoffs-in-the-expenditure-patterns-of-families-with-children.htm\nFoster, A.C., Rojas, A. (2018). Program participation and spending patterns of families receiving government means-tested assistance. Monthly Labor Review (U.S. Bureau of Labor Statistics). https://doi.org/10.21916/mlr.2018.3"
  },
  {
    "objectID": "publications.html#presentations",
    "href": "publications.html#presentations",
    "title": "Publications",
    "section": "Presentations",
    "text": "Presentations\nPaulin, G., Rojas, A., Taylor, W. (2018). Expenditures with Policy Implications: Results from the Consumer Expenditure Surveys on Housing, Alcohol, Tobacco, and Gambling. American Council on Consumer Interests. https://www.consumerinterests.org/assets/docs/CIA/CIA2018/Paulin%281%29CIA2018.pdf\nTan, L., Rojas, A. (2019). Exploring the Application of Machine Learning Techniques to Construct R-indicators. American Association for Public Opinion Research Conference. https://www.bls.gov/cex/research_papers/pdf/rojas-r-indicators-aapor-presentation.pdf\nGillman, D., Hubener, E., Noel, R., Rigg, B., Rojas, A., Tan, L., Wilson, T. (2017). Documenting the Consumer Expenditure Surveys with DDI and Colectica – An Update. North American Data Documentation Initiative. https://doi.org/10.5281/zenodo.556223\nGillman, D., Hubener, E., Noel, R., Rigg, B., Rojas, A., Tan, L., Wilson, T. (2017). A Complex Use Case - Documenting the Consumer Expenditure Survey at BLS. North American Data Documentation Initiative. https://doi.org/10.5281/zenodo.556223"
  },
  {
    "objectID": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html",
    "href": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html",
    "title": "Forecasting with {modeltime} - Part II",
    "section": "",
    "text": "Last time on Forecasting with {modeltime} - Part I I wrote about gathering the data for this project from various sources. Here I’ll go into the first part of the analysis which will include visualizing the dependent variable, checking and correcting for non-stationarity, identifying seasonality, and identifying the terms (AR, MA) to use in a S-/ARIMA model.\nBesides the usual tidyverse (Wickham et al. 2019), in this post I’ll be using the timetk (Dancho and Vaughan 2023) package – also from the folks at Business Science – for performing my data analysis.\nCode\nlibrary(tidyverse)\nlibrary(timetk)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")"
  },
  {
    "objectID": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#a-quick-look-at-the-case-shiller-hpi",
    "href": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#a-quick-look-at-the-case-shiller-hpi",
    "title": "Forecasting with {modeltime} - Part II",
    "section": "A quick look at the Case-Shiller HPI",
    "text": "A quick look at the Case-Shiller HPI\nIn this post I’ll only be looking at the Case-Shiller Home Price Index (HPI) for the four cities in my data – San Diego, Dallas, Detroit, and New York City – between January 2006 and December 2022. Here’s a quick peek at the data:\n\n\nCode\ngt_bold_head(head(econ_data, 10))\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\n\n\n\n\nDallas\n2006-01-01\n121.9108\n\n\nDallas\n2006-02-01\n121.3285\n\n\nDallas\n2006-03-01\n121.5217\n\n\nDallas\n2006-04-01\n122.3996\n\n\nDallas\n2006-05-01\n123.2863\n\n\nDallas\n2006-06-01\n124.4985\n\n\nDallas\n2006-07-01\n125.3672\n\n\nDallas\n2006-08-01\n125.7007\n\n\nDallas\n2006-09-01\n125.1859\n\n\nDallas\n2006-10-01\n124.5813\n\n\n\n\n\n\n\nNow I’ll look visualize the data in a line plot.\n\n\nCode\necon_data |&gt;\n  ggplot(aes(date, hpi, color = city)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = \"HPI\",\n    color = NULL,\n    title = \"Case-Shiller HPI from 2006 through 2022\"\n  ) +\n  theme_timetk +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nHere one can see that Dallas’s HPI did not drop as much during the Great Recession and has had a fast rise relative to those of the other cities ever since. It’s also pretty clear that NYC’s HPI has experienced a relatively mild increase over the years covered. There’s also more volatility between 2010 and 2015 in the HPIs for NYC, Dallas, and Detroit than in San Diego’s."
  },
  {
    "objectID": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#time-series-analysis",
    "href": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#time-series-analysis",
    "title": "Forecasting with {modeltime} - Part II",
    "section": "Time series analysis",
    "text": "Time series analysis\nThe goal of this initial analysis is to find the parameters to use in an ARIMA model, which means that I’ll need to ensure my data are stationary. I’ll try to get there through the following operations and analyses.\n\nPreparing the data\nBefore performing any analysis I’d like to add one column with a lagged first-difference and another with a lagged twelfth-difference. To accomplish this I’ll use the tk_augment_differences() from the timetk package. I’ll store this data set in a new variable called “hpi_series”. I chose these two lagged differences because the lagged first-difference generally any stationarity rooted in a trend and the lagged twelfth-difference should deal with seasonality as my data are monthly observations and I’m operating on the assumption that the housing market cycle is annual.\nBelow is a snippet of the data.\n\n\nCode\nhpi_series &lt;- econ_data |&gt;\n  select(city, date, hpi) |&gt;\n  group_by(city) |&gt;\n  tk_augment_differences(\n    .value = hpi,\n    .lags = 1,\n    .differences = c(1, 12),\n    .log = FALSE\n  ) |&gt;\n  ungroup()\n\nhead(hpi_series, 15) |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\nhpi_lag1_diff1\nhpi_lag1_diff12\n\n\n\n\nDallas\n2006-01-01\n121.9108\nNA\nNA\n\n\nDallas\n2006-02-01\n121.3285\n-0.58234\nNA\n\n\nDallas\n2006-03-01\n121.5217\n0.19324\nNA\n\n\nDallas\n2006-04-01\n122.3996\n0.87788\nNA\n\n\nDallas\n2006-05-01\n123.2863\n0.88666\nNA\n\n\nDallas\n2006-06-01\n124.4985\n1.21228\nNA\n\n\nDallas\n2006-07-01\n125.3672\n0.86869\nNA\n\n\nDallas\n2006-08-01\n125.7007\n0.33342\nNA\n\n\nDallas\n2006-09-01\n125.1859\n-0.51474\nNA\n\n\nDallas\n2006-10-01\n124.5813\n-0.60463\nNA\n\n\nDallas\n2006-11-01\n123.8517\n-0.72958\nNA\n\n\nDallas\n2006-12-01\n123.6697\n-0.18205\nNA\n\n\nDallas\n2007-01-01\n122.6361\n-1.03360\n-161.2924\n\n\nDallas\n2007-02-01\n122.7292\n0.09314\n188.2084\n\n\nDallas\n2007-03-01\n123.1562\n0.42699\n-251.1343\n\n\n\n\n\n\n\nAnd now I can look at line plots of the three different series with the original on the left ; the lagged first-difference in the middle; and the lagged twelfth-difference on the right with a the cities in the different rows. From left to right the plots go from being highly regular (seasonal and trending) to being less regular in the middle (only seasonal) and highly irregular on the right. That irregularity is indicative of stationarity, which is what I want to see. This also provides a sneak peak into what the following analyses will give us.\n\n\nCode\nhpi_series |&gt;\n  pivot_longer(starts_with(\"hpi\"), names_to = \"series\", values_to = \"value\") |&gt;\n  group_by(city, series) |&gt;\n  plot_time_series(\n    date,\n    value,\n    .facet_ncol = 3,\n    .title = \"Case-Shiller HPI from 2006 through 2022\",\n    .interactive = FALSE\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nOn might ask “why not use a lagged, log difference?” The below plot shows that using a lagged, log first- and twelfth-differenced series don’t change anything compared to the lagged first- and twelfth-differenced series except for generating data with much higher relative amplitude and possibly showing outliers. This might actually introduce heteroskedasticity, which I’d like to avoid.\n\n\nCode\necon_data |&gt;\n  tk_augment_differences(\n    .value = hpi,\n    .lags = 1,\n    .differences = c(1, 12),\n    .log = TRUE\n  ) |&gt;\n  pivot_longer(starts_with(\"hpi\"), names_to = \"series\", values_to = \"value\") |&gt;\n  group_by(city, series) |&gt;\n  plot_time_series(\n    date,\n    value,\n    .facet_ncol = 3,\n    .title = \"Case-Shiller HPI from 2006 through 2022\",\n    .interactive = FALSE\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nTime series decomposition\nThe following plot shows the Seasonal-Trend decomposition using Loess (STL), which is a common tool to determine how much of a time series’ movement is due to a trend versus that which is due to it’s seasonality. Again, here I’ll assume an annual seasonal cycle.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_stl_diagnostics(\n    date,\n    hpi,\n    .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),\n    .interactive = FALSE,\n    .frequency = 12\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nAccording to this plot most of the movement in the HPI is determined by the trend and very little by seasonality.\n\n\nAugmented Dickey-Fuller Test\nThe Augmented Dickey-Fuller Test (ADF) is commonly run to evaluate whether a time series is stationary or not. The null hypothesis of the ADF is that the time series is non-stationary, so want to reject the null hypothesis to be able to assume the alternative, i.e., that the series is stationary. The following table shows that one can reject the null hypothesis for every city but San Diego with just a lagged first-difference, but with the lagged twelfth-difference all four series can be thought of as stationary.\n\n\nCode\nhpi_adf &lt;- hpi_series |&gt;\n  pivot_longer(!c(city, date), names_to = \"variable\") |&gt;\n  group_by(city, variable) |&gt;\n  nest(data = c(date, value)) |&gt;\n  mutate(\n    adf_test = map(\n      data,\n      \\(x) drop_na(x, value) |&gt;\n        pull(value) |&gt;\n        tseries::adf.test() |&gt;\n        broom::tidy() |&gt;\n        select(statistic, p.value)\n    )\n  ) |&gt;\n  select(-data) |&gt;\n  unnest(adf_test) |&gt;\n  ungroup(variable)\n\nhpi_adf |&gt;\n  group_by(city) |&gt;\n  group_nest() |&gt;\n  mutate(data_gt = map(data, make_adf_gt)) |&gt;\n  select(!data) |&gt;\n  pivot_wider(names_from = city, values_from = data_gt) |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDallas\nDetroit\nNew York\nSan Diego\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−0.688\n0.970\n\n\nhpi_lag1_diff1\n−3.985\n0.011\n\n\nhpi_lag1_diff12\n−21.100\n0.010\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−2.927\n0.188\n\n\nhpi_lag1_diff1\n−6.023\n0.010\n\n\nhpi_lag1_diff12\n−21.563\n0.010\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−0.256\n0.990\n\n\nhpi_lag1_diff1\n−6.495\n0.010\n\n\nhpi_lag1_diff12\n−24.809\n0.010\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−2.368\n0.422\n\n\nhpi_lag1_diff1\n−2.982\n0.165\n\n\nhpi_lag1_diff12\n−27.132\n0.010\n\n\n\n\n\n\n\n\n\n\n\n\n\nACF / PACF plots\nThe Autocorrelation Function (ACF) calculates the correlation of a value, e.g., the most recent value of the HPI, with previous values in the same series. Knowing this is very useful for determining how many Moving Average (the “MA” in ARIMA) lags to use in an ARIMA mode. The Partial Autocorrelation Function (PACF) calculates the correlation of a value with previous values in the same series after accounting for intervening correlations. This is useful to help us determine how many lags to use for the Autoregressive (the “AR” in ARIMA) part of an ARIMA model. Below are the ACF and PACF plots for the HPI series (no lagged differences) for all four cities. The timetk::plot_acf_diagnostics() function plots ACF on top and PACF on bottom, which is conventional. Values outside of the red, dashed lines indicate statistically significant correlations.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_acf_diagnostics(\n    date,\n    hpi,\n    .lags = 60,\n    .title = \"HPI Lag Diagnostics\",\n    .interactive = FALSE,\n    .white_noise_line_color = \"red\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nIt’s clear from the ACF plot that the trend is causing a lot of the autocorrelation in each city. Next is the same plot, but for the first-differenced values.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_acf_diagnostics(\n    date,\n    hpi_lag1_diff1,\n    .lags = 60,\n    .title = \"First-Differenced HPI Lag Diagnostics\",\n    .interactive = FALSE,\n    .white_noise_line_color = \"red\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThe trend’s influence is significantly decreased here, but there are still some strong seasonal patterns in the data. Below is a look at the twelfth-differenced series.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_acf_diagnostics(\n    date,\n    hpi_lag1_diff12,\n    .lags = 60,\n    .title = \"Twelfth-Differenced HPI Lag Diagnostics\",\n    .interactive = FALSE,\n    .white_noise_line_color = \"red\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nLooking at this plot over 36 months it still appears that there is a small seasonal effect in Dallas, San Diego, and Detroit with Detroit’s possibly being a shorter season. Considering the infinite number of things that could affect one of these data series one could look at hundreds of plots like this with different combinations of lags and differences for any one series and never find a perfect one. This plot is pretty good. From this plot it looks like I’m going to us an AR(3) model. San Diego’s PACF suggests that I should use two for that city, but I’m looking for one value for all four cities. The MA component is a little more challenging. It looks like I would use MA lags of 5 or 6 for Dallas, 8 for San Diego, 4 for NYC, and 4 or 5 for Detroit. I’m going to use MA(5) for my ARIMA.\n\n\nSeasonality\nIt’s been pretty clear up to this point that seasonality is an important consideration with this data series and it’s something that I will expect to handle in the modeling portion of the analysis in a later post. If I wanted to handle seasonality in a conventional model I might run a SARIMA model, but I’m going to keep this part simple and only run ARIMA knowing that I’m partially ignoring the seasonality and only dealing with it by differencing. With that said, I’ll show two more plots here to demonstrate the timetk::plot_seasonal_diagnostics() . The first plot shows seasonal diagnostics for the unadjusted series and the second one for the twelfth-differenced series.\n\n\nCode\nhpi_series |&gt;\n  plot_seasonal_diagnostics(\n    date,\n    hpi,\n    .facet_vars = city,\n    .interactive = FALSE,\n    .title = \"HPI Seasonal Diagnostics\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows a clear pattern of HPI values being lowest in the first quarter across all four cities, highest in the second and third quarters for San Diego, and highest in the quarter for New York and December. From this data it seems that the peak in Dallas is December. This seasonality can obscure the findings of an ARIMA model.\n\n\nCode\nhpi_series |&gt;\n  plot_seasonal_diagnostics(\n    date,\n    hpi_lag1_diff12,\n    .facet_vars = city,\n    .interactive = FALSE,\n    .title = \"Twelfth-Differenced HPI Seasonal Diagnostics\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nIn this plot the patterns are generally gone across the quarterly plots. Even the trend is mostly gone from the annual plot.\nIn the next post I’ll use the AR and MA values from this post to run ARIMA models for all four cities using a global modeling process and an iterative modeling process with modeltime."
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html",
    "href": "posts/20240125-topic-modeling/topic-modeling.html",
    "title": "Topic Modeling with R",
    "section": "",
    "text": "For this project I’ll be following an example of performing topic modeling with R using the Tidytext format introduced by Julia Silge and David Robinson in their “Text Mining with R! A Tidy Approach”. And I’ll be using abstracts submitted by Johns Hopkins University for funding to the NIH HEAL Initiative. The list of funded projects can be found at https://heal.nih.gov/funding/awarded. A benefit of using this data is that it doesn’t require much cleaning as Twitter text might. For the purposes of this demonstration I won’t do any data cleaning. The objective will be to find the optimal number of topics to which to attribute the abstracts. I’ll be using the tidyverse and tidytext packages for wrangling, the stm package for modeling, the janitor package for cleaning column names (but I won’t load it because I’m only going to use it once), the knitr package for table formatting, and the pacman package to load my packages (also only used once).\n\n\nCode\npacman::p_load(tidyverse, tidytext, stm, knitr, gt)"
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#introduction",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#introduction",
    "title": "Topic Modeling with R",
    "section": "",
    "text": "For this project I’ll be following an example of performing topic modeling with R using the Tidytext format introduced by Julia Silge and David Robinson in their “Text Mining with R! A Tidy Approach”. And I’ll be using abstracts submitted by Johns Hopkins University for funding to the NIH HEAL Initiative. The list of funded projects can be found at https://heal.nih.gov/funding/awarded. A benefit of using this data is that it doesn’t require much cleaning as Twitter text might. For the purposes of this demonstration I won’t do any data cleaning. The objective will be to find the optimal number of topics to which to attribute the abstracts. I’ll be using the tidyverse and tidytext packages for wrangling, the stm package for modeling, the janitor package for cleaning column names (but I won’t load it because I’m only going to use it once), the knitr package for table formatting, and the pacman package to load my packages (also only used once).\n\n\nCode\npacman::p_load(tidyverse, tidytext, stm, knitr, gt)"
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#downloading-the-abstract-data",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#downloading-the-abstract-data",
    "title": "Topic Modeling with R",
    "section": "Downloading the abstract data",
    "text": "Downloading the abstract data\nI’ll first download the data from the NIH’s Funded Projects page and filter for John’s Hopkins. The code for this is below, though, I will not run it because the data change as proposals are submitted. Instead I’ll download the filtered data to my hard drive and the ensuing analysis will be based on the data available as of this writing (January 25, 2024).\n\n\nCode\n# Not run\nread_csv(\n  \"https://heal.nih.gov/funding/awarded/export?combine=johns%20hopkins&_format=csv\"\n) |&gt;\n  write_csv(file.path(data_dir, \"jh-data.csv\"))\n\n\nNow I’ll read that data in and just look at the column names to start familiarizing myself with the data\n\n\nCode\njh_data &lt;- read_csv(file.path(data_dir, \"jh-data.csv\"))\nglimpse(jh_data)\n\n\nRows: 30\nColumns: 10\n$ `Project #`           &lt;chr&gt; \"1R01DA059473-01\", \"1RF1NS134549-01\", \"5U54DA049…\n$ `Project Title`       &lt;chr&gt; \"Sleep and Circadian Rhythm Phenotypes and Mecha…\n$ `Research Focus Area` &lt;chr&gt; \"New Strategies to Prevent and Treat Opioid Addi…\n$ `Research Program`    &lt;chr&gt; \"Sleep Dysfunction as a Core Feature of Opioid U…\n$ `Administering IC(s)` &lt;chr&gt; \"NIDA\", \"NINDS\", \"NIDA\", \"NIAMS\", \"NIDA\", \"NIDA\"…\n$ `Institution(s)`      &lt;chr&gt; \"JOHNS HOPKINS UNIVERSITY\", \"JOHNS HOPKINS UNIVE…\n$ `Investigator(s)`     &lt;chr&gt; \"HUHN, ANDREW S (contact); RABINOWITZ, JILL ALEX…\n$ `Location(s)`         &lt;chr&gt; \"Baltimore, MD\", \"Baltimore, MD\", \"Baltimore, MD…\n$ `Year Awarded`        &lt;dbl&gt; 2023, 2023, 2023, 2023, 2022, 2022, 2022, 2022, …\n$ Summary               &lt;chr&gt; \"Chronic opioid use has well known effects on sl…\n\n\nIt looks like there are 30 columns in the data. I’m going to clean up the column names and select only the Project # (to use as an identifier), Project Title, and Summary.\n\n\nCode\njh_clean &lt;- jh_data |&gt;\n  janitor::clean_names() |&gt;\n  select(project_number, project_title, summary)\n\nhead(jh_clean, 3) |&gt; gt::gt() # kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\nproject_number\nproject_title\nsummary\n\n\n\n\n1R01DA059473-01\nSleep and Circadian Rhythm Phenotypes and Mechanisms Associated With Opioid Use Disorder Treatment Outcomes\nChronic opioid use has well known effects on sleep quality, including disordered breathing during sleep and other abnormalities related to circadian rhythms. However, little is known about the relationship between sleep-related symptoms and non-medical opioid use among individuals being treated for opioid use disorder. This longitudinal study aims to identify biological pathways that may account for these associations. The research will first determine associations of sleep and proxy measures of circadian rhythms with non-medical opioid use. Second, they will investigate emotional processes associated with sleep/circadian symptoms and opioid treatment outcomes.\n\n\n1RF1NS134549-01\nValidation of a New Large-Pore Channel as a Novel Target for Neuropathic Pain\nActivation of immune cells (microglia) in the central nervous system and neuroinflammation have emerged as key drivers of neuropathic pain. These processes can be triggered by release of ATP, the compound that provides energy to many biochemical reactions. The source and mechanism of ATP release are poorly understood but could be targets of novel treatment approaches for neuropathic pain. This project will use genetic, pharmacological, and electrophysiological approaches to determine whether a large pore channel called Swell 1 that spans the cell membrane is the source of ATP release and resulting neuropathic pain and thus could be a treatment target.\n\n\n5U54DA049110-04\nData Center for Acute to Chronic Pain Biosignatures\nUnderstanding the mechanisms underlying the transition to chronic pain is key to mitigating the dual epidemics of chronic pain and opioid use in the United States. As part of the National Institutes of Health-funded Acute to Chronic Pain Signatures Program, the Data Integration and Resource Center aims to This project will support a post-doctoral trainee to develop the skills and knowledge needed to pursue a successful career in clinical pain research. The research will involve integrating imaging, physiology, -omics, behavioral, and clinical data to develop biosignatures for the transition from acute to chronic pain, toward understanding how the nervous and immune systems affect post-surgical pain and opioid use."
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#tidy-the-data",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#tidy-the-data",
    "title": "Topic Modeling with R",
    "section": "Tidy the data",
    "text": "Tidy the data\nIn this next step I’m going to tidy the data and remove stop words. Then I’ll take a look at word frequencies and TF-IDF metrics to get an idea of what I might expect from my model.\nI’m first going to use the tidytext::unnest_tokens() function to tokenize the data by single word then remove stop words from the text. I’ll also get word frequencies in this one step. Also, I’m going to drop the project title for the time being, but I’ll bring it back later.\n\n\nCode\njh_tidy &lt;- jh_clean |&gt;\n  mutate(summary = str_c(project_title, summary, sep = \" \")) |&gt;\n  select(-project_title) |&gt;\n  unnest_tokens(word, summary, token = \"words\") |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(project_number, word)\n\njh_tidy |&gt; arrange(-n) |&gt; head(10) |&gt; gt()\n\n\n\n\n\n\n\n\nproject_number\nword\nn\n\n\n\n\n1RF1NS113883-01\npain\n9\n\n\n3R01MD009063-05S1\npain\n9\n\n\n1UG3NS115718-01\nmrgprx1\n8\n\n\n1U01HL150568-01\nsleep\n7\n\n\n1U01HL150835-01\nsleep\n7\n\n\n5U54DA049110-04\npain\n7\n\n\n1R01DA059473-01\nopioid\n6\n\n\n1R01DA059473-01\nsleep\n6\n\n\n1R61AT012279-01\nshoulder\n6\n\n\n1U01HL150835-01\nstress\n6\n\n\n\n\n\n\n\nThe words “pain” is one of the most frequently occurring word in documents 1RF1NS113883-01 and 5U54DA049110-04 while the words “pain” and “sleep” seem to be 2 of the most frequent across documents. Let’s see how these words do in terms of how important they are in distinguishing a document from other documents.\nI’m going to add TF-IDF metrics using the tidytext::bind_tf_idf() function.\n\n\nCode\njh_tfidf &lt;- jh_tidy |&gt; bind_tf_idf(word, project_number, n)\n\njh_tfidf |&gt;\n  arrange(-tf_idf) |&gt;\n  head(15) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nproject_number\nword\nn\ntf\nidf\ntf_idf\n\n\n\n\n3U24TR001609-04S1\ntin\n1\n0.33333333\n3.401197\n1.1337325\n\n\n3U24TR001609-04S1\nsummary\n1\n0.33333333\n2.708050\n0.9026834\n\n\n3U24TR001609-04S1\nsupplement\n1\n0.33333333\n2.302585\n0.7675284\n\n\n1R61HL156248-01\nintranasal\n1\n0.12500000\n3.401197\n0.4251497\n\n\n1R61HL156248-01\nleptin\n1\n0.12500000\n3.401197\n0.4251497\n\n\n1R61HL156248-01\ninduced\n1\n0.12500000\n2.708050\n0.3385063\n\n\n1R61HL156248-01\nrespiratory\n1\n0.12500000\n2.708050\n0.3385063\n\n\n1R61HL156248-01\nsummary\n1\n0.12500000\n2.708050\n0.3385063\n\n\n1R61HL156248-01\ndepression\n1\n0.12500000\n2.302585\n0.2878231\n\n\n1R61AT012279-01\nshoulder\n6\n0.08333333\n3.401197\n0.2834331\n\n\n1RF1AG068997-01\nbone\n3\n0.06976744\n3.401197\n0.2372928\n\n\n1R01DA057655-01\nharm\n4\n0.06666667\n3.401197\n0.2267465\n\n\n1R01DA057655-01\nreduction\n4\n0.06666667\n3.401197\n0.2267465\n\n\n1UG3NS115718-01\nmrgprx1\n8\n0.06557377\n3.401197\n0.2230293\n\n\n1R01DA059473-01\nsleep\n6\n0.10169492\n2.014903\n0.2049054\n\n\n\n\n\n\n\nIt’s very strange that 2 documents have the term “summary” as the only term in the summary. This is indicated by a term frequency (tf) of 1. Let’s take a look at the original text for these documents.\n\n\nCode\njh_clean |&gt;\n  filter(\n    project_number %in% c(\"1R61HL156248-01\", \"3U24TR001609-04S1\")\n  ) |&gt;\n  select(project_number, summary) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nproject_number\nsummary\n\n\n\n\n1R61HL156248-01\nno summary\n\n\n3U24TR001609-04S1\nno summary\n\n\n\n\n\n\n\nThere was no summary for either of these documents! The word “summary” had a really high TF-IDF because it was the only word (“no” is a stop word and was dropped) in these documents and it likely isn’t used very frequently in other documents. This is one of the benefits of doing this kind of check before going into modeling. Interestingly, though, neither “pain” nor “stress” are in the terms with the top 15 TF-IDF values. I imagine it’s because these words occur in a lot of the documents and, therefore, would not be very useful in distinguishing one project from another. So I’ll drop the two project with no summaries and go into the modeling.\n\n\nCode\njh_tidy &lt;- jh_tidy |&gt;\n  filter(\n    !project_number %in% c(\"1R61HL156248-01\", \"3U24TR001609-04S1\")\n  )"
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#modeling",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#modeling",
    "title": "Topic Modeling with R",
    "section": "Modeling",
    "text": "Modeling\nTo be able to perform text mining of any kind, including structural topic modeling, I first need to put my data into a sparse matrix. For this I’ll use the tidytext::cast_sparse() function.\n\n\nCode\njh_sparse &lt;- jh_tidy |&gt; cast_sparse(project_number, word, n)\n\ndim(jh_sparse)\n\n\n[1]   28 1023\n\n\nI now have a matrix of 28 and 1023 columns. There is now one column for each distinct word in my corpus of data. I’m going to create a structural model with 3 topics (number chosen arbitrarily) just to get an idea of how I might evaluate a model. I’m going to use the stm::stm() function and use the LDA algorithm to generate my model.\n\n\nCode\njh_lda_3 &lt;- jh_sparse |&gt;\n  stm(K = 3, init.type = \"LDA\", seed = 212, verbose = FALSE)\n\n\nNow I want to see what some of the most frequently occurring words are within each topic. For this I’m going to simply use the tidy method from stm on the model. By default, the tidy method extracts values of “beta” which tells us the probability that a word comes from a given topic.\n\n\nCode\njh_lda_3 |&gt;\n  tidy() |&gt; \n  group_by(topic) |&gt;\n  slice_max(beta, n = 10, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  labs(y = \"Beta\", x = NULL) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe word “pain” is in the top 2 most frequently occurring words in each topic and the word “opioid” occurs in the top 10 words of 2 of the topics. It looks like the model’s not doing a great job of separating topics based on this.\nAnother interesting metric to look at is “gamma” which is the probability that a given document is related to a given topic and this is also provided by our model output, which we can access by specifying the matrix = \"gamma\" argument in our tidy() function.\n\n\nCode\njh_lda_3 |&gt;\n  tidy(matrix = \"gamma\") |&gt;\n  mutate(topic = str_c(\"Topic\", topic, sep = \" \") |&gt; factor()) |&gt;\n  ggplot(aes(document, gamma, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(breaks = 1:nrow(distinct(jh_tidy, project_number))) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 1) +\n  labs(x = \"Document\", y = \"Gamma\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIt looks like each document except for Document 16 has a high probability of coming from only 1 topic. This is pretty good considering what the beta values showed.\nNext I’ll see if there’s a better number of topics to separate these documents into."
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#choosing-a-number-of-topics",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#choosing-a-number-of-topics",
    "title": "Topic Modeling with R",
    "section": "Choosing a number of topics",
    "text": "Choosing a number of topics\nFirst I’m going to build a set of LDA models with values of K (number of topics) ranging from 2 to 25. I’m also going to create a hold-out data set to calculate the heldout-likelihood metric later. This is somewhat akin to using a cross-validation holdout set in other machine learning methodologies.\n\n\nCode\njh_tune_models &lt;- tibble(k = 2:24) |&gt;\n  mutate(\n    lda_mod = map(\n      k,\n      \\(x) stm(\n        jh_sparse,\n        K = x,\n        init.type = \"LDA\",\n        seed = 212,\n        verbose = FALSE\n      )\n    )\n  )\n\nheldout &lt;- make.heldout(jh_sparse)\n\n\nNow I’m going to extract the metrics that I want to use to evaluate my model. I’m going to focus on the approach of find the value of K with high held-out likelihood and semantic coherence metrics, but I’m also interested in looking at the residual and exclusivity metrics, so I’ll keep those as well.\n\n\nCode\njh_tune_results &lt;- jh_tune_models |&gt;\n  mutate(\n    exclusivity = map(lda_mod, exclusivity),\n    semantic_coherence = map(\n      lda_mod,\n      semanticCoherence,\n      documents = jh_sparse,\n      M = 10\n    ),\n    eval_heldout = map(lda_mod, eval.heldout, heldout$missing),\n    residual = map(lda_mod, checkResiduals, jh_sparse)\n  )\n\n\n\n\nCode\njh_tune_results |&gt;\n  transmute(\n    k,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\"),\n    Exclusivity = map_dbl(exclusivity, mean)\n  ) |&gt;\n  pivot_longer(-k, names_to = \"metrics\", values_to = \"value\") |&gt;\n  ggplot(aes(k, value, color = metrics)) +\n  geom_line(show.legend = FALSE) +\n  labs(x = \"Number of Topics\", y = NULL) +\n  facet_wrap(~ metrics, scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe highest held-out likelihood score comes from the model with K = 17 and the highest semantic coherence comes from the model with K = 20, so there’s a little bit of a trade-off. In this case I’m going to go with parsimony and choose the model with K = 17. This model also has relatively high exclusivity. Notably, exclusivity and the residual seem to flatten out and semantic coherence increases dramatically at K &gt; 10.\nNow I’m going to look at both the K = 17 and K = 20 models to see whether one is clearly better. I’m going to start by looking at how easily each model would predict each document to belong to a specific topic. For that, I’m going to look at the maximum values of gamma for each document. For comparison I’m going to show the same plot for the approach of maximizing semantic coherence (K = 21).\n\n\nCode\njh_tune_models |&gt; \n  filter(k %in% c(17, 20)) |&gt;\n  mutate(\n    max_gammas = map(\n      lda_mod,\n      \\(x) tidy(x, matrix = \"gamma\") |&gt;\n        group_by(document) |&gt;\n        summarise(gamma = max(gamma), .groups = \"drop\")\n    ),\n    k_label = str_c(\"Number of Topics:\", k, sep = \" \") |&gt; fct_reorder(k)\n  ) |&gt;\n  select(-lda_mod) |&gt;\n  unnest(max_gammas) |&gt;\n  ggplot(aes(document, gamma, fill = factor(k))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(breaks = 1:24) +\n  labs(x = \"Document\", y = \"Maximum Gamma\") +\n  facet_wrap(~ k_label, ncol = 1) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIt looks like both models yield relatively high probabilities that each document corresponds to one specific topic across the board with only documents 23 and 24 having maximum gammas below 0.75, but the model with K = 17 generally performs a bit better.\nFor the purposes of visualization I’ll look at the top 5 words defining the topics for the 17-topic model.\n\n\nCode\njh_tune_models |&gt; \n  filter(k == 17) |&gt;\n  pluck(\"lda_mod\", 1) |&gt;\n  tidy() |&gt;\n  group_by(topic) |&gt;\n  slice_max(beta, n = 5, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(\n    topic = str_c(\"Topic\", topic, sep = \" \") |&gt;\n      factor(levels = str_c(\"Topic\", 1:17, sep = \" \"))\n  ) |&gt;\n  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  labs(y = \"Beta\", x = NULL) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe word “pain” appears in the top 5 words of 7 topics; in further analysis I might consider making this a stop word along with words like “study” and “research”. But topic 11 seems to be about joint issues given the prevalence of the words “osteoarthritis” and “knee”; topic 12 seems to be about demographics with words like “american” and “urban” in the top 5 words; and topic 17 seems to be clearly about digestion issues with words like “pain”, “ibs”, and “abdominal” in the top 5 words.\nFinally, I’ll look at the titles grouped by each project’s maximum gamma value. You be the judge.\n\n\nCode\njh_tune_models |&gt; \n  filter(k == 17) |&gt;\n  pluck(\"lda_mod\", 1) |&gt;\n  tidy(matrix = \"gamma\") |&gt;\n  nest(data = -document) |&gt;\n  mutate(\n    data = map(data,\\(x) slice_max(x, gamma, n = 1, with_ties = FALSE)),\n    project_number = rownames(jh_sparse)\n  ) |&gt;\n  unnest(data) |&gt;\n  left_join(\n    jh_clean |&gt; select(project_number, project_title),\n    by = \"project_number\"\n  ) |&gt;\n  select(topic, project_title, gamma) |&gt;\n  mutate(gamma = round(gamma, 3)) |&gt;\n  arrange(topic, gamma) |&gt;\n  gt()\n\n\n\n\n\n\n\n\ntopic\nproject_title\ngamma\n\n\n\n\n1\nValidation of peripheral CGRP signaling as a target for the treatment of pain in chronic pancreatitis\n0.991\n\n\n1\nDevelopment of MRGPRX1 positive allosteric modulators as non-addictive therapies for neuropathic pain\n0.994\n\n\n2\nDEVELOPMENT &amp; MALLEABILITY FROM CHILDHOOD TO ADULTHOOD\n0.993\n\n\n3\nData Center for Acute to Chronic Pain Biosignatures\n0.991\n\n\n3\nSocial Networks among Native American caregivers participating in an evidence-based and culturally informed intergenerational intervention\n0.993\n\n\n4\nHEALthy ORCHARD: Developing plans for a Baltimore site of the HEALthy BCD study\n0.989\n\n\n5\nSubchondral Bone Cavities in Osteoarthritis Pain\n0.985\n\n\n5\nImplementing and Evaluating the Impact of Novel Mobile Harm Reduction Services on Overdose Among Women who use Drugs: The SHOUT Study\n0.989\n\n\n6\nImproving Function and Reducing Opioid Use for Patients with Chronic Low Back Pain in Rural Communities Through Improved Access to Physical Therapy Using Telerehabilitation\n0.992\n\n\n6\nUNDERSTANDING THE INTERSECTION BETWEEN OPIOIDS AND SUICIDE THROUGH THE SOUTHWEST HUB\n0.993\n\n\n7\nEvaluating the Role of the Orexin System in Circadian Rhythms of Sleep and Stress in Persons on Medication-Assisted Treatments for Opioid Use Disorder\n0.993\n\n\n7\nEvaluating Suvorexant for Sleep Disturbance in Opioid Use Disorder\n0.993\n\n\n8\nValidation of a New Large-Pore Channel as a Novel Target for Neuropathic Pain\n0.987\n\n\n9\nThe Short and Long-Term Dynamics of Opioid/Stimulant Use: Mixed Methods to Inform Overdose Prevention and Treatment Related to Polysubstance Use\n0.992\n\n\n9\nSympathetic-mediated sensory neuron cluster firing as a novel therapeutic target for neuropathic pain\n0.995\n\n\n10\n7/24 Healthy Brain and Child Development National Consortium\n0.990\n\n\n11\nA sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain\n0.989\n\n\n11\nA sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain\n0.990\n\n\n11\nIncreasing Participant Diversity in a 'Sequenced-Strategy to Improve Outcomes in People with Knee Osteoarthritis Pain (SKOAP)\n0.991\n\n\n11\nMentorship of Junior Investigators on HEAL-SKOAP\n0.992\n\n\n11\nA sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain\n0.994\n\n\n13\nDC Research Infrastructure Building &amp; Initiative to Reach, Engage, and Retain in MOUD Patients with OUD\n0.754\n\n\n13\nDC Research Infrastructure Building &amp; Initiative to Reach, Engage, and Retain in MOUD Patients with OUD\n0.754\n\n\n14\nQuantifying and Treating Myofascial Dysfunction in Post Stroke Shoulder Pain\n0.990\n\n\n14\nETHNIC DIFFERENCES IN ENDOGENOUS PAIN REGULATION: PET IMAGING OF OPIOID RECEPTORS\n0.993\n\n\n15\nSleep and Circadian Rhythm Phenotypes and Mechanisms Associated With Opioid Use Disorder Treatment Outcomes\n0.988\n\n\n16\nEffects of experimental sleep disruption and fragmentation on cerebral Mu-opioid receptor function, Mu-opioid receptor agonist analgesia, and abuse liability.\n0.991\n\n\n17\nHome-based transcutaneous electrical acustimulation for abdominal pain\n0.991"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "",
    "text": "In any binary classification analysis it is common to have to deal with heavily unbalanced data, i.e., the frequency of the majority class in the dependent variable is overwhelmingly greater than that of the minority class. One of the main reasons this can be problematic is that many classification algorithms are biased toward the majority class. If 95% of the observations belong to the majority class in the training data and the algorithm always predicts the majority class, then it will have achieved 95% accuracy.\nThere are many ways of dealing with unbalanced data including changing from a classification algorithm to an anomaly detection algorithm like an isoforest or a one-class SVM, using SMOTE to re-balance the data sets in pre-processing, up- or down-sampling, and changing the classification threshold. Here I will only deal with up-sampling and changing the classification threshold to see how it affects classification metrics by way of going through an exercise. I’ll first create and tune a base model then do the same for an up-sampled model using Tidymodels (Kuhn and Wickham 2020). Hyperparameter tuning will be done with the finetune package (Kuhn 2023a). I’ll then find the optimal threshold for each using the probably package (Kuhn, Vaughan, and Ruiz 2023). Finally I’ll show a comparison of their respective classification metrics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(themis)\nlibrary(finetune)\nlibrary(probably)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#introduction",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#introduction",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "",
    "text": "In any binary classification analysis it is common to have to deal with heavily unbalanced data, i.e., the frequency of the majority class in the dependent variable is overwhelmingly greater than that of the minority class. One of the main reasons this can be problematic is that many classification algorithms are biased toward the majority class. If 95% of the observations belong to the majority class in the training data and the algorithm always predicts the majority class, then it will have achieved 95% accuracy.\nThere are many ways of dealing with unbalanced data including changing from a classification algorithm to an anomaly detection algorithm like an isoforest or a one-class SVM, using SMOTE to re-balance the data sets in pre-processing, up- or down-sampling, and changing the classification threshold. Here I will only deal with up-sampling and changing the classification threshold to see how it affects classification metrics by way of going through an exercise. I’ll first create and tune a base model then do the same for an up-sampled model using Tidymodels (Kuhn and Wickham 2020). Hyperparameter tuning will be done with the finetune package (Kuhn 2023a). I’ll then find the optimal threshold for each using the probably package (Kuhn, Vaughan, and Ruiz 2023). Finally I’ll show a comparison of their respective classification metrics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(themis)\nlibrary(finetune)\nlibrary(probably)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#lending-club-data",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#lending-club-data",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Lending Club Data",
    "text": "Lending Club Data\nFor this project I’ll be using the lending_club data set from the modeldata (Kuhn 2023b) package. The data contain the Class variable which indicates whether a loan was “good” or bad”. For the purposes of this analysis I also will consider the “bad” outcome as the event to predict."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#data-exploration",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#data-exploration",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe first step is to load the data using the readr package from the Tidyverse (Wickham et al. 2019) and summarize it using the skimr package (Waring et al. 2022).\n\n\nCode\ndata(\"lending_club\", package = \"modeldata\")\n\nskim(lending_club) |&gt; select(-complete_rate)\n\n\n\nData summary\n\n\nName\nlending_club\n\n\nNumber of rows\n9857\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\nordered\nn_unique\ntop_counts\n\n\n\n\nterm\n0\nFALSE\n2\nter: 7047, ter: 2810\n\n\nsub_grade\n0\nFALSE\n35\nC1: 672, B5: 624, A1: 612, B3: 607\n\n\naddr_state\n0\nFALSE\n50\nCA: 1324, TX: 900, NY: 767, FL: 731\n\n\nverification_status\n0\nFALSE\n3\nSou: 3742, Not: 3434, Ver: 2681\n\n\nemp_length\n0\nFALSE\n12\nemp: 3452, emp: 893, emp: 769, emp: 733\n\n\nClass\n0\nFALSE\n2\ngoo: 9340, bad: 517\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nfunded_amnt\n0\n15683.56\n8879.11\n1000.00\n8500.00\n15000.00\n21000.00\n40000.00\n▆▇▅▃▂\n\n\nint_rate\n0\n12.53\n4.89\n5.32\n8.49\n11.99\n15.31\n28.99\n▇▇▃▂▁\n\n\nannual_inc\n0\n80320.36\n53450.17\n0.00\n50000.00\n68900.00\n96000.00\n960000.00\n▇▁▁▁▁\n\n\ndelinq_2yrs\n0\n0.33\n0.89\n0.00\n0.00\n0.00\n0.00\n22.00\n▇▁▁▁▁\n\n\ninq_last_6mths\n0\n0.58\n0.88\n0.00\n0.00\n0.00\n1.00\n5.00\n▇▁▁▁▁\n\n\nrevol_util\n0\n51.73\n24.38\n0.00\n33.20\n51.80\n70.40\n144.30\n▅▇▇▂▁\n\n\nacc_now_delinq\n0\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nopen_il_6m\n0\n2.75\n2.93\n0.00\n1.00\n2.00\n3.00\n32.00\n▇▁▁▁▁\n\n\nopen_il_12m\n0\n0.74\n1.01\n0.00\n0.00\n0.00\n1.00\n20.00\n▇▁▁▁▁\n\n\nopen_il_24m\n0\n1.62\n1.70\n0.00\n0.00\n1.00\n2.00\n30.00\n▇▁▁▁▁\n\n\ntotal_bal_il\n0\n35286.92\n41923.62\n0.00\n9450.00\n23650.00\n46297.00\n585583.00\n▇▁▁▁▁\n\n\nall_util\n0\n60.31\n20.28\n0.00\n47.00\n62.00\n75.00\n198.00\n▂▇▂▁▁\n\n\ninq_fi\n0\n0.93\n1.47\n0.00\n0.00\n0.00\n1.00\n15.00\n▇▁▁▁▁\n\n\ninq_last_12m\n0\n2.19\n2.44\n0.00\n0.00\n2.00\n3.00\n32.00\n▇▁▁▁▁\n\n\ndelinq_amnt\n0\n12.17\n565.47\n0.00\n0.00\n0.00\n0.00\n42428.00\n▇▁▁▁▁\n\n\nnum_il_tl\n0\n8.64\n7.52\n0.00\n4.00\n7.00\n11.00\n82.00\n▇▁▁▁▁\n\n\ntotal_il_high_credit_limit\n0\n45400.75\n45103.21\n0.00\n16300.00\n34375.00\n60786.00\n554119.00\n▇▁▁▁▁\n\n\n\n\n\nThe above data summary shows some important things. First, there are 9,857 observations and 23 variables with no missing values. Also, the data has 6 factor variables and 17 numeric variables. It shows that most of the numeric variables are left skewed. The “sub_grade” and “addr_state” factor variables have 35 and 50 levels respectively. With so many unique values it’s very likely that some of the classes in each of those variables become overwhelmed by majority classes. While there are feature engineering steps that I can take such as consolidating infrequent classes or splitting up sub_grade into constituent parts, I’ll just allow the feature selection step to handle it. Finally, the “Class” factor variable (dependent variable) has two levels with the majority class – “good” – having 9,340 observations and the minority class – “bad” – having 517 observations.\nHere I’ll just look at the proportions of the Class variable.\n\n\nCode\nlending_club |&gt;\n  ggplot(aes(y = Class)) +\n  geom_bar(aes(x = (..count..) / sum(..count..))) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Proportion\",\n    y = NULL,\n    title = \"Frequency of loan outcome categories (Class)\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nHere we see a sever class unbalance with only about 5% of loans having a “bad” outcome."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#create-data-splits-and-other-common-elements",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#create-data-splits-and-other-common-elements",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Create Data Splits and Other Common Elements",
    "text": "Create Data Splits and Other Common Elements\nFor the analysis I’ll split the data into training (75%) and test (25%) sets stratified by the Class variable. I’ll then create 100 bootstraps of the training data for model tuning, also stratified by the Class variable. Stratification will ensure that both cuts of the data and the bootstraps retain the class unbalance as close to the proportions of the original data.\nHere I also create a list of features to control the model tuning process.\n\n\nCode\n# Create training/test split\nset.seed(95)\ntt_split &lt;- initial_split(lending_club, prop = 0.75, strata = Class)\n\n# Create the bootstrap object\nset.seed(386)\nbstraps &lt;- bootstraps(training(tt_split), times = 100, strata = Class)\n\n# Create the model object for the workflows\nlr_mod &lt;- logistic_reg(mode = \"classification\", engine = \"glm\")\n\n# Create a list of control options\nrace_control &lt;- control_race(\n  verbose = FALSE,\n  allow_par = FALSE,\n  burn_in = 3,\n  save_workflow = TRUE,\n  save_pred = TRUE\n)"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-base-model",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-base-model",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Train Base Model",
    "text": "Train Base Model\nThe base recipe will perform the following steps:\n\nRemove variables with near-zero variance\nScale and center all numeric variables\nConvert categorical variables to dummy variables\nSelect variables based on mRMR using the colino package (Pawley, Kuhn, and Jacques-Hamilton 2023); the percentile threshold used to decide which variables to keep will be chosen through model tuning\n\nThe base recipe will be combined with a logistic regression model specification to put into a workflow. This workflow then goes through a Bayesian hyperparameter tuning process to find the best mRMR threshold as mentioned above. “Best” is defined as the full model specification that yields the highest area under the receiver operator curve (ROC-AUC) because I’ll be choosing the optimal probability threshold on the basis of this curve, even though it would be preferable to choose on the basis of the area under the precision-recall curve (PR-AUC) with unbalanced data. (Rosenberg 2022)\n\n\nCode\n# Create a base recipe\nbase_rec &lt;- recipe(Class ~ ., data = training(tt_split)) |&gt;\n  step_nzv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;\n  step_select_mrmr(\n    all_predictors(),\n    threshold = tune(),\n    outcome = \"Class\"\n  )\n\n# First tune the base model to get the best set of variables with MRMR\nbase_wflow &lt;- workflow() |&gt; add_recipe(base_rec) |&gt; add_model(lr_mod)\n\nset.seed(40)\nbase_tuned &lt;- tune_race_win_loss(\n  base_wflow,\n  resamples = bstraps,\n  control = race_control,\n  grid = 10,\n  metrics = metric_set(pr_auc, roc_auc, accuracy)\n)\n\n# Get the workflow with the best MRMR threshold\nbest_base &lt;- select_best(base_tuned, \"roc_auc\")\n\n# Get the MRMR threshold from the best base model\nmrmr_threshold &lt;- best_base |&gt; pull(threshold)\n\n\nThe mRMR threshold associated with the best model is 0.9478018 ."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-upsampled-model",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-upsampled-model",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Train Upsampled Model",
    "text": "Train Upsampled Model\nThe upsampled model will only add the pre-processing step of upsampling the minority class to the same proportion as the majority class (or close to it) by resampling observations with a Class value from the minority class using the themis package (Hvitfeldt 2023). To ensure that this is the only difference, I’ll take the mRMR threshold that was tuned for the base model directly from the best base model and specify it as the threshold for the upsampling model.\n\n\nCode\n# Build a recipe for upsampling by first updating the MRMR step from the base recipe with the threshold from the tuning process then adding upsampling\nupsample_rec &lt;- base_rec\nselect_mrmr_step &lt;- tidy(upsample_rec) |&gt;\n  filter(type %in% \"select_mrmr\") |&gt;\n  pull(number)\n\nupsample_rec$steps[[select_mrmr_step]] &lt;- update(\n  upsample_rec$steps[[select_mrmr_step]],\n  threshold = mrmr_threshold\n)\n\nupsample_rec &lt;- upsample_rec |&gt;\n  step_upsample(Class, over_ratio = tune())\n\n# Create the upsample workflow object\nupsample_wflow &lt;- workflow() |&gt; add_recipe(upsample_rec) |&gt; add_model(lr_mod)\n\n# Tune the workflow containing the upsample recipe\nset.seed(40)\nupsample_tuned &lt;- tune_race_win_loss(\n  upsample_wflow,\n  resamples = bstraps,\n  control = race_control,\n  grid = 10,\n  metrics = metric_set(pr_auc, roc_auc, accuracy)\n)\n\n# Get the workflow with the best MRMR threshold\nbest_upsample &lt;- select_best(upsample_tuned, \"roc_auc\")\n\n\nThe best over-sampling ratio is 1.1065577 ."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#compare-model-classification-metrics",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#compare-model-classification-metrics",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Compare Model Classification Metrics",
    "text": "Compare Model Classification Metrics\nNow that I’ve run both models and gotten the best hyperparameters for each, I’ll finalize both models and ensure that I have the same sets of variables in the models.\n\n\nCode\n# Get evaluation datasets for both workflows\nset.seed(75)\neval_base &lt;- base_wflow |&gt;\n  finalize_workflow(best_base) |&gt;\n  last_fit(tt_split)\n\n# Build the evaluation tibble for the upsample case\nset.seed(212)\neval_upsample &lt;- upsample_wflow |&gt;\n  finalize_workflow(select_best(upsample_tuned, \"roc_auc\")) |&gt;\n  last_fit(tt_split)\n\neval_base |&gt;\n  extract_fit_engine() |&gt;\n  tidy() |&gt;\n  select(base = term) |&gt;\n  bind_cols(\n    eval_upsample |&gt;\n      extract_fit_engine() |&gt;\n      tidy() |&gt;\n      select(upsample = term)\n  ) |&gt;\n  kable(booktabs = TRUE) |&gt; \n  kable_styling(\"striped\")\n\n\n\n\n\nbase\nupsample\n\n\n\n\n(Intercept)\n(Intercept)\n\n\nint_rate\nint_rate\n\n\ninq_last_6mths\ninq_last_6mths\n\n\nopen_il_12m\nopen_il_12m\n\n\nsub_grade_G4\nsub_grade_G4\n\n\naddr_state_SD\naddr_state_SD\n\n\nverification_status_Verified\nverification_status_Verified\n\n\n\n\n\n\n\nThe above table shows that both models ended up with the same sets of variables. Next we’ll find the optimal threshold cut-offs for each set of predictions on the test data using Youden’s J-Index.\n\n\nCode\nbase_preds &lt;- tibble(\n  truth = collect_predictions(eval_base) |&gt; pull(Class),\n  estimate = collect_predictions(eval_base) |&gt; pull(.pred_bad),\n  pred_class = collect_predictions(eval_base) |&gt; pull(.pred_class)\n)\n\n# Find the optimal classification threshold for the base model\nopt_thresh_base &lt;- base_preds |&gt;\n  threshold_perf(truth, estimate, seq(0, 1, length.out = 1000)) |&gt;\n  filter(.metric %in% \"j_index\") |&gt;\n  slice_max(.estimate, with_ties = FALSE) |&gt;\n  pull(.threshold)\n\nupsample_preds &lt;- tibble(\n  truth = collect_predictions(eval_upsample) |&gt; pull(Class),\n  estimate = collect_predictions(eval_upsample) |&gt; pull(.pred_bad),\n  pred_class = collect_predictions(eval_upsample) |&gt; pull(.pred_class)\n)\n\n# Find the optimal classification threshold for the upsample model\nopt_thresh_upsample &lt;- upsample_preds |&gt;\n  threshold_perf(truth, estimate, seq(0, 1, length.out = 1000))|&gt;\n  filter(.metric %in% \"j_index\") |&gt;\n  slice_max(.estimate, with_ties = FALSE) |&gt;\n  pull(.threshold)\n\n\nThe optimal probability threshold for classifying a loan as bad with the base model is 0.0550551 and that for the upsampling model is 0.5525526\nEach table of predictions currently has 3 columns showing the actual class for a given observation (truth), the prediction probability associated with that observation being a bad loan (estimate), and a predicted class based on a probability threshold of 0.5 (pred_class). The table below shows the first few predictions from the base model.\n\n\nCode\nbase_preds |&gt;\n  slice_head(n = 10) |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling(\"striped\")\n\n\n\n\n\ntruth\nestimate\npred_class\n\n\n\n\ngood\n0.0350714\ngood\n\n\ngood\n0.0107027\ngood\n\n\ngood\n0.0297951\ngood\n\n\ngood\n0.0432709\ngood\n\n\ngood\n0.0335889\ngood\n\n\ngood\n0.0310299\ngood\n\n\ngood\n0.0452632\ngood\n\n\ngood\n0.0226798\ngood\n\n\ngood\n0.0170578\ngood\n\n\ngood\n0.0286983\ngood\n\n\n\n\n\n\n\nNow I’d like to add to each of these tables another column containing the predicted class based on the optimal probability threshold for each respective table. The below table shows a few cases for which the predicted class using a 0.5 probability threshold does not match the predicted class using the optimal probability threshold of 0.5525526 .\n\n\nCode\nbase_preds &lt;- base_preds |&gt;\n  mutate(\n    opt_pred_class = if_else(estimate &gt;= opt_thresh_base, \"bad\", \"good\") |&gt;\n      factor(levels = c(\"bad\", \"good\"))\n  )\n\nupsample_preds &lt;- upsample_preds |&gt;\n  mutate(\n    opt_pred_class = if_else(\n      estimate &gt;= opt_thresh_upsample,\n      \"bad\",\n      \"good\"\n    ) |&gt;\n      factor(levels = c(\"bad\", \"good\"))\n  )\n\nupsample_preds |&gt; \n  filter(opt_pred_class != pred_class) |&gt;\n  slice_head(n = 10) |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling(\"striped\")\n\n\n\n\n\ntruth\nestimate\npred_class\nopt_pred_class\n\n\n\n\ngood\n0.5030688\nbad\ngood\n\n\nbad\n0.5284232\nbad\ngood\n\n\ngood\n0.5373165\nbad\ngood\n\n\ngood\n0.5007787\nbad\ngood\n\n\ngood\n0.5173375\nbad\ngood\n\n\ngood\n0.5463947\nbad\ngood\n\n\nbad\n0.5297278\nbad\ngood\n\n\ngood\n0.5227147\nbad\ngood\n\n\ngood\n0.5284232\nbad\ngood\n\n\ngood\n0.5284232\nbad\ngood\n\n\n\n\n\n\n\nNext I’ll plot confusion matrices for each set of predictions.\n\n\nCode\nconf_mats &lt;- list(base = base_preds, upsample = upsample_preds) |&gt;\n  map(\n    \\(x) list(\n      conf_mat(x, truth = truth, estimate = pred_class),\n      conf_mat(x, truth = truth, estimate = opt_pred_class)\n    ) |&gt;\n      set_names(\"standard\", \"optimal\")\n  ) |&gt;\n  list_flatten()\n\nconf_mat_plots &lt;- conf_mats |&gt;\n  imap(\n    \\(x, y) {\n      plot_title &lt;- str_replace(y, \"_\", \" \") |&gt;\n        str_to_title()\n      \n      autoplot(x, type = \"heatmap\") +\n        ggtitle(plot_title) +\n        theme(plot.title = element_text(hjust = 0.5))\n    }\n  )\n\ncowplot::plot_grid(plotlist = conf_mat_plots)\n\n\n\n\n\n\n\n\n\nThe above confusion matrices show that both changing the probability threshold for classification as a bad loan and upsampling have a significant impact on the number of positive predictions (“bad” loan), even though most of those are predicted incorrectly with this model. It’s also interesting to note that doing both – changing the probability threshold and upsampling – shows very little improvement over doing just one or the other. Below are some classification metrics for each model specification.\n\n\nCode\nconf_mats |&gt;\n  imap(\n    \\(x, y) summary(x) |&gt;\n      select(-.estimator) |&gt;\n      set_names(\"metric\", str_replace(y, \"_\", \" \") |&gt; str_to_title())\n  ) |&gt;\n  reduce(left_join, by = \"metric\") |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling(\"striped\")\n\n\n\n\n\nmetric\nBase Standard\nBase Optimal\nUpsample Standard\nUpsample Optimal\n\n\n\n\naccuracy\n0.9436105\n0.7083164\n0.6908722\n0.7452333\n\n\nkap\n0.0103118\n0.1137115\n0.1086066\n0.1316040\n\n\nsens\n0.0073529\n0.6470588\n0.6691176\n0.6176471\n\n\nspec\n0.9982825\n0.7118935\n0.6921426\n0.7526836\n\n\nppv\n0.2000000\n0.1159420\n0.1126238\n0.1272727\n\n\nnpv\n0.9451220\n0.9718640\n0.9728425\n0.9711911\n\n\nmcc\n0.0285977\n0.1775336\n0.1757144\n0.1909560\n\n\nj_index\n0.0056355\n0.3589523\n0.3612602\n0.3703306\n\n\nbal_accuracy\n0.5028177\n0.6794762\n0.6806301\n0.6851653\n\n\ndetection_prevalence\n0.0020284\n0.3079108\n0.3277890\n0.2677485\n\n\nprecision\n0.2000000\n0.1159420\n0.1126238\n0.1272727\n\n\nrecall\n0.0073529\n0.6470588\n0.6691176\n0.6176471\n\n\nf_meas\n0.0141844\n0.1966480\n0.1927966\n0.2110553\n\n\n\n\n\n\n\nThe first thing that one might notices from the above is how high the accuracy is for the base model. It’s important to remember, though, that when a data set is so unbalanced all a model has to do is predict the majority class all the time and it will have high accuracy. That model, though, has a very low sensitivity and high specificity. The high specificity, again, is due to the fact that the actual data has an overwhelming proportion of the majority class. Looking across at the metrics for the other models one sees drops in both accuracy and specificity, but much more significant increases in sensitivity from the base model. It’s also useful to note that other metrics like balanced accuracy, kappa, mcc, and f_measure also improve. This is further evidence of the effect that the imbalance in classes has on the base model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "I’m an economist turned consultant/data scientist with strong interests in the divergence between inferential modeling and predictive modeling. Over the last few years I’ve started to really enjoy programming in the Tidyverse style and have even begun to work quite a bit with Tidymodels.\n\nI hold a B.S. in Applied Economics from Ithaca College and an M.S. in Economics from the City University of New York - City College. I enjoy projects that allow me to learn and implement new techniques ranging from different types of regression analysis to more advanced machine learning algorithms.\n\n\n\nOne of the most fun personal projects of mine has been to create the cepumd package. You can read more about it here.\nI also enjoy tutoring econometrics, data science, R, and statistics on Wyzant. Check out my profile!\nI built this website using some great, open-source tools including R, RStudio, GitHub, and Quarto. I’m very grateful to all the folks that work on these tools for what they do. I’m also grateful to Sam Csik for her blog on how to add a blog to a Quarto website."
  },
  {
    "objectID": "ce-dashboards.html",
    "href": "ce-dashboards.html",
    "title": "Consumer Expenditure Surveys Dashboards",
    "section": "",
    "text": "I built the Interactive CE Visualization Tool while working as an economist at the U.S. Bureau of Labor Statistics Consumer Expenditure Surveys program. The motivation was to help Public-Use Microdata (PUMD) users better understand some of the methods the program employs to generate annual expenditure estimates. The dashboard helps to answer questions about why it’s not statistically sound practice to generate mean expenditure estimates for certain cross-sections of the data, for example.\n\n\n\nInteractive CE Visualization Tool - 2015 Data"
  },
  {
    "objectID": "ce-dashboards.html#interactive-ce-visualization-tool",
    "href": "ce-dashboards.html#interactive-ce-visualization-tool",
    "title": "Consumer Expenditure Surveys Dashboards",
    "section": "",
    "text": "I built the Interactive CE Visualization Tool while working as an economist at the U.S. Bureau of Labor Statistics Consumer Expenditure Surveys program. The motivation was to help Public-Use Microdata (PUMD) users better understand some of the methods the program employs to generate annual expenditure estimates. The dashboard helps to answer questions about why it’s not statistically sound practice to generate mean expenditure estimates for certain cross-sections of the data, for example.\n\n\n\nInteractive CE Visualization Tool - 2015 Data"
  },
  {
    "objectID": "ce-dashboards.html#interactive-ce-dictionary",
    "href": "ce-dashboards.html#interactive-ce-dictionary",
    "title": "Consumer Expenditure Surveys Dashboards",
    "section": "Interactive CE Dictionary",
    "text": "Interactive CE Dictionary\nThe purpose of the Interactive CE Dictionary is to help data users interactively search for variables, codes, and other elements of the CE PUMD rather than read through various PDF documents.\n\n\n\nInteractive CE Dictionary"
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "",
    "text": "[Image from “Interpretable Machine Learning” by Christopher Molnar](https://christophm.github.io/interpretable-ml-book/shapley.html)\n\n\nRecently I worked on a project which required explanation of ML algorithms more precisely than with precision (I’ve heard you should always open with a bad joke). Being fairly new to the world of ML, I wasn’t aware of many ways to do that, but in my research I found DALEX developed by MI2DataLab. I was impressed, surprised, and excited. I very quickly developed some code for my project and even built a modelStudio dashboard as a proof of concept. Some of the advantages of DALEX are that it…\n\nis model agnostic with comparable results across models\nhas an easy, consistent interface\nproduces easily interpretable visualizations\ncan generates HTML widgets/pages through modelStudio\nprovides Various ways to look at contributions of each observation to model predictions\n\nUpon presenting my proof of concept, though, I learned about some other project requirements that highlighted three disadvantages of DALEX. The R package does not…\n\nrun in parallel\nprovide SHAP values\nwork natively (or easily) with Tidymodels objects\n\nThese disadvantages were enough to have me go back to the drawing board. I wondered if there were other ways to get the same types of visualizations and model break-downs through or from other R packages. I found…\n\nfastshap for computing Shapley values\npdp for generating partial dependence plots\nvip for generating variable importance using different methods\nbreakDown for explaining variable contributions to individual predictions\n\nSince I was using Shapley values as a basis on which to explain everything I used only fastshap and pdp, but found that vip would work well if I wanted to use another method for computing variable importance like permutation. I found it a bit challenging to use breakDown with Tidymodels objects, otherwise, it was pretty easy to work with.\nI’m going to demonstrate how I used these packages to “open the black box” of ML algorithms. In the example below I’ll show how I did it using an XG Boost and a linear model algorithm since ‘lm’ is very standard and XG Boost can present some challenging data formatting requirements. I’ll be using the Ames Housing Data included in the modeldata package. I’m using this version rather than that from the AmesHousing package because the good folks at RStudio who work on Tidymodels did a great job of cleaning up the data a bit (thank you!) in their version and I didn’t want to spend a lot of time doing that for this demo. Also, I chose this data set because it has some things in common with the data that I used in the aforementioned project that prompted this work such as having mixed data types (discrete and numeric) in the explanatory variables, a large enough number of observations for training (and tuning) and testing a model, and some problems with multicollinearity (or redundancy) in the explanatory variables. As a quick aside, I’m also very grateful to the folks that work on the Tidyverse of packages; I use these in nearly every R session."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#introduction-motivation",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#introduction-motivation",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "",
    "text": "[Image from “Interpretable Machine Learning” by Christopher Molnar](https://christophm.github.io/interpretable-ml-book/shapley.html)\n\n\nRecently I worked on a project which required explanation of ML algorithms more precisely than with precision (I’ve heard you should always open with a bad joke). Being fairly new to the world of ML, I wasn’t aware of many ways to do that, but in my research I found DALEX developed by MI2DataLab. I was impressed, surprised, and excited. I very quickly developed some code for my project and even built a modelStudio dashboard as a proof of concept. Some of the advantages of DALEX are that it…\n\nis model agnostic with comparable results across models\nhas an easy, consistent interface\nproduces easily interpretable visualizations\ncan generates HTML widgets/pages through modelStudio\nprovides Various ways to look at contributions of each observation to model predictions\n\nUpon presenting my proof of concept, though, I learned about some other project requirements that highlighted three disadvantages of DALEX. The R package does not…\n\nrun in parallel\nprovide SHAP values\nwork natively (or easily) with Tidymodels objects\n\nThese disadvantages were enough to have me go back to the drawing board. I wondered if there were other ways to get the same types of visualizations and model break-downs through or from other R packages. I found…\n\nfastshap for computing Shapley values\npdp for generating partial dependence plots\nvip for generating variable importance using different methods\nbreakDown for explaining variable contributions to individual predictions\n\nSince I was using Shapley values as a basis on which to explain everything I used only fastshap and pdp, but found that vip would work well if I wanted to use another method for computing variable importance like permutation. I found it a bit challenging to use breakDown with Tidymodels objects, otherwise, it was pretty easy to work with.\nI’m going to demonstrate how I used these packages to “open the black box” of ML algorithms. In the example below I’ll show how I did it using an XG Boost and a linear model algorithm since ‘lm’ is very standard and XG Boost can present some challenging data formatting requirements. I’ll be using the Ames Housing Data included in the modeldata package. I’m using this version rather than that from the AmesHousing package because the good folks at RStudio who work on Tidymodels did a great job of cleaning up the data a bit (thank you!) in their version and I didn’t want to spend a lot of time doing that for this demo. Also, I chose this data set because it has some things in common with the data that I used in the aforementioned project that prompted this work such as having mixed data types (discrete and numeric) in the explanatory variables, a large enough number of observations for training (and tuning) and testing a model, and some problems with multicollinearity (or redundancy) in the explanatory variables. As a quick aside, I’m also very grateful to the folks that work on the Tidyverse of packages; I use these in nearly every R session."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#getting-started",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#getting-started",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Getting Started",
    "text": "Getting Started\nPrior to diving into the data, first I loaded packages that I would use throughout the code.\n\n\nCode\npacman::p_load(tidyverse, tidymodels, yardstick, fastshap, kableExtra)\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()\n\n\nNext I loaded a script containing some functions that I defined (available on GitHub).\n\n\nCode\nsource(\"./tidy-ml-evaluation-funs.R\")\n\n\nAt this point I was ready to start with data exploration."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#data-exploration",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#data-exploration",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Data Exploration",
    "text": "Data Exploration\nFirst I loaded the data and made some small adjustments, for example, I learned in my tinkering that there are some unused levels in the “Neighborhood” variable.\n\n\nCode\n# Load the Ames data set\ndata(ames)\n\n# Convert month variable to a factor and set the largest category of each\n# factor to that factor's base level (for contrast matrix for LM)\nhousing &lt;- ames |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    mo_sold = as_factor(mo_sold),\n    across(where(is.factor), \\(x) fct_drop(x) |&gt; fct_infreq())\n  )\n\nneighborhoods &lt;- table(ames$Neighborhood) |&gt; \n  enframe(name = \"Neighborhood\", value = \"Count\") |&gt; \n  arrange(Count) |&gt;\n  slice(1:6)\n\nhl_neigh_rows &lt;- which(neighborhoods$Count == 0)\n\nneighborhoods |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling() |&gt;\n  row_spec(hl_neigh_rows, bold = T, color = \"black\", background = \"yellow\")\n\n\n\n\n\nNeighborhood\nCount\n\n\n\n\nHayden_Lake\n0\n\n\nLandmark\n1\n\n\nGreen_Hills\n2\n\n\nGreens\n8\n\n\nBlueste\n10\n\n\nNorthpark_Villa\n23\n\n\n\n\n\n\n\nNext I just wanted to check whether there were any missing values in the data (there shouldn’t be).\n\n\nCode\nany(is.na(housing))\n\n\n[1] FALSE\n\n\nThen I wanted to see how many categories there were in each discrete variable to determine whether there might be any near-zero variance issues. When there are too many categories in a discrete variable, that variable would not do a good job of grouping (how informative is a category of size 1?) and if there are too few variables relative to the observation count then it’s likely that a small number of categories will be too dominant and possibly mask the effects of other categories.\n\n\nCode\nselect(housing, -sale_price) |&gt;\n  select_if(is.factor) |&gt;\n  summarise(across(everything(), \\(x) n_distinct(x, na.rm = TRUE))) |&gt;\n  pivot_longer(\n    everything(), \n    names_to = \"variable\", \n    values_to = \"num_categories\"\n  ) |&gt;\n  ggplot(aes(x = num_categories, y = fct_reorder(variable, num_categories))) +\n  geom_bar(stat = \"identity\", width = 0.8) +\n  labs(\n    y = \"Variable\",\n    x = \"Number of Categories\",\n    title = \"Number of Categories in Each Factor Variable\"\n  ) +\n  ml_eval_theme()\n\n\n\n\n\n\n\n\n\nThere are a few concerning variables, so next I’ll do a quick check for near-zero variance using the parameters from the recipes::step_nzv() function.\n\n\nCode\n# Check for near-zero variance\nuniqueCut &lt;- select(housing, -sale_price) |&gt;\n  select_if(is.factor) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"category\") |&gt; \n  group_by(variable) |&gt;\n  summarise(uniqueCut = (n_distinct(category) * 100) / n(), .groups = \"drop\")\n\nfreqCut &lt;- select(housing, -sale_price) |&gt;\n  select_if(is.factor) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"category\") |&gt;\n  count(variable, category, name = \"count\") |&gt;\n  group_by(variable) |&gt;\n  slice_max(count, n = 2, with_ties = FALSE) |&gt;\n  mutate(rank = c(\"first\", \"second\")) |&gt;\n  ungroup() |&gt;\n  select(-category) |&gt;\n  pivot_wider(names_from = rank, values_from = count) |&gt;\n  mutate(freqCut = first / second)\n\nhousing_nzv &lt;- left_join(freqCut, uniqueCut, by = \"variable\") |&gt;\n  mutate(nzv = as.numeric(uniqueCut &lt; 10 & freqCut &gt; 19))\n\nhousing_nzv_counts &lt;- housing_nzv |&gt;\n  count(nzv)\n\nhl_nzv_rows &lt;- which(housing_nzv_counts$nzv == 1)\n\nkable(slice(housing_nzv, 1:6), booktabs = TRUE) |&gt; kable_styling()\n\n\n\n\n\nvariable\nfirst\nsecond\nfreqCut\nuniqueCut\nnzv\n\n\n\n\nalley\n2732\n120\n22.766667\n0.1023891\n1\n\n\nbldg_type\n2425\n233\n10.407725\n0.1706485\n0\n\n\nbsmt_cond\n2616\n122\n21.442623\n0.2047782\n1\n\n\nbsmt_exposure\n1906\n418\n4.559809\n0.1706485\n0\n\n\nbsmt_fin_type_1\n859\n851\n1.009401\n0.2389078\n0\n\n\nbsmt_fin_type_2\n2499\n106\n23.575472\n0.2389078\n1\n\n\n\n\n\n\n\nCode\nkable(housing_nzv_counts, booktabs = TRUE) |&gt; \n  kable_styling() |&gt;\n  row_spec(hl_nzv_rows, bold = T, color = \"black\", background = \"yellow\")\n\n\n\n\n\nnzv\nn\n\n\n\n\n0\n28\n\n\n1\n13\n\n\n\n\n\n\n\nNext I want to look at the distributions of numeric data with violin plots from ggplot2.\n\n\nCode\nselect(housing, where(is.numeric)) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") |&gt;\n  ggplot(aes(x = variable, y = value)) +\n  geom_violin(fill = \"gray\") +\n  facet_wrap(~ variable, scales = \"free\", ncol = 5) +\n  labs(\n    y = NULL,\n    x = NULL,\n    title = \"Distributions of Numeric Variables\"\n  ) +\n  ml_eval_theme() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\nWhile it looks like some of the numeric variables are multimodal or have skewed distributions, the real concern is that sales_price, the target variable, looks a bit right skewed (as financial data often are). I’ll normalize the numeric explanatory variables in my recipe later and perform a Yeo-Johnson transformation on the target variable for modeling.\nNow that I have an idea of what the variables look like individually, I’m going to see if there are any relationships that might suggest multicollinearity or redundancy. I’ll start with the discrete variables.\n\n\nCode\nfactor_names &lt;- select(housing, -sale_price, where(is.factor)) |&gt;  names()\n\nchi_sq_dat &lt;- crossing(var1 = factor_names, var2 = factor_names) |&gt;\n  mutate(\n    chi_sq_results = map2(\n      var1,\n      var2,\n      \\(x, y) select(housing, any_of(c(x, y))) |&gt;\n        table() |&gt;\n        chisq.test() |&gt;\n        broom::tidy()\n    )\n  ) |&gt;\n  unnest(chi_sq_results) |&gt;\n  select(var1, var2, p.value) |&gt;\n  pivot_wider(names_from = var2, values_from = p.value) |&gt;\n  column_to_rownames(\"var1\")\n\nchi_sq_dat[!upper.tri(chi_sq_dat)] &lt;- NA\n\nchi_sq_dat |&gt;\n  rownames_to_column(\"var1\") |&gt;\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"p.value\") |&gt;\n  drop_na(p.value) |&gt;\n  ggplot(aes(fct_rev(var2), var1, color = p.value)) +\n  geom_point(size = 3) +\n  scale_color_viridis_c(direction = -1) +\n  labs(title = \"Chi-square Plot of Categorical Variables\", color = \"P-value\") +\n  ml_eval_theme() +\n  theme(\n    axis.title = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.border = element_blank(),\n    axis.line = element_line()\n  )\n\n\n\n\n\n\n\n\n\nA low p-value from a chi-square test indicates non-independence between 2 variables and this plot shows that there seems to be quite a lot of interdependence among these categorical variables. Next I’ll look at the relationships among the numeric variables with a correlation plot.\n\n\nCode\nselect(housing, -sale_price) |&gt;\n  select_if(is.numeric) |&gt;\n  corrr::correlate(method = \"spearman\", use = \"pairwise.complete.obs\") |&gt;\n  corrr::rearrange(absolute = FALSE) |&gt;\n  corrr::shave() |&gt;\n  corrr::rplot(colors = c(\"red\", \"white\", \"blue\")) +\n  labs(title = \"Correlation Plot of Numeric Variables\", color = \"Correlation\") +\n  ml_eval_theme() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.border = element_blank(),\n    axis.line = element_line()\n  )\n\n\n\n\n\n\n\n\n\nThere seems to be less of an interdependence problem among the numeric variables. To deal with this problem in both types of variables I’ll use the colino::step_mrmr() function step in my pre-processing recipe.\nNow I’ll move on to preparing the data for modeling."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling-prep",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling-prep",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Modeling Prep",
    "text": "Modeling Prep\nFirst I’ll split the data for training and testing and further split the training data into cross-validation folds.\n\n\nCode\n# Set the random number seed\nset.seed(485)\n\n# Split the data\nhousing_split &lt;- initial_split(housing, prop = 0.75)\nhousing_train &lt;- training(housing_split)\nhousing_test &lt;- testing(housing_split)\n\n# Create the CV dataframe\nhousing_folds &lt;- vfold_cv(housing_train, v = 10)\n\n\nNext I’ll write my pre-processing recipes; one for each type of model. The only difference between the two recipes is that in the linear model recipe I set one_hot = FALSE to keep the reference level out of the contrast matrix. The steps are as follows:\n\nDefine roles for the outcome and predictor variables\nRemove near-zero-variance variables\nNormalize all numeric predictors\nPerform a Yeo-Johnson transformation on the target variable\nApply minimum Redundancy Maximum Relevance (mRMR) feature selection\nConvert discrete variables to dummy variables\n\n\n\nCode\n# XGBoost recipe\nxgb_rec &lt;- recipe(housing_train) |&gt;\n  update_role(sale_price, new_role = \"outcome\") |&gt;\n  update_role(-has_role(\"outcome\"), new_role = \"predictor\") |&gt;\n  step_nzv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_YeoJohnson(all_outcomes()) |&gt;\n  colino::step_select_mrmr(\n    all_predictors(),\n    outcome = \"sale_price\",\n    threshold = 0.9,\n    skip = TRUE\n  ) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\n# Linear Model recipe\nnum_steps &lt;- length(xgb_rec$steps)\nlm_rec &lt;- xgb_rec\nlm_rec$steps[[num_steps]] &lt;- update(lm_rec$steps[[num_steps]], one_hot = FALSE)\n\n\nNext I want to generate a tuning grid for the XGB model. To limit the number of features selected randomly at each node I’ll first get the number of features that will be in the data after the pre-processing recipe has been applied. I also set the range for the learning rate a bit arbitrarily because I found that the default range resulted in the model learning too quickly and not keeping enough information from early iterations.\n\n\nCode\n# Get the number of features in the training data for XGB tuning grid\nn_features &lt;- bake(prep(xgb_rec), new_data = housing_train) |&gt;\n  ncol() |&gt;\n  magrittr::subtract(1) |&gt;\n  sqrt()\n\n# Create a tuning grid for XGB\nset.seed(55)\nxgb_grid &lt;- grid_random(\n  mtry(c(1, floor(n_features))),   # Range of number of features to try\n  trees(),   # Range of number of trees\n  learn_rate(range = c(-7, 1)),   # Learning rate\n  loss_reduction(),\n  size = 50\n)\n\n\nNow I’ll create the workflow objects that I’ll use in model tuning and evaluation.\n\n\nCode\n# Create model objects for both XGB and LM\nxgb_mod &lt;- boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  learn_rate = tune(),\n  loss_reduction = tune(),\n  mode = \"regression\"\n) |&gt;\n  set_engine(\"xgboost\")\n\nlm_mod &lt;- linear_reg(mode = \"regression\") |&gt; set_engine(\"lm\")\n\n# Create workflow objects for tuning\nxgb_wflow &lt;- workflow() |&gt; add_recipe(xgb_rec) |&gt; add_model(xgb_mod)\nlm_wflow &lt;- workflow() |&gt; add_recipe(lm_rec) |&gt; add_model(lm_mod)"
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Modeling",
    "text": "Modeling\n\nTuning the Models\nBecause model tuning can be quite computationally intensive (and time consuming) even for a small example like the one I’m working with, I first set up a parallel back end to run simple processes simultaneously. Some important things to note are that I had to ensure that the colino package was available to each processor and that I set the same random seed on each processor for reproducibility. Once that’s done, I tune the models.\n\n\nCode\n# Register and set up the parallel backend\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE) - 1)\ndoParallel::registerDoParallel(cl)\ninvisible(parallel::clusterEvalQ(cl, set.seed(853)))\n\n# Tune both models\nxgb_tune &lt;- tune_grid(\n  xgb_wflow,\n  grid = xgb_grid,\n  resamples = housing_folds,\n  metrics = yardstick::metric_set(rmse, rsq_trad),\n  control = control_grid(allow_par = TRUE, parallel_over = \"everything\")\n)\n\nlm_tune &lt;- fit_resamples(\n  lm_wflow,\n  resamples = housing_folds,\n  metrics = metric_set(rmse, rsq_trad),\n  control = control_grid(allow_par = TRUE, parallel_over = \"resamples\")\n)\n\n# Close the connection to the cluster\ndoParallel::stopImplicitCluster()\n\n\n\n\nModel Selection and Fitting\nNow that the model hyperparameters have been tuned (at least for XGB) I can pull the models that performed best from our cross-validation process.\n\n\nCode\n# Get the best models from each tuning process\nbest_models &lt;- list(xgb = xgb_tune, lm = lm_tune) |&gt;\n  map(select_best, metric = \"rmse\")\n\n# Collect tuning metrics for each tuning process\ntune_metrics &lt;- list(xgb = xgb_tune, lm = lm_tune) |&gt;\n  map(collect_metrics)\n\n# Fit the models with the best parameters to the entire training data set\nfinal_wflows &lt;- map2(\n  list(xgb = xgb_wflow, lm = lm_wflow),\n  best_models,\n  \\(x, y) finalize_workflow(x, y) |&gt; fit(data = housing_train)\n)\n\n\n\n\nModel Evaluation\nWith the best performing models from cross-validation now fit to the entire training data set, I can evaluate these models against the test data.\n\n\nCode\n# Evaluate each model with the test data and store 'last_fit' objects\nset.seed(485)\nfinal_wflow_evals &lt;- map(\n  final_wflows,\n  \\(x) last_fit(\n    x,\n    split = housing_split,\n    metrics = metric_set(rmse, mae, mape, rsq_trad)\n  )\n)\n\n\nNow I can see how each model performed on the test data.\n\n\nCode\n# Generate a dataframe of model metrics to compare the two models\nmodel_metrics &lt;- final_wflow_evals |&gt;\n  map2_df(\n    names(final_wflow_evals),\n    ~ collect_metrics(.x) |&gt; \n      select(.metric, .estimate) |&gt; \n      pivot_wider(names_from = .metric, values_from = .estimate) |&gt;\n      mutate(algorithm = .y) |&gt;\n      relocate(algorithm)\n  )\n\nkable(model_metrics, booktabs = TRUE) |&gt; kable_styling()\n\n\n\n\n\nalgorithm\nrmse\nmae\nmape\nrsq_trad\n\n\n\n\nxgb\n0.1365643\n0.0954771\n0.8758136\n0.8525943\n\n\nlm\n0.1518358\n0.1056075\n0.9692341\n0.8177832\n\n\n\n\n\n\n\nIt looks like the XGB model outperformed the Linear Model on every metric, but not by very much. Both seem to have performed pretty well at predicting home sale prices. It’s important to keep in mind here that the data were transformed, which is why the RMSE and MAE are so small. Now let’s go ahead and open these “black boxes”."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#opening-the-black-box",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#opening-the-black-box",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Opening the “Black Box”",
    "text": "Opening the “Black Box”\n\nGenerate SHAP Values\nAs stated earlier, this part of the analysis will be based on SHAP values. This explanation from Christoph Molnar’s book Interpretable Machine Learning is a great primer on what SHAP values are and how they can help someone understand the effect of a given feature/variable on a prediction.\nWith that in mind, now that the models have been tuned, trained, and evaluated, I can generate SHAP values to understand what’s going on under the hood (kinda). Note that this is still being run in parallel until the end of this portion.\n\n\nCode\n# Get matrices of training features for each model\ntraining_features &lt;- final_wflows |&gt; \n  map(\\(x) pull_workflow_mold(x) |&gt; pluck(\"predictors\"))\n\n# Register and set up the parallel backend\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE) - 1)\ndoParallel::registerDoParallel(cl)\ninvisible(parallel::clusterEvalQ(cl, set.seed(44)))\n\nshap &lt;- final_wflows |&gt; map(get_shap)\n\n# Get SHAP variable importance features\nvar_imp &lt;- shap |&gt; map(get_shap_imp)\n\n# Close the connection to the cluster\nparallel::stopCluster(cl)\n\n\n\n\nPlot Relationships Between SHAP and the Training Data\nI now have my SHAP values and can begin to visualize a few different things. The first visualization will be a SHAP summary plot, which I also found in Christoph Molnar’s Interpretable Machine Learning. When I saw this plot I was amazed at how much information it actually contains once you grasp the dimensionality.\n\n\nCode\n# Pull data from the outermost point on the SHAP summary plot\npoint_dat_xgb &lt;- get_shap_summary(\n  var_imp$xgb, \n  shap$xgb, \n  training_features$xgb, \n  max_features = 1\n) |&gt;\n  mutate(shap_value = as.numeric(shap_value)) |&gt;\n  slice_max(shap_value, with_ties = FALSE)\n\npoint_dat_lm &lt;- get_shap_summary(\n  var_imp$lm, \n  shap$lm, \n  training_features$lm, \n  max_features = 1\n) |&gt;\n  mutate(shap_value = as.numeric(shap_value)) |&gt;\n  slice_max(shap_value, with_ties = FALSE)\n\n# Put the data from that point into a string object\npoint_annotation_xgb &lt;- str_glue(\n  \"Row = {point_dat_xgb$id}\n  Variable = {point_dat_xgb$variable}\n  SHAP Value = {round(point_dat_xgb$shap_value, 3)}\n  Feature Value = {round(point_dat_xgb$feature_value, 3)}\"\n)\n\npoint_annotation_lm &lt;- str_glue(\n  \"Row = {point_dat_lm$id}\n  Variable = {point_dat_lm$variable}\n  SHAP Value = {round(point_dat_lm$shap_value, 3)}\n  Feature Value = {round(point_dat_lm$feature_value, 3)}\"\n)\n\n# Create SHAP summaries containing data frames and plots\nshap_summary_plots &lt;- list(\n  var_imp, \n  shap, \n  training_features\n) |&gt;\n  pmap(get_shap_summary, max_features = 10) %&gt;%\n  map2(c(\"XG Boost\", \"Linear Regression\"), plot_shap_summary)\n\ntotal_bsmt_sf_pdp &lt;- final_wflows |&gt;\n  map2(c(\"XG Boost\", \"Linear Regression\"), plot_pdp, pred_var = total_bsmt_sf)\n\n\n\n\nCode\n# Generate the SHAP summary plot with an annotation for the outermost point\nset.seed(23)\nshap_summary_plots$xgb + \n  geom_segment(\n    aes(x = 0.5, xend = 0.575, y = 8, yend = 10.01), \n    color = \"black\",\n    arrow = arrow(length = unit(0.03, \"npc\"))\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 0.46, \n    y = 6.8, label = point_annotation_xgb, \n    hjust = \"center\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate the SHAP summary plot with an annotation for the outermost point\nset.seed(23)\nshap_summary_plots$lm + \n  geom_segment(\n    aes(x = 0.79, xend = 0.9, y = 8, yend = 10), \n    color = \"black\",\n    arrow = arrow(length = unit(0.03, \"npc\"))\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 0.8, \n    y = 6.8, label = point_annotation_lm, \n    hjust = \"center\"\n  )\n\n\n\n\n\n\n\n\n\nThe first plot above shows that row 1693 has a positive SHAP value of 0.5879337 for the gr_liv_area (0.095) in the XGB model, which had an above average value of 4.1097005 in the training data. When considering all the observations collectively the feature value is ideally positively related with the SHAP value. Next I’ll look at partial dependence plots to show the effect that the gr_liv_area has on sale_price according to each model.\n\n\nCode\ngr_liv_area_pdp &lt;- final_wflows |&gt;\n  map2(c(\"XG Boost\", \"Linear Regression\"), plot_pdp, pred_var = gr_liv_area)\n\ngr_liv_area_pdp$xgb\n\n\n\n\n\n\n\n\n\nCode\ngr_liv_area_pdp$lm\n\n\n\n\n\n\n\n\n\nAccording to both models the gr_liv_area variable positively effects the predicted sale price of a home in the training data, which seems consistent with general knowledge about home prices given that gr_liv_area is a measure of the above ground living area (more commonly known as “square footage”). It’s important to note, however, that, unlike the coefficient in a regression model, one should not make inferences from a partial dependency beyond the dimensions of the training data. A partial dependency plot is meant only to provide an overview on the broad relationship between an explanatory variable and the target variable.\nLastly, I can look at how each feature affects a particular observation with a contribution plot. I’ll consider the house observed in row 2049 of the training data set.\n\n\nCode\nobs_2409_contrib &lt;- shap |&gt;\n  map2(\n    c(\"XG Boost\", \"Linear Regression\"), \n    get_contributions, \n    rnum = 2049, \n    nfeat = 15 \n  )\n\n\n\n\nCode\nobs_2409_contrib$xgb$contrib_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\nobs_2409_contrib$lm$contrib_plot\n\n\n\n\n\n\n\n\n\nThe above XG Boost contribution plot shows that total_bsmt_sf had the largest positive effect on the predicted sale price of the sale in observation 2049 and having garage_finish_Unf (having an unfinished garage) had the largest negative effect. The Linear Regression contribution plot shows that gr_liv_area had the largest positive effect and neighborhood_Brookside had the largest negative effect on the predicted sale price. However, it’s interesting to note that garage_finish_Fin, the opposite of garage_finish_Unf, had a negative effect in the Linear Regression."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#final-thoughts",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#final-thoughts",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWhile these methods of looking into machine learning algorithms are not designed for inference, they do go a long way in helping to elucidate the relationships or patterns that machine learning algorithms find. They help answer the questions about why some predictions come out the way that they do, whether they make sense or not."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html",
    "title": "Introduction to cepumd",
    "section": "",
    "text": "The purpose of cepumd is to make working with Consumer Expenditure Surveys (CE) Public-Use Microdata (PUMD) easier toward calculating mean, weighted, annual expenditures (henceforth “mean expenditures”). The challenges cepumd seeks to address deal primarily with pulling together the necessary data toward this end. Some of the overarching ideas underlying the package are as follows:\n\n\n\nUse a Tidyverse framework for most operations and be (hopefully) generally Tidyverse friendly\nBalance the effort to make the end user’s experience with CE PUMD easier while being flexible enough to allow that user to perform any analysis with the data they wish\nOnly designed to help users calculate mean expenditures on and of the consumer unit (CU), i.e., not income, not assets, not liabilities, not gifts."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#motivation",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#motivation",
    "title": "Introduction to cepumd",
    "section": "",
    "text": "The purpose of cepumd is to make working with Consumer Expenditure Surveys (CE) Public-Use Microdata (PUMD) easier toward calculating mean, weighted, annual expenditures (henceforth “mean expenditures”). The challenges cepumd seeks to address deal primarily with pulling together the necessary data toward this end. Some of the overarching ideas underlying the package are as follows:\n\n\n\nUse a Tidyverse framework for most operations and be (hopefully) generally Tidyverse friendly\nBalance the effort to make the end user’s experience with CE PUMD easier while being flexible enough to allow that user to perform any analysis with the data they wish\nOnly designed to help users calculate mean expenditures on and of the consumer unit (CU), i.e., not income, not assets, not liabilities, not gifts."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#overview-of-the-ce-and-ce-pumd",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#overview-of-the-ce-and-ce-pumd",
    "title": "Introduction to cepumd",
    "section": "Overview of the CE and CE PUMD",
    "text": "Overview of the CE and CE PUMD\nFirst a little history…\nThe first Consumer Expenditure Survey happened in 1888 (https://www.bls.gov/opub/hom/cex/history.htm), it was first used to revise CPI weights in 1972-1973, and it has been collected on a monthly basis since 1979. For a little bit more detail on the history of the CE, check out the slide deck of a presentation delivered by Steve Henderson (former Chief of the Branch of Information and Analysis) and Adam Safir (current Division Chief of CE) called 130 Years of the Consumer Expenditure Surveys (CE): 1888 - 2018\nFrom the CE home page:\n\n“The Consumer Expenditure Surveys (CE) program provides data on expenditures, income, and demographic characteristics of consumers in the United States. The CE program provides these data in tables, LABSTAT database, news releases, reports, and public use microdata files.\n\n\nCE data are collected by the Census Bureau for BLS in two surveys, the Interview Survey for major and/or recurring items and the Diary Survey for more minor or frequently purchased items. CE data are primarily used to revise the relative importance of goods and services in the market basket of the Consumer Price Index. The CE is the only Federal household survey to provide information on the complete range of consumers’ expenditures and incomes. Here is an overview of the CE program and its methods.”\n\nSome important things to note are that expenditure data are collected through two different survey instruments (Diary and Interview), expenditure categories are organized hierarchically, and data are stored across thousands of files to which the CE provides access through their website. Also, given the length of the program, it would be difficult to harmonize data across all those years and files, so there are some inconsistencies in the way data are stored, which cepumd seeks to address (more on this further down).\nPlease visit the following pages to learn more about the CE program overall and CE PUMD more specifically.\n\nCE homepage: (https://www.bls.gov/cex/)\nCE PUMD page: (https://www.bls.gov/cex/pumd.htm)\nCE PUMD Getting Started Guide: https://www.bls.gov/cex/pumd-getting-started-guide.htm\nCE Dictionary for Interview and Diary Surveys (XLSX download) (https://www.bls.gov/cex/pumd/ce_pumd_interview_diary_dictionary.xlsx)\nCE PUMD published tables: (https://www.bls.gov/cex/tables.htm)\nCE PUMD Handbook of Methods: https://www.bls.gov/opub/hom/cex/\nCE Frequently Asked Questions: https://www.bls.gov/cex/csxfaqs.htm"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#challenges-addressed-by-cepumd",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#challenges-addressed-by-cepumd",
    "title": "Introduction to cepumd",
    "section": "Challenges addressed by cepumd",
    "text": "Challenges addressed by cepumd\ncepumd seeks to address challenges in three categories: data gathering/organization; managing data inconsistencies; and calculating weighted, annual metrics.\n\nData wrangling\n\nConvert hierarchical grouping (HG) files to data tables using ce_hg()\nHelp the user identify the Universal Classification Codes (UCCs) related to their analysis using a combination of ce_hg() and ce_uccs()\nCombine all required files and variables using ce_prepdata()\n\nManaging data inconsistencies\n\nProvide the ability to re-code variable categories using the CE Dictionary for Interview and Diary Surveys\nResolve some inconsistencies such as differences code definitions between the Interview and Diary (check the definitions of the “FAM_TYPE” variable categories in 2015 for an example)\nProvide useful errors or warnings when there are multiple categories of something the user is trying to access, e.g., some titles in the hierarchical grouping files (“stub” or “HG” files) repeat and requires more careful selection of UCCs\n\nCalculating weighted, annual metrics\n\nCalculate a mean expenditure with ce_mean() or expenditure quantile with ce_quantile()\nAccount for the factor (annual vs. quarterly expenditure)\nAccount for the “months in scope” of a given consumer unit (CU)\nAnnualize expenditures for either Diary or Interview expenditures\nIntegrate Interview and Diary data as necessary\n\n\nSource code and other package information is available at https://github.com/arcenis-r/cepumd"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#cautions-and-recommendations",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#cautions-and-recommendations",
    "title": "Introduction to cepumd",
    "section": "Cautions and recommendations",
    "text": "Cautions and recommendations\n\nEstimates produced using PUMD, which is top-coded by the CE and has some records suppressed to protect respondent confidentiality, will not match the published estimates released by the CE in most cases. The CE’s published estimates are based on confidential data that are not top-coded nor have records suppressed. You can learn more at CE Protection of Respondent Confidentiality.\nWhen calculating estimates for sub-samples or cross-sections of data it is best to stick to the combinations of variables that the CE uses in it’s publication tables, e.g., income, geography, composition of CU, size of CU. This is because CE data are collected using a stratified, random sample (a.k.a., “representative sample”) and only analyses conducted using the stratification variables are statistically valid. Using other variables can be helpful to understand spending across different groups, but unweighted estimates are likely more useful for this. cepumd currently does not support unweighted estimates, but data for such an analysis can be prepared using ce_prepdata().\nQuantiles should only be generated using data from 1 survey instrument as the samples for the Interview and Diary are different.\nCheck the expenditure category in the appropriate HG file to ensure that it is the category for which you intend to generate an estimate.\nStore an HG object in the environment and call that directly in ce_prepdata()."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#installation",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#installation",
    "title": "Introduction to cepumd",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of cepumd from its GitHub repo.\n\n\nCode\npak::pkg_install(\"arcenis-r/cepumd\")"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#key-cepumd-functions",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#key-cepumd-functions",
    "title": "Introduction to cepumd",
    "section": "Key cepumd functions",
    "text": "Key cepumd functions\n\nThe workhorse of cepumd is ce_prepdata(). It merges the household characteristics file (FMLI/-D) with the corresponding expenditure tabulation file (MTBI/EXPD) for a specified year, adjusts weights for months-in-scope and the number of collection quarters, adjusts some cost values by their periodicity factor (some cost categories are represented as annual figures and others as quarterly). With the recent update it only requires the first 3 arguments to function: the year, the survey type, and one or more valid UCCs. ce_prepdata() now creates all of the other necessary objects within the function if not provided.\nThere are two functions for wrangling hierarchical grouping data into more usable formats:\n\nce_hg() pulls the requested type of HG file (Interview, Diary, or Integrated) for a specified year.\nce_uccs() filters the HG file for the specified expenditure category and returns either a data frame with only that section of the HG file or the Universal Classification Codes (UCCs) that make up that expenditure category.\n\nThere are two functions that the user can use to calculate CE summary statistics:\n\nce_mean() calculates a mean expenditure, standard error of the mean, coefficient of variation, and an aggregate expenditure.\nce_quantiles() calculates weighted expenditure quantiles. It is important to note that calculating medians for integrated expenditures is not recommended because the calculation involves using weights from both the Diary and Survey instruments."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#example-workflows",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#example-workflows",
    "title": "Introduction to cepumd",
    "section": "Example workflows",
    "text": "Example workflows\nThe following are a few sample workflows that show how cepumd can be used. Before jumping into those I’ll first install and load the necessary packages and store some CEPUMD files. I’ll keep the path to those files in a variable called ce_data_dir.\n\n\nCode\npacman::p_load(knitr, readxl, tidyverse, cepumd)\n\n\n\nSimple workflow: Integrated pet expenditures\nThe following is an example of how someone might go about using cepumd to calculate a 2021 annual, weighted estimate of mean expenditures on pets for all of the U.S. using CE integrated data. This is just a quick and easy calculation.\n\n\nCode\ninteg21_hg &lt;- ce_hg(\n  2021,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\nce_prepdata(\n  2021,\n  integrated,\n  integ21_hg,\n  uccs = ce_uccs(integ21_hg, expenditure = \"Pets\", ucc_group = \"PETS\"),\n  dia_zp = file.path(ce_data_dir, \"diary21.zip\"),\n  int_zp = c(\n    file.path(ce_data_dir, \"intrvw20.zip\"),\n    file.path(ce_data_dir, \"intrvw21.zip\")\n  )\n) |&gt;\n  ce_mean() |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nagg_exp\nmean_exp\nse\ncv\n\n\n\n\n130886736176\n981.3035\n53.5767\n5.459748\n\n\n\n\n\nYup… that’s all it takes. I simply ran ce_hg to get the hierarchical grouping (stub) file for integrated expenditures for 2021; then ran ce_prepdata() with the year, the survey type, the stub file, uccs I needed, and the file paths to where I downloaded the data files; then I piped that directly into ce_mean(). An important thing to notice is that I provided two file paths to the int_zp argument. I did this because calculating integrated CE annual estimates actually requires 5 quarters of data from the Interview survey. Some of the data for calculating 2021 estimates is provided in the 2020 Interview data.This is one of the reasons it’s important to be familiar with CE methodology and how it changes over time when working with CE PUMD. Prior to 2020, file storing practices were different as stated in the Getting Started Guide Interview Survey section.\n\n\nSlightly more advanced workflow: Used Car & Truck Expenditures by Urbanicity\nIn this example I’ll calculate estimated annual expenditures on new and used cars by urbanicity also for 2021. Once the data are prepped with ce_data() I’ll just nest the data by urbanicity and run ce_means() and ce_quantiles() on the nested data sets. Since the overwhelming number of reports of vehicle purchases occur in the Interview survey, I’ll only use Interview data.\nFirst I’ll get the stub file and filter it for categories involving new or used cars.\n\n\nCode\nint21_hg &lt;- ce_hg(\n  2021,\n  interview,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\nint21_hg |&gt;\n  filter(str_detect(title, \"[C|c]ars\")) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n4\nCars and trucks, new\nNEWCARS\nG\n1\n\n\n5\nNew cars\n450110\nI\n1\n\n\n4\nCars and trucks, used\nUSEDCARS\nG\n1\n\n\n5\nUsed cars\n460110\nI\n1\n\n\n\n\n\nSo there’s one UCC for “New cars” and one for “Used cars”. I’ll use the code above to grab those UCCs and prepare my data.\n\n\nCode\ncar_data &lt;- ce_prepdata(\n  2021,\n  interview,\n  int21_hg,\n  uccs = int21_hg |&gt;\n    filter(str_detect(title, \"[C|c]ars\"), !is.na(as.numeric(ucc))) |&gt;\n    pull(ucc),\n  bls_urbn,  # &lt;------- this is the variable for urbanicity\n  int_zp = c(\n    file.path(ce_data_dir, \"intrvw20.zip\"),\n    file.path(ce_data_dir, \"intrvw21.zip\")\n  ),\n  recode_variables = TRUE,\n  dict_path = file.path(ce_data_dir, \"ce-data-dict.xlsx\")\n)\n\ncar_data |&gt;\n  group_nest(bls_urbn) |&gt;\n  mutate(ce_ann_means = map(data, ce_mean)) |&gt;\n  select(-data) |&gt;\n  unnest(ce_ann_means) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nbls_urbn\nagg_exp\nmean_exp\nse\ncv\n\n\n\n\nUrban\n168458799558\n1341.7184\n106.2495\n7.918912\n\n\nRural\n4756065697\n591.5108\n155.0020\n26.204421\n\n\n\n\n\nGetting the annual, weighted estimate of the median (or another quantile) would be just as easy. Since I’m using interview data only here (it would be bad practice to try to calculate quantiles with integrated data), this would be a good example. I’ll calculate the first percentile and the median along with the 0.991 through 0.999 quantiles for the overall sample rather than breaking it down by urbanicity.\n\n\nCode\nce_quantiles(\n  car_data,\n  probs = c(0.01, 0.5, 0.95, seq(0.99, 0.999, by = 0.001))\n) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nprobs\nquantile\n\n\n\n\n1.0%\n0\n\n\n50.0%\n0\n\n\n95.0%\n0\n\n\n99.0%\n8300\n\n\n99.1%\n10000\n\n\n99.2%\n12434\n\n\n99.3%\n15000\n\n\n99.4%\n17850\n\n\n99.5%\n20000\n\n\n99.6%\n22948\n\n\n99.7%\n26593\n\n\n99.8%\n30000\n\n\n99.9%\n40000\n\n\n\n\n\nAt least 95% of households in the Interview survey did not report expenditures on cars in 2021, which explains why the mean is so low.\n\n\nVery advanced workflow: Inflation adjusted food away from home expenditures by household size\nIn this last example I’m going to assume very little knowledge about the CE. I’d like to compare mean annual expenditures on food away from home between 2010 and 2020 by household size and I want to convert expenditures to 2023 dollars using the CPI. First I’d go to the CE PUMD Data Files page and download the files that I need for both years. I’ll also go to the CE PUMD Documentation page to download the hierarchical grouping files to get the UCCs for “Food away from home” and the CE Dictionary to find out what variable has data on the household size.\nWith all that done, now I want to look at the hierarchical grouping files for 2010 and 2020 for integrated data as they relate to “Food away from home”.\n\n\nCode\ninteg10_hg &lt;- ce_hg(\n  2010,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\ninteg20_hg &lt;- ce_hg(\n  2020,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\n\nFirst I’ll explore the titles of the hierarchical grouping files to see if any of them mention “food away”\n\n\nCode\ninteg10_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"food away\")) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nFood away from home\nFOODAWAY\nG\n1\n\n\n\n\n\nNow I’ll do the same for 2020.\n\n\nCode\ninteg20_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"food away\")) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nFood away from home\nFOODAW\nG\n1\n\n\n\n\n\nHere I’ll take note of the title, which is the same in both years (“Food away from home”). I’ll use that to get the UCCs for both years.\n\n\nCode\nfood_away_uccs_10 &lt;- integ10_hg |&gt;\n  ce_uccs(expenditure = \"Food away from home\")\n\nfood_away_uccs_20 &lt;- integ20_hg |&gt;\n  ce_uccs(expenditure = \"Food away from home\")\n\n\nHere’s a quick look at the UCCs from 2010.\n\n\nCode\nfood_away_uccs_10\n\n\n [1] \"190111\" \"190112\" \"190113\" \"190114\" \"190211\" \"190212\" \"190213\" \"190214\"\n [9] \"190311\" \"190312\" \"190313\" \"190314\" \"190321\" \"190322\" \"190323\" \"190324\"\n[17] \"190901\" \"190902\" \"190903\" \"790430\" \"800700\"\n\n\nNow the 2020 UCCs.\n\n\nCode\nfood_away_uccs_10\n\n\n [1] \"190111\" \"190112\" \"190113\" \"190114\" \"190211\" \"190212\" \"190213\" \"190214\"\n [9] \"190311\" \"190312\" \"190313\" \"190314\" \"190321\" \"190322\" \"190323\" \"190324\"\n[17] \"190901\" \"190902\" \"190903\" \"790430\" \"800700\"\n\n\nThe vectors of UCCs look identical, but I’ll keep both just to be cautious.\nNext I’ll turn to finding the variable for household size in the CE data dictionary. It’s important to remember that the dictionary is stored as an “XLSX” file. I’ll use functions from the readxl package to work with the dictionary.\n\n\nCode\nexcel_sheets(file.path(ce_data_dir, \"ce-data-dict.xlsx\"))\n\n\n[1] \"Cover\"     \"Variables\" \"Codes \"   \n\n\nNow I’ll see what variables contain anything about the number of household members. To do that I’ll have to load the sheet from the dictionary containing the variable definitions. I also want to filter the variable data to only the FMLI where the “Last year” column is missing, i.e., the variable definition is still in use.\n\n\nCode\nce_variables &lt;- read_excel(\n  file.path(ce_data_dir, \"ce-data-dict.xlsx\"),\n  sheet = \"Variables\"\n)\n\nce_variables |&gt;\n  filter(\n    str_detect(File, \"FMLI\"),\n    str_detect(\n      tolower(`Variable description`), \"number of members\"\n    )\n  ) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey\nFile\nVariable Name\nVariable description\nFormula\nFlag name\nSection number\nSection description\nSection part\nFirst year\nFirst Quarter\nLast quarter\nLast year\nComment\n\n\n\n\nINTERVIEW\nFMLI\nAS_COMP5\nNumber of members under age 2 in CU\nCOUNT (AGE &lt; 2)\nAS_C_MP5\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1984\n1\nNA\nNA\nNA\n\n\nINTERVIEW\nFMLI\nAS_COMP5\nNumber of members under age 2 in CU\nNA\nAS_C_MP5\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1980\n1\n4\n1981\nNA\n\n\nINTERVIEW\nFMLI\nFAM_SIZE\nNumber of Members in CU\nNA\nFAM__IZE\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1984\n1\nNA\nNA\nNA\n\n\nINTERVIEW\nFMLI\nFAM_SIZE\nNumber of Members in CU\nNA\nFAM__IZE\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1980\n1\n4\n1981\nNA\n\n\n\n\n\nIt looks like FAM_SIZE is the variable I want. I can see that this variable was used from 1980 through 1981 then was dropped and re-introduced in 1984 and has been in use since. So it looks like it’s available for my 2 years of interest. Next I’ll check whether the FAM_SIZE variable has any value codes associated with it. I’ll have to pull in the “Codes” sheet. (Check your spelling here.)\n\n\nCode\nce_codes &lt;- read_excel(\n  file.path(ce_data_dir, \"ce-data-dict.xlsx\"),\n  sheet = \"Codes \"\n)\n\nce_codes |&gt;\n  filter(File %in% \"FMLI\", Variable %in% \"FAM_SIZE\") |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey\nFile\nVariable\nCode value\nCode description\nFirst year\nFirst quarter\nLast year\nLast quarter\nComment\n…11\n\n\n\n\n\n\n\nIt looks like FAM_SIZE is not a coded variable (no observations in the “Codes” sheet), so it must be numeric. With all that, I’m ready to prepare my data. I’ll start by preparing the 2010 data and getting a summary of the FAM_SIZE variable since it is a continuous variable.\n\n\nCode\nfood_away_data_10 &lt;- ce_prepdata(\n  2010,\n  integrated,\n  integ10_hg,\n  food_away_uccs_10,\n  dia_zp = file.path(ce_data_dir, \"diary10.zip\"),\n  int_zp = file.path(ce_data_dir, \"intrvw10.zip\"),\n  fam_size\n)\n\nsummary(food_away_data_10$fam_size)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   2.000   2.666   4.000  14.000 \n\n\nSince some households have as many as 14 people, I’ll create a FAM_SIZE label with any number greater than 4 taking on the value “5+”. Next, I’ll prepare the 2020 data and row bind it with the 2010 data as well as create the “fam_size_label” variable. I’m also going to convert “ref_mo” and “ref_yr” to character to make it compatible with the CPI data that I’ll get later. I’ll also take a look at just a snippet of the data.\n\n\nCode\nfood_away_data_20 &lt;- ce_prepdata(\n  2020,\n  integrated,\n  integ10_hg,\n  food_away_uccs_20,\n  dia_zp = file.path(ce_data_dir, \"diary20.zip\"),\n  int_zp = c(\n    file.path(ce_data_dir, \"intrvw19.zip\"),\n    file.path(ce_data_dir, \"intrvw20.zip\")\n  ),\n  fam_size\n)\n\nfood_away_comp_data &lt;- food_away_data_10 |&gt;\n  mutate(year = \"2010\") |&gt;\n  bind_rows(food_away_data_20 |&gt; mutate(year = \"2020\")) |&gt;\n  mutate(\n    fam_size_label = if_else(fam_size &gt; 4, \"5+\", as.character(fam_size)),\n    ref_yr = as.character(ref_yr)\n  )\n\nfood_away_comp_data |&gt;\n  select(survey, year, newid, finlwt21, cost, ucc, ref_yr, ref_mo) |&gt;\n  filter(!is.na(ucc)) |&gt;\n  group_by(year, survey) |&gt;\n  slice_sample(n = 3) |&gt;\n  ungroup() |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nsurvey\nyear\nnewid\nfinlwt21\ncost\nucc\nref_yr\nref_mo\n\n\n\n\nD\n2010\n01047471\n43881.11\n2.30971\n190321\n2010\n1\n\n\nD\n2010\n01077871\n24204.65\n66.30000\n190111\n2010\n5\n\n\nD\n2010\n01116361\n62846.94\n18.33000\n190313\n2010\n8\n\n\nI\n2010\n02265533\n12788.96\n144.00000\n790430\n2010\n10\n\n\nI\n2010\n02309802\n22657.97\n33.33333\n790430\n2010\n7\n\n\nI\n2010\n02247652\n15588.84\n52.00000\n790430\n2010\n3\n\n\nD\n2020\n04630171\n33468.71\n523.51000\n190211\n2020\n11\n\n\nD\n2020\n04591042\n60503.51\n137.67000\n190311\n2020\n8\n\n\nD\n2020\n04436801\n49119.67\n156.00000\n190112\n2020\n2\n\n\nI\n2020\n04316443\n16358.45\n52.00000\n800700\n2020\n6\n\n\nI\n2020\n04227663\n27787.28\n100.00000\n190901\n2020\n1\n\n\nI\n2020\n04234823\n28645.27\n92.00000\n790430\n2020\n1\n\n\n\n\n\nI’ll now get CPI data for the years in the analysis and for 2023 to set as a base using the blsR package (https://github.com/groditi/blsR). I’m going to use the “All Urban Consumers (Current Series)” series, which has series ID “CUUR0000SA0”.\n\n\nCode\ncpi_data &lt;- blsR::get_series(\n  \"CUUR0000SA0\",\n  start_year = 2010,\n  end_year = 2023\n) |&gt;\n  pluck(\"data\") |&gt;\n  map(\n    \\(x) list_flatten(x) |&gt;\n      enframe() |&gt;\n      filter(!name %in% \"footnotes\") |&gt;\n      unnest(value) |&gt;\n      pivot_wider(values_from = value, names_from = name)\n  ) |&gt;\n  list_rbind() |&gt;\n  rename(cpi = \"value\") |&gt;\n  mutate(month = match(periodName, month.name))\n\ncpi_base &lt;- cpi_data |&gt; filter(year %in% \"2023\", month %in% \"12\")\n\ncpi_data &lt;- cpi_data |&gt; filter(year %in% unique(food_away_comp_data$ref_yr))\n\ncpi_data |&gt; slice(1:10) |&gt; kable(booktabs = TRUE)\n\n\n\n\n\nyear\nperiod\nperiodName\ncpi\nmonth\n\n\n\n\n2021\nM12\nDecember\n278.802\n12\n\n\n2021\nM11\nNovember\n277.948\n11\n\n\n2021\nM10\nOctober\n276.589\n10\n\n\n2021\nM09\nSeptember\n274.310\n9\n\n\n2021\nM08\nAugust\n273.567\n8\n\n\n2021\nM07\nJuly\n273.003\n7\n\n\n2021\nM06\nJune\n271.696\n6\n\n\n2021\nM05\nMay\n269.195\n5\n\n\n2021\nM04\nApril\n267.054\n4\n\n\n2021\nM03\nMarch\n264.877\n3\n\n\n\n\n\nThe base that I’m going to covert to is December 2023.\n\n\nCode\ncpi_base |&gt; kable(booktabs = TRUE)\n\n\n\n\n\nyear\nperiod\nperiodName\ncpi\nmonth\n\n\n\n\n2023\nM12\nDecember\n306.746\n12\n\n\n\n\n\nNext I’m going to join the CPI data to the CE data and adjust the “cost” variable for inflation. Note that I replace resulting missing values in the “cost” variable with “0”. Missing values will result when I multiply a cost of “0” by an adjustment factor and ce_mean() will not function with missing values.\n\n\nCode\nfood_away_comp_data &lt;- food_away_comp_data |&gt;\n  left_join(\n    select(cpi_data, year, month, cpi),\n    by = c(\"ref_yr\" = \"year\", \"ref_mo\" = \"month\")\n  ) |&gt;\n  mutate(\n    base_cpi = pull(cpi_base, cpi),\n    across(c(base_cpi, cpi), as.numeric),\n    cost = cost * (base_cpi / cpi) |&gt; replace_na(0)\n  )\n\nfood_away_comp_data |&gt;\n  select(survey, year, newid, finlwt21, cost, ucc, ref_yr, ref_mo) |&gt;\n  filter(!is.na(ucc)) |&gt;\n  group_by(year, survey) |&gt;\n  slice_sample(n = 3) |&gt;\n  ungroup() |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nsurvey\nyear\nnewid\nfinlwt21\ncost\nucc\nref_yr\nref_mo\n\n\n\n\nD\n2010\n01062182\n31911.373\n632.33499\n190111\n2010\n4\n\n\nD\n2010\n01132291\n42762.166\n75.39493\n190312\n2010\n9\n\n\nD\n2010\n01151261\n34508.670\n71.78432\n190321\n2010\n11\n\n\nI\n2010\n02222433\n3456.019\n33.02286\n790430\n2010\n2\n\n\nI\n2010\n02202583\n14948.874\n233.51876\n190903\n2010\n2\n\n\nI\n2010\n02183355\n15016.399\n703.51037\n190903\n2010\n7\n\n\nD\n2020\n04433811\n74006.452\n63.35846\n190311\n2020\n2\n\n\nD\n2020\n04438592\n32749.351\n64.59171\n190211\n2020\n2\n\n\nD\n2020\n04636492\n41910.471\n306.47607\n190211\n2020\n11\n\n\nI\n2020\n04291913\n70403.081\n356.52248\n190903\n2020\n3\n\n\nI\n2020\n04386872\n20595.360\n1181.86902\n800700\n2020\n2\n\n\nI\n2020\n04683051\n33054.288\n280.67215\n800700\n2020\n12\n\n\n\n\n\nThe next step is to calculate means, for which I’ll use some more Tidyverse functions.\n\n\nCode\nfood_away_means &lt;- food_away_comp_data |&gt;\n  group_nest(year, fam_size_label, .key = \"data\") |&gt;\n  mutate(ce_mn_df = map(data, ce_mean)) |&gt;\n  select(-data) |&gt; \n  unnest(ce_mn_df) |&gt;\n  mutate(lower = mean_exp - cv, upper = mean_exp + cv)\n\nfood_away_means |&gt; kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nfam_size_label\nagg_exp\nmean_exp\nse\ncv\nlower\nupper\n\n\n\n\n2010\n1\n77335813000\n2207.926\n87.23315\n3.950909\n2203.975\n2211.877\n\n\n2010\n2\n137538943234\n3479.533\n102.54849\n2.947192\n3476.585\n3482.480\n\n\n2010\n3\n70833802054\n4028.171\n187.60152\n4.657238\n4023.514\n4032.829\n\n\n2010\n4\n78621552689\n4994.423\n196.31421\n3.930669\n4990.492\n4998.354\n\n\n2010\n5+\n60764779535\n4668.419\n252.13159\n5.400792\n4663.018\n4673.820\n\n\n2020\n1\n67173826416\n1713.043\n84.53129\n4.934569\n1708.108\n1717.977\n\n\n2020\n2\n122787510310\n2825.619\n131.55436\n4.655771\n2820.963\n2830.275\n\n\n2020\n3\n61184160903\n3170.855\n203.25085\n6.409971\n3164.445\n3177.265\n\n\n2020\n4\n68948170493\n4210.973\n262.49920\n6.233695\n4204.739\n4217.206\n\n\n2020\n5+\n48767026459\n3737.376\n327.18302\n8.754351\n3728.622\n3746.130\n\n\n\n\n\nPlotting these data would be pretty straightforward, as well.\n\n\nCode\nfood_away_means |&gt;\n  ggplot(aes(x = fam_size_label, y = mean_exp, fill = year, group = year)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.8) +\n  geom_errorbar(\n    aes(ymin = lower, ymax = upper),\n    width = 0.4,\n    position = position_dodge(0.75)\n  ) +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    title =\n      \"Estimated annual mean food away from home expenditures by CU size\",\n    x = \"CU size\",\n    y = \"Estimated, weighted, annual mean expenditure\",\n    fill = \"Year\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nHere we can see that on an inflation-adjusted basis, households of all sizes had higher expenditures on food away from home in 2010 than they did in 2020.\nNow I’ll generate a plot of the expenditures at each weighted, annual, estimated quantile (from 0.01 through 0.99, by 0.01) for the same years, but only using Diary data, since most of the UCCs (16 out of 21) in the “food away from home” category come from the Diary.\n\n\nCode\nfood_away_comp_quantiles &lt;- map2(\n  c(2010, 2020),\n  c(\n    file.path(ce_data_dir, \"diary10.zip\"),\n    file.path(ce_data_dir, \"diary20.zip\")\n  ),\n  \\(x, y) {\n    dia_hg &lt;- ce_hg(\n      x,\n      diary,\n      hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n    )\n    \n    food_uccs &lt;- ce_uccs(dia_hg, expenditure = \"Food away from home\")\n    \n    ce_prepdata(\n      x,\n      diary,\n      dia_hg,\n      food_uccs,\n      dia_zp = y\n    ) |&gt;\n      mutate(year = x, ref_yr = as.character(ref_yr))\n  }\n) |&gt;\n  list_rbind() |&gt;\n  left_join(\n    select(cpi_data, year, month, cpi),\n    by = c(\"ref_yr\" = \"year\", \"ref_mo\" = \"month\")\n  ) |&gt;\n  mutate(year = factor(year)) |&gt;\n  nest(data = -year) |&gt;\n  mutate(\n    fa_qtile = map(data, ce_quantiles, probs = c(seq(0, 0.95, by = 0.05), 0.99))\n  ) |&gt;\n  select(-data) |&gt;\n  unnest(fa_qtile) |&gt;\n  mutate(probs = parse_number(probs) / 100)\n\nfood_away_comp_quantiles |&gt;\n  ggplot(aes(x = probs, y = quantile, group = year, color = year)) +\n  geom_line() +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    title =\n      \"Estimated, annual food away from home expenditure quantiles\",\n    x = \"Quantile\",\n    y = \"Estimated, weighted, annual expenditure\",\n    color = \"Year\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nInterestingly the expenditures don’t appear to have changed much between 2010 and 2020 across quantiles on an inflation-adjusted basis, but we can see that across all quantiles, CU’s spent less in 2020 than they did in 2010 on food away from home, which is consistent with the means that we calculated above. There are a lot of 0-value reported expenditures, though, in the CE on food away from home. Unfortunately, I can’t perform an analysis using only respondents that did have expenditures in this category, i.e., dropping the 0’s, because whether someone had an expenditure on food away from home is not one of the variables used for generating the survey weights. In other words, the analysis can be done, but it would not be statistically valid and I definitely wouldn’t be able to infer from it. This is just another cautionary note to anyone using this package who might use it in a way that does not follow statistically sound practices. Please visit the CE’s website and read the CE PUMD Getting Started Guide for more information.\n\n\nDealing with inconsistent code definitions\nIn this workflow I’m going to calculate estimated mean utilities expenditures for 2015 using integrated data by CU composition using the FAM_TYPE variable. In this case I’m going to start by looking at the codes for that variable to show how one might run into an inconsistency in code definitions across survey instruments. First I’m going to set up a sub-directory in my temporary directory and store what I’ll need to get started.\nFirst, I’ll look at code descriptions for the “FAM_TYPE” variable in the dictionary and I’m going to focus on the code values of 3, 5, and 7. I still have the ce_codes object in memory, so I’ll just use that.\n\n\nCode\nce_codes |&gt;\n  janitor::clean_names() |&gt;\n  filter(\n    variable %in% \"FAM_TYPE\",\n  first_year &lt;= 2015,\n  (last_year &gt;= 2015 | is.na(last_year)),\n  code_value %in% c(\"3\", \"5\", \"7\")\n  ) |&gt;\n  select(survey, code_value, code_description) |&gt;\n  arrange(code_value, survey) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\nsurvey\ncode_value\ncode_description\n\n\n\n\nDIARY\n3\nMarried couple, own children only, oldest child &gt; 6, &lt; 18\n\n\nINTERVIEW\n3\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n\n\nDIARY\n5\nAll other Married couple families\n\n\nINTERVIEW\n5\nAll other Husband and wife families\n\n\nDIARY\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\nINTERVIEW\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\n\n\n\nThe code descriptions for these 3 code values are different across instruments. To resolve this I’m going to create a table containing only codes from the Interview survey.\n\n\nCode\nfam_type_codes &lt;- ce_codes |&gt;\n  janitor::clean_names() |&gt;\n  filter(\n    variable %in% \"FAM_TYPE\",\n    first_year &lt;= 2015,\n    (last_year &gt;= 2015 | is.na(last_year))\n  )\n\ncodes2keep &lt;- fam_type_codes |&gt;\n  filter(survey %in% \"INTERVIEW\") |&gt;\n  select(code_value, code_description)\n\nfam_type_codes &lt;- fam_type_codes |&gt;\n  select(-code_description) |&gt;\n  left_join(codes2keep, by = \"code_value\") |&gt;\n  relocate(code_description, .after = code_value)\n\nfam_type_codes |&gt;\n  filter(code_value %in% c(\"3\", \"5\", \"7\")) |&gt;\n  select(survey, code_value, code_description) |&gt;\n  arrange(code_value, survey) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\nsurvey\ncode_value\ncode_description\n\n\n\n\nDIARY\n3\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n\n\nINTERVIEW\n3\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n\n\nDIARY\n5\nAll other Husband and wife families\n\n\nINTERVIEW\n5\nAll other Husband and wife families\n\n\nDIARY\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\nINTERVIEW\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\n\n\n\nNow the codes are consistent across survey instruments and I can use this code-book in my call to ce_prepdata() using the “own_code-book” argument. Then I’ll pass that to ce_mean() per usual.\nNext I’ll get some information about how utilities expenditures are organized using the stub file.\n\n\nCode\ninteg15_hg &lt;- ce_hg(\n  2015,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\ninteg15_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"utilities\")) |&gt;\n  kable(bookmarks = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nUtilities, fuels, and public services\nUTILS\nG\n1\n\n\n\n\n\nThe expenditure category associated with utilities is “Utilities, fuels, and public services”. I’ll store that title to work with later and narrow down the section of the stub file that includes only these expenditures.\n\n\nCode\nutilities_title &lt;- integ15_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"utilities\")) |&gt;\n  pull(title)\n\nutilities_hg &lt;- ce_uccs(\n  integ15_hg,\n  expenditure = utilities_title,\n  uccs_only = FALSE\n)\n\nutilities_hg |&gt; kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nUtilities, fuels, and public services\nUTILS\nG\n1\n\n\n4\nNatural gas\nNATRLG\nG\n1\n\n\n5\nUtility-natural gas (renter)\n260211\nI\n1\n\n\n5\nUtility-natural gas (owned home)\n260212\nI\n1\n\n\n5\nUtility-natural gas (owned vacation)\n260213\nI\n1\n\n\n5\nUtility-natural gas (rented vacation)\n260214\nI\n1\n\n\n4\nElectricity\nELECTR\nG\n1\n\n\n5\nElectricity (renter)\n260111\nI\n1\n\n\n5\nElectricity (owned home)\n260112\nI\n1\n\n\n5\nElectricity (owned vacation)\n260113\nI\n1\n\n\n5\nElectricity (rented vacation)\n260114\nI\n1\n\n\n4\nFuel oil and other fuels\nOTHRFU\nG\n1\n\n\n5\nFuel oil\nFUELOI\nG\n1\n\n\n6\nFuel oil (renter)\n250111\nI\n1\n\n\n6\nFuel oil (owned home)\n250112\nI\n1\n\n\n6\nFuel oil (owned vacation)\n250113\nI\n1\n\n\n6\nFuel oil (rented vacation)\n250114\nI\n1\n\n\n5\nCoal, wood, and other fuels\nCLWDOT\nG\n1\n\n\n6\nCoal, wood, other fuels (renter)\n250911\nI\n1\n\n\n6\nCoal, wood, other fuels (owned home)\n250912\nI\n1\n\n\n6\nCoal, wood, other fuels (owned vacation)\n250913\nI\n1\n\n\n6\nCoal, wood, other fuels (rented vacation)\n250914\nI\n1\n\n\n5\nBottled gas\nBOTTLG\nG\n1\n\n\n6\nGas, btld/tank (renter)\n250211\nI\n1\n\n\n6\nGas, btld/tank (owned home)\n250212\nI\n1\n\n\n6\nGas, btld/tank (owned vacation)\n250213\nI\n1\n\n\n6\nGas, btld/tank (rented vacation)\n250214\nI\n1\n\n\n4\nTelephone services\nPHONE\nG\n1\n\n\n5\nResidential phone service, VOIP, and phone cards\nRESPHO\nG\n1\n\n\n6\nPhone cards\n270104\nI\n1\n\n\n6\nResidential telephone including VOIP\n270106\nI\n1\n\n\n5\nCellular phone service\n270102\nI\n1\n\n\n4\nWater and other public services\nWATER\nG\n1\n\n\n5\nWater and sewerage maintenance\nSEWER\nG\n1\n\n\n6\nWater/sewer maint. (renter)\n270211\nI\n1\n\n\n6\nWater/sewer maint. (owned home)\n270212\nI\n1\n\n\n6\nWater/sewer maint. (owned vacation)\n270213\nI\n1\n\n\n6\nWater/sewer maint. (rented vacation)\n270214\nI\n1\n\n\n5\nTrash and garbage collection\nTRASH\nG\n1\n\n\n6\nTrash/garb. coll. (renter)\n270411\nI\n1\n\n\n6\nTrash/garb. coll. (owned home)\n270412\nI\n1\n\n\n6\nTrash/garb. coll. (owned vacation)\n270413\nI\n1\n\n\n6\nTrash/garb. coll. (rented vacation)\n270414\nI\n1\n\n\n5\nSeptic tank cleaning\nSEPTAN\nG\n1\n\n\n6\nSeptic tank clean. (renter)\n270901\nI\n1\n\n\n6\nSeptic tank clean. (owned home)\n270902\nI\n1\n\n\n6\nSeptic tank clean. (owned vacation)\n270903\nI\n1\n\n\n6\nSeptic tank clean. (rented vacation)\n270904\nI\n1\n\n\n\n\n\nI also want to know what survey instruments the expenditures are collected through for published estimates. My stub file is the integrated stub file, so I should see both “I” and “D” in the survey column of the stub file if expenditures are collected through both instruments.\n\n\nCode\nutilities_hg |&gt; distinct(survey) |&gt; kable(booktabs = TRUE)\n\n\n\n\n\nsurvey\n\n\n\n\nG\n\n\nI\n\n\n\n\n\nIt seems utilities expenditures are collected only through the Interview survey, so I’ll only need to use Interview data files to calculate estimates.\n\n\nCode\nfam_type_utilities &lt;- ce_prepdata(\n  2015,\n  interview,\n  utilities_hg,\n  uccs = ce_uccs(utilities_hg, expenditure = utilities_title),\n  fam_type, \n  int_zp = file.path(ce_data_dir, \"intrvw15.zip\"),\n  recode_variables = TRUE,\n  dict_path = file.path(ce_data_dir, \"ce-data-dict.xlsx\")\n) |&gt;\n  group_nest(fam_type) |&gt;\n  mutate(ce_mean_df = map(data, ce_mean)) |&gt;\n  select(-data) |&gt;\n  unnest(ce_mean_df)\n\nfam_type_utilities |&gt;\n  arrange(fam_type) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nfam_type\nagg_exp\nmean_exp\nse\ncv\n\n\n\n\nMarried Couple only\n122623482952\n4378.377\n76.25699\n1.741673\n\n\nMarried Couple, own children only, oldest child &lt; 6\n21592090354\n4073.529\n241.48233\n5.928087\n\n\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n73899626502\n5136.781\n132.53306\n2.580080\n\n\nMarried Couple, own children only, oldest child &gt; 17\n53687574363\n5585.027\n190.15164\n3.404668\n\n\nAll other Husband and wife families\n26767356778\n5699.177\n331.83382\n5.822487\n\n\nOne parent, male, own children at least one age &lt; 18\n4647315390\n3887.863\n532.21229\n13.689069\n\n\nOne parent, female, own children, at least one age &lt; 18\n22862016917\n3684.480\n237.12762\n6.435851\n\n\nSingle consumers\n88002147354\n2348.182\n40.59815\n1.728919\n\n\nOther families\n84986387660\n3942.345\n122.97865\n3.119429\n\n\n\n\n\nAnd finally, a quick lollipop plot.\n\n\nCode\nfam_type_utilities |&gt;\n  mutate(fam_type = fct_reorder(fam_type, mean_exp)) |&gt;\n  ggplot(aes(x = mean_exp, y = fam_type, mean_exp)) +\n  geom_segment(aes(x = 0, xend = mean_exp, yend = fam_type)) +\n  geom_point(color = \"red\", size = 5) +\n  scale_y_discrete(labels = function(x) str_wrap(x, width = 25)) +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    y = \"CU composition (FAM_TYPE)\",\n    x = \"Estimated, weighted, annual mean expenditure\",\n    title =\n      \"Estimated annual mean utilities expenditures by CU composition\"\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#conclusion",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#conclusion",
    "title": "Introduction to cepumd",
    "section": "Conclusion",
    "text": "Conclusion\nThat wraps up this introduction to cepumd. Thank you for taking a look. If you find any bugs, please report them on the Github repo issues section."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "",
    "text": "Image by [Images Money](https://www.flickr.com/photos/59937401@N07/)\n\n\nU.S. housing prices have been all over the news in recent times, which comes as no surprise given recent concerns about inflation and interest rates. With this in mind I thought about trying to forecast a housing price index. Over the last year the folks at Business Science have been adding lots of tools to their suite of packages for time series analysis in R. One of the more recent packages is modeltime. When I learned about this package I wanted an excuse to use it. My curiosity about housing price trends presented the perfect opportunity.\nI’ve broken this project into four parts:\n\nGathering Data\nTime series analysis\nARIMA with global and iterative modeling processes\nGlobal forecasting models\nIterative forecasting models\n\nThis post will cover the first part: Gathering Data."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#introduction",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#introduction",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "",
    "text": "Image by [Images Money](https://www.flickr.com/photos/59937401@N07/)\n\n\nU.S. housing prices have been all over the news in recent times, which comes as no surprise given recent concerns about inflation and interest rates. With this in mind I thought about trying to forecast a housing price index. Over the last year the folks at Business Science have been adding lots of tools to their suite of packages for time series analysis in R. One of the more recent packages is modeltime. When I learned about this package I wanted an excuse to use it. My curiosity about housing price trends presented the perfect opportunity.\nI’ve broken this project into four parts:\n\nGathering Data\nTime series analysis\nARIMA with global and iterative modeling processes\nGlobal forecasting models\nIterative forecasting models\n\nThis post will cover the first part: Gathering Data."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#choosing-house-price-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#choosing-house-price-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Choosing house price data",
    "text": "Choosing house price data\nTo perform this analysis, obviously the first thing I would need is a data set. I wanted data that had sufficient observations with enough granularity to be able to identify seasonality and level changes. I also wanted to compare time series across different regions or cities of the U.S. This meant getting data that was available at a regional/metropolitan level that went back at least 10 years with very few missing values (ideally none). I looked at the data available through the Federal Reserve Bank of St. Louis which hosts one of the largest repositories of U.S. economic data available by using the fredr (Boysel and Vaughan 2021) package and narrowed my search for data series down to either median home prices or the Case-Shiller Home Price Index. I chose the Case-Shiller HPI because I could get the data at the level of metro area rather than just U.S. region for a longer period of time, though there is the trade-off that this index only accounts for existing home sales. For my purposes this wasn’t a problem.\nThe Case-Shiller HPI data was available through 2023, but population data (discussed below) was available only through 2022 and there were some other issues with data from before 2006, so I got data starting in January 2006 and ending in December 2022.\nOnce I chose a data source, the next step was to choose cities. I wanted to compare cities from different regions of the U.S. that were different in other characteristics as well, e.g., population, climate, etc., to see how forecasts might differ. I chose San Diego, Dallas, Detroit, and New York City."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#additional-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#additional-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Additional data",
    "text": "Additional data\nOne of the benefits of using modeltime it has functions for running models beyond ARIMA such as Prophet and eXtreme Gradient Boosting (XGB) for time series. To take advantage of this I wanted to add some features to my data. I chose to add unemployment rates, average gas prices, variables relating to demographic composition, and population for each city at the monthly level. I got the unemployment rate and average gas prices through fredr, population data from the U.S. Census’s American Community Survey (ACS) using the tidycensus (Walker and Herman 2024) package, and other demographic data from the Current Population Survey (CPS) (which is a joint program of the U.S. Census and the U.S. Bureau of Labor Statistics) using the cpsR (Saenz 2023) package.\n\n\n\n\n\n\nImportant\n\n\n\nBoth tidycensus and cpsR require a U.S. Census API key. This post will not go into detail on how to obtain a key or how to set it up for use with the relevant packages.\n\n\n\n\n\n\n\n\nStatistical Area Definitions\n\n\n\n\n\nSome of these data are at the metropolitan statistical area (MSA) level and others are at the core-based statistical area (CBSA) level. While these broadly overlap, which is why I didn’t make a big fuss about mixing them, they are technically different. The U.S. Census provides some definitions here.\n\n\n\nAs usual for me, I performed data importing and wrangling using various tidyverse (Wickham et al. 2019)package functions. I also used multidplyr (Wickham 2023) to perform some operations in parallel."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#loading-packages",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#loading-packages",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Loading Packages",
    "text": "Loading Packages\nTo begin with, I first loaded my packages. I’ll include the gt package for displaying tables.\n\n\nCode\nlibrary(tidyverse)\nlibrary(fredr)\nlibrary(tidycensus)\nlibrary(cpsR)\nlibrary(gt)\nlibrary(multidplyr)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")\nconflicted::conflict_prefer(\"between\", \"dplyr\")"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-case-shiller-hpi-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-case-shiller-hpi-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting Case-Shiller HPI data",
    "text": "Getting Case-Shiller HPI data\nI started by getting the appropriate Case-Shiller HPI series IDs for my cities of interest.\n\n\nCode\nhpi_ids &lt;- fredr_series_search_text(\"Case-Shiller\") |&gt;\n  filter(\n    str_detect(\n      title,\n      \"CA-San Diego|TX-Dallas|NY-New York|MI-Detroit Home Price Index\"\n    ),\n    seasonal_adjustment_short %in% \"NSA\",\n    frequency %in% \"Monthly\"\n  ) |&gt;\n  mutate(city = str_extract(title, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  select(city, id)\n\ngt(hpi_ids)\n\n\n\n\n\n\n\n\ncity\nid\n\n\n\n\nDallas\nDAXRNSA\n\n\nSan Diego\nSDXRNSA\n\n\nNew York\nNYXRNSA\n\n\nDetroit\nDEXRNSA\n\n\n\n\n\n\n\nNext I read in the Case-Shiller data and kept only the necessary variables.\n\n\nCode\nhpi_data &lt;- hpi_ids |&gt;\n  mutate(\n    data = map(\n      id,\n      \\(x) fredr(\n        series_id = x,\n        observation_start = ymd(\"2006-01-01\"),\n        observation_end = ymd(\"2023-12-31\"),\n        frequency = \"m\"\n      )\n    )\n  ) |&gt;\n  select(-id) |&gt;\n  unnest(data) |&gt;\n  select(city, date, hpi = value)\n\ngt(hpi_data[1:10, ])\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\n\n\n\n\nDallas\n2006-01-01\n121.9108\n\n\nDallas\n2006-02-01\n121.3285\n\n\nDallas\n2006-03-01\n121.5217\n\n\nDallas\n2006-04-01\n122.3996\n\n\nDallas\n2006-05-01\n123.2863\n\n\nDallas\n2006-06-01\n124.4985\n\n\nDallas\n2006-07-01\n125.3672\n\n\nDallas\n2006-08-01\n125.7007\n\n\nDallas\n2006-09-01\n125.1859\n\n\nDallas\n2006-10-01\n124.5813"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-unemployment-rate-and-gas-price-data-from-fred",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-unemployment-rate-and-gas-price-data-from-fred",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting unemployment rate and gas price data from FRED",
    "text": "Getting unemployment rate and gas price data from FRED\nNext I got series IDs for both average gas prices and unemployment rates for all four cities and got the data for the appropriate time span.\n\n\nCode\n# Get average gas price for each CBSA by month\ngas_ids &lt;- fredr_series_search_text(\n  \"Average Price: Gasoline, Unleaded Regular\"\n) |&gt;\n  filter(\n    str_detect(title, \"New York|Dallas|San Diego|Detroit\"),\n    frequency %in% \"Monthly\",\n    seasonal_adjustment_short %in% \"NSA\"\n  ) |&gt;\n  mutate(city = str_extract(title, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  select(city, id)\n\ngas_data &lt;- gas_ids |&gt;\n  mutate(\n    data = map(\n      id,\n      \\(x) fredr(\n        series_id = x,\n        observation_start = ymd(\"2000-01-01\"),\n        observation_end = ymd(\"2023-12-31\"),\n        frequency = \"m\"\n      )\n    )\n  ) |&gt;\n  select(-id) |&gt;\n  unnest(data) |&gt;\n  select(city, date, avg_gas_price = value)\n\n# Get average unemployment rate for each MSA by month\nunemp_ids &lt;- fredr_series_search_text(\"Unemployment: Rate\") |&gt;\n  filter(\n    str_detect(title, \"New York|Dallas|San Diego|Detroit\"),\n    frequency %in% \"Monthly\",\n    seasonal_adjustment_short %in% \"NSA\"\n  ) |&gt;\n  mutate(city = str_extract(title, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  group_by(city) |&gt;\n  mutate(\n    rnum = n(),\n    msa_str = str_detect(title, \"MSA\")\n  ) |&gt;\n  ungroup() |&gt;\n  filter(rnum == 1 | msa_str) |&gt;\n  select(city, id)\n\nunemp_data &lt;- unemp_ids |&gt;\n  mutate(\n    data = map(\n      id,\n      \\(x) fredr(\n        series_id = x,\n        observation_start = ymd(\"2000-01-01\"),\n        observation_end = ymd(\"2023-12-31\"),\n        frequency = \"m\"\n      )\n    )\n  ) |&gt;\n  select(-id) |&gt;\n  unnest(data) |&gt;\n  select(city, date, unemp_rate = value)"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-cps-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-cps-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting CPS data",
    "text": "Getting CPS data\nGetting CPS data at the MSA level is a little more complicated than it is getting data series from FRED. CPS data can contain well over 100K rows for each month because the microdata are released at the household level and each data set has hundreds of variables. It also requires filtering and grouping the data by MSA code, which means getting the MSA codes that correspond to each MSA in my analysis. However, I knew that MSAs can change over time as population shifts. I felt confident that this wouldn’t be the case for my chosen cities because of their population sizes, but I wanted to check anyway. Finally, I did thought that pulling in all of that data at once then filtering and aggregating would be computationally inefficient and slow, so, instead, I wrote a function to perform all of the necessary operations and ran it in parallel. I chose the method that I did because most of the time was spent connecting to the CPS API. Once data are imported, the computations are fairly simple.\nThe first step was to get the MSA codes that corresponded to my cities of interest and check that I had a unique set across all of my years of interest. The data for these MSA codes came from the U.S. Census’s technical documentation page and MSA definitions only change once every few years.\n\n\nCode\nmsa_code_tbl &lt;- tribble(\n  ~start_yr, ~end_yr,\n  2003, 2006,\n  2007, 2011,\n  2012, 2016,\n  2017, 2021\n) |&gt;\n  mutate(\n    suffix = str_sub(start_yr, 3, 4),\n    msa_url = map_chr(\n      suffix,\n      \\(x) str_c(\n        \"https://www2.census.gov/programs-surveys/cbp/technical-documentation/\",\n        \"reference/metro-area-geography-reference/msa_county_reference\",\n        x,\n        \".txt\"\n      )\n    ),\n    msa_code_data = map(\n      msa_url,\n      \\(x) read_delim(x, delim = \",\", show_col_types = FALSE) |&gt;\n        filter(\n          str_detect(\n            name_msa,\n            \"Dallas.*TX|San Diego.*CA|New York.*NY|Detroit.*MI\"\n          )\n        ) |&gt;\n        mutate(\n          city = str_extract(name_msa, \"Dallas|New York|San Diego|Detroit\"),\n          msa = as.character(msa)\n        ) |&gt;\n        distinct(city, msa)\n    )\n  ) |&gt;\n  distinct(msa_code_data) |&gt;\n  unnest(msa_code_data)\n\ngt(msa_code_tbl)\n\n\n\n\n\n\n\n\ncity\nmsa\n\n\n\n\nDallas\n19100\n\n\nDetroit\n19820\n\n\nNew York\n35620\n\n\nSan Diego\n41740\n\n\n\n\n\n\n\nIn the next step I defined the function for importing, filtering, and aggregating CPS data.\n\n\nCode\nget_cps_summaries &lt;- function(cps_df, geographies) {\n  cps_df |&gt;\n    rename(hispanic = matches(\"hspnon\"), msa = starts_with(\"gt\")) |&gt;\n    filter(msa %in% geographies) |&gt;\n    relocate(msa, pwsswgt, prtage, pemaritl, peeduca, hispanic) |&gt;\n    mutate(\n      msa = as.character(msa),\n      pwsswgt = as.numeric(pwsswgt),\n      across(prtage:hispanic, as.integer),\n      across(prtage:hispanic, \\(x) na_if(x, -1)),\n      age_lt_18 = prtage &lt; 18,\n      age_18_35 = between(prtage, 18, 35),\n      age_36_65 = between(prtage, 36, 65),\n      age_gt_65 = prtage &gt; 65,\n      educ_hs_less = peeduca &lt; 40,\n      educ_college = peeduca %in% 40:43,\n      educ_grad = peeduca &gt; 43,\n      status_married = pemaritl %in% 1:2,\n      status_nev_mar = pemaritl == 6,\n      status_other = !pemaritl %in% c(1:2, 6),\n      hispanic = hispanic == 1,\n      across(\n        c(age_lt_18:status_other, hispanic),\n        \\(x) (x * pwsswgt)\n      )\n    ) |&gt;\n    group_by(msa) |&gt;\n    summarise(\n      across(\n        c(age_lt_18:status_other, hispanic),\n        \\(x) sum(x / sum(pwsswgt), na.rm = TRUE)\n      )\n    )\n}\n\n\nWith that function now defined, I move on to get CPS data using that function.\n\n\nCode\n# Set up a cluster to get all of the CPS data aggregations\ncps_clust &lt;- new_cluster(12)\ncluster_library(cps_clust, c(\"tidyverse\", \"cpsR\"))\ncluster_copy(cps_clust, c(\"get_cps_summaries\", \"msa_code_tbl\"))\n\n# Get CPS data aggregations\ncps_grid &lt;- expand.grid(\n  yr = 2006:2023,\n  mo = 1:12\n) |&gt;\n  partition(cps_clust)\n\ncps_data &lt;- cps_grid |&gt;\n  arrange(yr, mo) |&gt;\n  mutate(\n    cps = map2(\n      yr, mo,\n      possibly(\n        \\(y, m) {\n          get_basic(\n            y, m,\n            c(\"gtcbsa\", \"prtage\", \"pemaritl\", \"peeduca\", \"pehspnon\", \"pwsswgt\")\n          ) |&gt;\n            get_cps_summaries(msa_code_tbl$msa)\n        }\n      )\n    )\n  ) |&gt;\n  collect() |&gt;\n  unnest(cps) |&gt;\n  mutate(msa = as.character(msa)) |&gt;\n  left_join(msa_code_tbl, by = \"msa\") |&gt;\n  select(-msa) |&gt;\n  unite(\"date\", yr, mo, sep = \"-\") |&gt;\n  mutate(date = ym(date)) |&gt;\n  relocate(city, date)\n\n\nThis is what the CPS data looked like.\n\n\nCode\ngt(head(cps_data))\n\n\n\n\n\n\n\n\ncity\ndate\nage_lt_18\nage_18_35\nage_36_65\nage_gt_65\neduc_hs_less\neduc_college\neduc_grad\nstatus_married\nstatus_nev_mar\nstatus_other\nhispanic\n\n\n\n\nDallas\n2006-01-01\n0.2891927\n0.2805833\n0.3565649\n0.07365909\n0.3590709\n0.3335333\n0.06725694\n0.4250032\n0.2129236\n0.3620733\n0.26143808\n\n\nDetroit\n2006-01-01\n0.2702241\n0.2240328\n0.3916348\n0.11410831\n0.3705743\n0.3349701\n0.06632826\n0.3994205\n0.2335969\n0.3669827\n0.03511936\n\n\nNew York\n2006-01-01\n0.2499559\n0.2424744\n0.3919559\n0.11561378\n0.3735838\n0.3262784\n0.09740889\n0.4059393\n0.2664301\n0.3276306\n0.21865626\n\n\nSan Diego\n2006-01-01\n0.2391931\n0.2444161\n0.4007565\n0.11563422\n0.2952627\n0.4322152\n0.08048637\n0.4026175\n0.2536992\n0.3436833\n0.28814238\n\n\nDallas\n2006-03-01\n0.2991726\n0.2852235\n0.3324601\n0.08314374\n0.3481381\n0.3412725\n0.06266023\n0.4069059\n0.2147862\n0.3783079\n0.27328065\n\n\nDetroit\n2006-03-01\n0.2660104\n0.2155320\n0.4070590\n0.11139867\n0.3738536\n0.3548189\n0.05050216\n0.4191846\n0.2244699\n0.3563455\n0.03402642"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-population-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-population-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting population data",
    "text": "Getting population data\nOne would think that if any demographic data is simple to get, population data would be simple, right? I learned something different. I was interested in getting population data at a monthly level for each MSA of interest. I knew population was not counted monthly, but I thought I could get it from the CPS. In fact, one might notice that the code actually calls a population variable (pwsswgt ) to perform some calculations. The problem here was that at the MSA level population estimates varied in some cases by as much as 16%. While I I didn’t know if this was true or not I did believe (subjectively) that it was unlikely and I believed that CPS weights are not calibrated to MSAs. My understanding is that they are calibrated to the national and state levels. So, I thought a reasonable alternative would be to get data from the ACS 1-year survey and interpolate data from one year to the next. In fact, the reason that I started my time series in 2006 is that the ACS 1-year and 5-year survey data became available in 2005 and I needed one year before the time series for interpolation. However, there was wrinkle caused by COVID. The 1-year ACS was not released for 2020 because of understandable challenges with data collection. So, for 2020 I used ACS 5-year survey data. I started by getting all of the ACS 1-year data. Here again, I processed the data in parallel.\n\n\nCode\n# Create a cluster for getting ACS data\nacs_clust &lt;- new_cluster(12)\ncluster_library(acs_clust, c(\"tidyverse\", \"tidycensus\"))\n\n# Get ACS data for 2005 to 2019, 2021, and 2022\nacs1_pop_data &lt;- tibble(year = c(2005:2019, 2021:2022)) |&gt;\n  partition(acs_clust) |&gt;\n  mutate(\n    pop_data = map(\n      year,\n      \\(x) get_acs(\n        year = x,\n        variables = \"B01001_001\",\n        geography = \"cbsa\",\n        survey = \"acs1\",\n        show_call = FALSE\n      ) |&gt;\n        filter(str_detect(NAME, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n        mutate(city = str_extract(NAME, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n        select(city, population = estimate)\n    )\n  ) |&gt;\n  collect() |&gt;\n  unnest(pop_data) |&gt;\n  mutate(date = str_c(year, \"12\") |&gt; ym(), .keep = \"unused\")\n\n\nNext I got the data for 2020 and combined it with the ACS 1-year data. I performed interpolation using the ts_impute_vec() function from the timetk (Dancho and Vaughan 2023) package.\n\n\nCode\n# Get data for 2020 separately since it wasn't available in the ACS 1-year\nacs5_pop_data &lt;- get_acs(\n  year = 2020,\n  variables = \"B01001_001\",\n  geography = \"cbsa\",\n  survey = \"acs5\",\n  show_call = FALSE\n) |&gt;\n  filter(str_detect(NAME, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  mutate(\n    city = str_extract(NAME, \"New York|Dallas|San Diego|Detroit\"),\n    date = rep(ym(\"2020-12\"), 4),\n  ) |&gt;\n  select(city, date, population = estimate)\n\n\nGetting data from the 2016-2020 5-year ACS\n\n\nCode\n# Put together all the ACS population data and interpolate between annual\n# estimates\nacs_pop_data &lt;- expand_grid(\n  yr = 2006:2022,\n  mo = 1:12\n) |&gt;\n  add_row(yr = 2005, mo = 12) |&gt;\n  expand_grid(city = c(\"New York\", \"Dallas\", \"San Diego\", \"Detroit\")) |&gt;\n  unite(date, yr, mo, sep = \"-\", remove = TRUE) |&gt;\n  mutate(date = ym(date)) |&gt;\n  arrange(date, city) |&gt;\n  left_join(\n    bind_rows(acs1_pop_data, acs5_pop_data),\n    by = c(\"city\", \"date\")\n  ) |&gt;\n  group_by(city) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    population = timetk::ts_impute_vec(population, period = 1),\n    pop_chg = (population / lag(population)) - 1\n  ) |&gt;\n  drop_na(pop_chg) |&gt;\n  ungroup() |&gt;\n  select(!population)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\ngt(head(acs_pop_data))\n\n\n\n\n\n\n\n\ndate\ncity\npop_chg\n\n\n\n\n2006-01-01\nDallas\n0.0040551186\n\n\n2006-01-01\nDetroit\n0.0007530958\n\n\n2006-01-01\nNew York\n0.0021226567\n\n\n2006-01-01\nSan Diego\n0.0034579867\n\n\n2006-02-01\nDallas\n0.0040387411\n\n\n2006-02-01\nDetroit\n0.0007525290"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#merge-data-and-check",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#merge-data-and-check",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Merge data and check",
    "text": "Merge data and check\nWith all of the data gathered and each data set containing variables for city and date (monthly) along with at least one demographic or economic variable, they were ready to be merged.\n\n\nCode\necon_data &lt;- list(hpi_data, unemp_data, gas_data, cps_data, acs_pop_data) |&gt;\n  reduce(inner_join, by = c(\"city\", \"date\"))\n\n\nIt would be a mistake to ever think that there is no more cleaning that can be done, so I skimmed the data using the skimr() function from the skimr (Waring et al. 2022) package.\n\n\nCode\nskimr::skim(econ_data)\n\n\n\nData summary\n\n\nName\necon_data\n\n\nNumber of rows\n816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2006-01-01\n2022-12-01\n2014-06-16\n204\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhpi\n0\n1.00\n172.19\n64.38\n64.47\n122.40\n168.06\n203.87\n427.80\n▆▇▃▁▁\n\n\nunemp_rate\n0\n1.00\n6.50\n2.93\n2.80\n4.20\n5.60\n8.30\n23.90\n▇▃▁▁▁\n\n\navg_gas_price\n144\n0.82\n2.95\n0.76\n1.36\n2.38\n2.84\n3.48\n6.29\n▅▇▅▁▁\n\n\nage_lt_18\n0\n1.00\n0.24\n0.02\n0.19\n0.22\n0.24\n0.25\n0.30\n▂▇▇▅▂\n\n\nage_18_35\n0\n1.00\n0.25\n0.02\n0.20\n0.24\n0.25\n0.26\n0.31\n▁▃▇▃▁\n\n\nage_36_65\n0\n1.00\n0.39\n0.02\n0.33\n0.38\n0.39\n0.40\n0.44\n▁▃▇▂▁\n\n\nage_gt_65\n0\n1.00\n0.12\n0.02\n0.07\n0.11\n0.12\n0.14\n0.19\n▃▆▇▅▁\n\n\neduc_hs_less\n0\n1.00\n0.33\n0.03\n0.24\n0.31\n0.33\n0.35\n0.40\n▁▃▇▆▂\n\n\neduc_college\n0\n1.00\n0.38\n0.03\n0.31\n0.36\n0.38\n0.40\n0.48\n▂▇▆▃▁\n\n\neduc_grad\n0\n1.00\n0.09\n0.02\n0.05\n0.08\n0.09\n0.11\n0.16\n▃▇▇▅▁\n\n\nstatus_married\n0\n1.00\n0.40\n0.02\n0.36\n0.39\n0.40\n0.41\n0.45\n▁▆▇▅▁\n\n\nstatus_nev_mar\n0\n1.00\n0.27\n0.03\n0.19\n0.25\n0.27\n0.29\n0.34\n▂▅▇▇▂\n\n\nstatus_other\n0\n1.00\n0.33\n0.02\n0.28\n0.32\n0.33\n0.34\n0.39\n▂▇▇▃▁\n\n\nhispanic\n0\n1.00\n0.22\n0.11\n0.02\n0.17\n0.25\n0.30\n0.42\n▅▁▆▇▂\n\n\npop_chg\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▂▇▃▁\n\n\n\n\n\nOverall the data look good except for the avg_gas_price variable, which is the only variable that has any missing values. I’ll turn to that shortly, but I’ll briefly describe the variables in the data set below:\n\ncity : self-explanatory\ndate: month expressed as the first day of the month\nhpi: Case-Shiller Home Price Index value\nunemp_rate: Unemployment rate\navg_gas_price: Average gas price\nage_lt_18: Proportion of the population younger than 18 years old\nage_18_35: Proportion of the population between the ages of 18 and 35\nage_36_65: Proportion of the population between the ages of 36 and 65\nage_gt_65: Proportion of the population older than 65 years old\neduc_hs_less: Proportion of the population with up to a high school diploma\neduc_college: Proportion of the population with more than a high school diploma and up to a bachelor’s degree\neduc_grad: Proportion of the population with education higher than a bachelor’s degree\nstatus_married: Proportion of the population that is married; including separated\nstatus_nev_mar: Proportion of the population that has never been married\nstatus_other: Proportion of the population not categorized as either status_married or status_nev_mar\nhispanic: Proportion of the population that reported Hispanic ethnicity\npop_chg: Population change from one month to the next\n\nOnce I skimmed the data, I took a closer look at the percent of avg_gas_price values that were missing for each city.\n\n\nCode\necon_data |&gt;\n  group_by(city) |&gt;\n  summarise(pct_miss_gas = sum(is.na(avg_gas_price)) / n()) |&gt;\n  ggplot(aes(y = pct_miss_gas, x = city)) +\n  geom_col(fill = \"#2c3e50\") +\n  scale_y_continuous(limits = c(-0.1, 1.1), labels = scales::percent) +\n  labs(\n    y = \"Percent Missing\",\n    x = NULL,\n    title = \"Missingness in avg_gas_price variable by city\"\n  ) + \n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nEvidently data were missing only for San Diego. Below are the start and end dates of average gas price data for San Diego in this data set.\n\n\nCode\necon_data |&gt;\n  filter(city %in% \"San Diego\", !is.na(avg_gas_price)) |&gt;\n  summarise(start_date = min(date), end_date = max(date)) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nstart_date\nend_date\n\n\n\n\n2018-01-01\n2022-12-01\n\n\n\n\n\n\n\nAs it turns out, reports of average gas price data for San Diego start in January of 2018, which means that 12 years of data would be missing. With that many values missing for one of the cities it makes sense to drop the average gas price variable.\n\n\nCode\necon_data &lt;- select(econ_data, !avg_gas_price)\n\necon_data |&gt; slice(1:6) |&gt; gt()\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\nunemp_rate\nage_lt_18\nage_18_35\nage_36_65\nage_gt_65\neduc_hs_less\neduc_college\neduc_grad\nstatus_married\nstatus_nev_mar\nstatus_other\nhispanic\npop_chg\n\n\n\n\nDallas\n2006-01-01\n121.9108\n5.1\n0.2891927\n0.2805833\n0.3565649\n0.07365909\n0.3590709\n0.3335333\n0.06725694\n0.4250032\n0.2129236\n0.3620733\n0.2614381\n0.004055119\n\n\nDallas\n2006-02-01\n121.3285\n5.2\n0.2849980\n0.2793908\n0.3506100\n0.08500122\n0.3512125\n0.3498026\n0.05843764\n0.4175663\n0.2206110\n0.3618227\n0.2618064\n0.004038741\n\n\nDallas\n2006-03-01\n121.5217\n5.0\n0.2991726\n0.2852235\n0.3324601\n0.08314374\n0.3481381\n0.3412725\n0.06266023\n0.4069059\n0.2147862\n0.3783079\n0.2732806\n0.004022495\n\n\nDallas\n2006-04-01\n122.3996\n4.8\n0.2938252\n0.2784571\n0.3445776\n0.08314010\n0.3324701\n0.3589532\n0.06155738\n0.3932068\n0.2133219\n0.3934713\n0.2540514\n0.004006380\n\n\nDallas\n2006-05-01\n123.2863\n4.9\n0.2744636\n0.2824573\n0.3631222\n0.07995692\n0.3610289\n0.3519889\n0.05978182\n0.4005990\n0.2167855\n0.3826155\n0.2722199\n0.003990393\n\n\nDallas\n2006-06-01\n124.4985\n5.3\n0.2739175\n0.2861816\n0.3643186\n0.07558240\n0.3512415\n0.3512756\n0.06253083\n0.4065603\n0.2122996\n0.3811401\n0.2734042\n0.003974533\n\n\n\n\n\n\n\nFinally, I’d like to visualize the densities of all of the variables by city just to make sure nothing jumps out that might need any correction.\n\n\nCode\necon_data |&gt;\n  select(-date) |&gt;\n  pivot_longer(-city, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = city)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(\"variable\", scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThere are some interesting patterns, particularly in the differences across the age groups, but there’s nothing alarming. With all this data in hand, the next step is to perform some time series analysis on the Case-Shiller HPI which I’ll show in Part 2 of this set of posts."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects in Data Science and Statistics",
    "section": "",
    "text": "Forecasting with {modeltime} - Part II\n\n\nTime series analysis\n\n\n\ntime series\n\n\neconomic data\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting with {modeltime} - Part I\n\n\nGathering data\n\n\n\ntime series\n\n\neconomic data\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Modeling with R\n\n\n\n\n\n\ntext analysis\n\n\ntopic modeling\n\n\ndata science\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to cepumd\n\n\n\n\n\n\npackage development\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Threshold vs. Upsampling\n\n\nDealing with Unbalanced Data\n\n\n\nmachine learning\n\n\ndata science\n\n\nclassification\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nOpening the ML ‘Black Box’ with Shapley Values\n\n\nA Tidy Approach\n\n\n\ndata science\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nAug 29, 2021\n\n\nArcenis Rojas\n\n\n\n\n\n\nNo matching items"
  }
]