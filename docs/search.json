[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Green, D., Melzer, B. T., Parker, J. A., & Rojas, A. (2020). Accelerator or Brake? Cash for Clunkers, Household Liquidity, and Aggregate Demand. American Economic Journal: Economic Policy, 12(4), 178–211. https://www.aeaweb.org/articles?id=10.1257/pol.20170122\nHubener, E., Rojas, A., & Tseng, N. (2018). Tradeoffs in the expenditure patterns of families with children. Beyond the Numbers: Prices & Spending (U.S. Bureau of Labor Statistics), 7(11). https://www.bls.gov/opub/btn/volume-7/tradeoffs-in-the-expenditure-patterns-of-families-with-children.htm\nFoster, A.C., Rojas, A. (2018). Program participation and spending patterns of families receiving government means-tested assistance. Monthly Labor Review (U.S. Bureau of Labor Statistics). https://doi.org/10.21916/mlr.2018.3"
  },
  {
    "objectID": "publications.html#papers",
    "href": "publications.html#papers",
    "title": "Publications",
    "section": "",
    "text": "Green, D., Melzer, B. T., Parker, J. A., & Rojas, A. (2020). Accelerator or Brake? Cash for Clunkers, Household Liquidity, and Aggregate Demand. American Economic Journal: Economic Policy, 12(4), 178–211. https://www.aeaweb.org/articles?id=10.1257/pol.20170122\nHubener, E., Rojas, A., & Tseng, N. (2018). Tradeoffs in the expenditure patterns of families with children. Beyond the Numbers: Prices & Spending (U.S. Bureau of Labor Statistics), 7(11). https://www.bls.gov/opub/btn/volume-7/tradeoffs-in-the-expenditure-patterns-of-families-with-children.htm\nFoster, A.C., Rojas, A. (2018). Program participation and spending patterns of families receiving government means-tested assistance. Monthly Labor Review (U.S. Bureau of Labor Statistics). https://doi.org/10.21916/mlr.2018.3"
  },
  {
    "objectID": "publications.html#presentations",
    "href": "publications.html#presentations",
    "title": "Publications",
    "section": "Presentations",
    "text": "Presentations\nTan, L., Rojas, A. (2019). Exploring the Application of Machine Learning Techniques to Construct R-indicators. American Association for Public Opinion Research Conference. https://www.bls.gov/cex/research_papers/pdf/rojas-r-indicators-aapor-presentation.pdf\nPaulin, G., Rojas, A., Taylor, W. (2018). Expenditures with Policy Implications: Results from the Consumer Expenditure Surveys on Housing, Alcohol, Tobacco, and Gambling. American Council on Consumer Interests. https://www.consumerinterests.org/assets/docs/CIA/CIA2018/Paulin%281%29CIA2018.pdf\nGillman, D., Hubener, E., Noel, R., Rigg, B., Rojas, A., Tan, L., Wilson, T. (2017). Documenting the Consumer Expenditure Surveys with DDI and Colectica – An Update. North American Data Documentation Initiative. https://doi.org/10.5281/zenodo.556223\nGillman, D., Hubener, E., Noel, R., Rigg, B., Rojas, A., Tan, L., Wilson, T. (2017). A Complex Use Case - Documenting the Consumer Expenditure Survey at BLS. North American Data Documentation Initiative. https://doi.org/10.5281/zenodo.556223"
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "",
    "text": "Up to this point in the project I acquired data in Forecasting with {modeltime} - Part I, performed some standard time series analysis in Forecasting with {modeltime} - Part II, and explored the two types of modeling processes – global and iterative – for multiple time series in Forecasting with {modeltime} - Part III. In this post I’m going to tune multiple models using a global process to select a small number of them that I will run iteratively in the final post. I’ll incorporate some of the other variables in the data set that I gathered in the the first post to forecast the Case-Shiller Home Price Index (HPI). Since I’ll be creating variations of model specifications, I’m also going to use functions from the Tidymodels workflowsets (Kuhn and Couch 2023) package to run the models. As before I’ll be using modeltime (Dancho 2023), timetk (Dancho and Vaughan 2023), tidymodels (Kuhn and Wickham 2020), tidyverse (Wickham et al. 2019), and gt (Iannone et al. 2024). Before jumping in, below I show a glimpse of the data that I’ll be using.\n\n\nCode\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(tidymodels)\nlibrary(gt)\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")\ntidymodels_prefer()\n\n\n\n\nCode\nskimr::skim(econ_data)\n\n\n\nData summary\n\n\nName\necon_data\n\n\nNumber of rows\n816\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2006-01-01\n2022-12-01\n2014-06-16\n204\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhpi\n0\n1\n172.19\n64.38\n64.47\n122.40\n168.06\n203.87\n427.80\n▆▇▃▁▁\n\n\nunemp_rate\n0\n1\n6.50\n2.93\n2.80\n4.20\n5.60\n8.30\n23.90\n▇▃▁▁▁\n\n\nage_lt_18\n0\n1\n0.24\n0.02\n0.19\n0.22\n0.24\n0.25\n0.30\n▂▇▇▅▂\n\n\nage_18_35\n0\n1\n0.25\n0.02\n0.20\n0.24\n0.25\n0.26\n0.31\n▁▃▇▃▁\n\n\nage_36_65\n0\n1\n0.39\n0.02\n0.33\n0.38\n0.39\n0.40\n0.44\n▁▃▇▂▁\n\n\nage_gt_65\n0\n1\n0.12\n0.02\n0.07\n0.11\n0.12\n0.14\n0.19\n▃▆▇▅▁\n\n\neduc_hs_less\n0\n1\n0.33\n0.03\n0.24\n0.31\n0.33\n0.35\n0.40\n▁▃▇▇▃\n\n\neduc_college\n0\n1\n0.38\n0.03\n0.31\n0.36\n0.38\n0.40\n0.48\n▂▇▆▃▁\n\n\neduc_grad\n0\n1\n0.10\n0.02\n0.05\n0.08\n0.10\n0.11\n0.16\n▃▇▇▅▁\n\n\nstatus_married\n0\n1\n0.40\n0.02\n0.32\n0.39\n0.40\n0.41\n0.45\n▁▁▇▇▂\n\n\nstatus_nev_mar\n0\n1\n0.27\n0.03\n0.19\n0.25\n0.27\n0.29\n0.34\n▂▅▇▇▂\n\n\nstatus_other\n0\n1\n0.33\n0.02\n0.28\n0.32\n0.33\n0.34\n0.45\n▃▇▃▁▁\n\n\nhispanic\n0\n1\n0.22\n0.11\n0.02\n0.17\n0.25\n0.30\n0.42\n▅▁▆▇▂\n\n\npopulation\n0\n1\n8471150.57\n6499655.47\n2834025.25\n4050215.00\n5109791.12\n10555276.77\n20320876.00\n▇▃▁▁▃"
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#review",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#review",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "",
    "text": "Up to this point in the project I acquired data in Forecasting with {modeltime} - Part I, performed some standard time series analysis in Forecasting with {modeltime} - Part II, and explored the two types of modeling processes – global and iterative – for multiple time series in Forecasting with {modeltime} - Part III. In this post I’m going to tune multiple models using a global process to select a small number of them that I will run iteratively in the final post. I’ll incorporate some of the other variables in the data set that I gathered in the the first post to forecast the Case-Shiller Home Price Index (HPI). Since I’ll be creating variations of model specifications, I’m also going to use functions from the Tidymodels workflowsets (Kuhn and Couch 2023) package to run the models. As before I’ll be using modeltime (Dancho 2023), timetk (Dancho and Vaughan 2023), tidymodels (Kuhn and Wickham 2020), tidyverse (Wickham et al. 2019), and gt (Iannone et al. 2024). Before jumping in, below I show a glimpse of the data that I’ll be using.\n\n\nCode\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(tidymodels)\nlibrary(gt)\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")\ntidymodels_prefer()\n\n\n\n\nCode\nskimr::skim(econ_data)\n\n\n\nData summary\n\n\nName\necon_data\n\n\nNumber of rows\n816\n\n\nNumber of columns\n16\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n14\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2006-01-01\n2022-12-01\n2014-06-16\n204\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhpi\n0\n1\n172.19\n64.38\n64.47\n122.40\n168.06\n203.87\n427.80\n▆▇▃▁▁\n\n\nunemp_rate\n0\n1\n6.50\n2.93\n2.80\n4.20\n5.60\n8.30\n23.90\n▇▃▁▁▁\n\n\nage_lt_18\n0\n1\n0.24\n0.02\n0.19\n0.22\n0.24\n0.25\n0.30\n▂▇▇▅▂\n\n\nage_18_35\n0\n1\n0.25\n0.02\n0.20\n0.24\n0.25\n0.26\n0.31\n▁▃▇▃▁\n\n\nage_36_65\n0\n1\n0.39\n0.02\n0.33\n0.38\n0.39\n0.40\n0.44\n▁▃▇▂▁\n\n\nage_gt_65\n0\n1\n0.12\n0.02\n0.07\n0.11\n0.12\n0.14\n0.19\n▃▆▇▅▁\n\n\neduc_hs_less\n0\n1\n0.33\n0.03\n0.24\n0.31\n0.33\n0.35\n0.40\n▁▃▇▇▃\n\n\neduc_college\n0\n1\n0.38\n0.03\n0.31\n0.36\n0.38\n0.40\n0.48\n▂▇▆▃▁\n\n\neduc_grad\n0\n1\n0.10\n0.02\n0.05\n0.08\n0.10\n0.11\n0.16\n▃▇▇▅▁\n\n\nstatus_married\n0\n1\n0.40\n0.02\n0.32\n0.39\n0.40\n0.41\n0.45\n▁▁▇▇▂\n\n\nstatus_nev_mar\n0\n1\n0.27\n0.03\n0.19\n0.25\n0.27\n0.29\n0.34\n▂▅▇▇▂\n\n\nstatus_other\n0\n1\n0.33\n0.02\n0.28\n0.32\n0.33\n0.34\n0.45\n▃▇▃▁▁\n\n\nhispanic\n0\n1\n0.22\n0.11\n0.02\n0.17\n0.25\n0.30\n0.42\n▅▁▆▇▂\n\n\npopulation\n0\n1\n8471150.57\n6499655.47\n2834025.25\n4050215.00\n5109791.12\n10555276.77\n20320876.00\n▇▃▁▁▃"
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#splitting-the-data",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#splitting-the-data",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Splitting the data",
    "text": "Splitting the data\nThe data split object for a global modeling process is different than that for an iterative modeling process. For the global modeling process I’ll use timetk::time_series_split() . Below is a look at how the data are split.\n\n\nCode\necon_splits &lt;- econ_data |&gt;\n  time_series_split(\n    assess = \"2 year\",\n    cumulative = TRUE,\n    date_var = date\n  )\n\necon_splits\n\n\n&lt;Analysis/Assess/Total&gt;\n&lt;720/96/816&gt;"
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#choosing-algorithms",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#choosing-algorithms",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Choosing algorithms",
    "text": "Choosing algorithms\nAs with selecting the cities in the analysis I’m interested in getting a little variation in the algorithms to see how they perform and I want to vary the parameters a bit. To get something of a traditional flavor I’m going to run ARIMA models with three different model formulas:\n1) hpi ~ date\n2) hpi ~ date + unemployment\n3) hpi ~ date + unemployment + demographic variables\nI’ll also have four specifications for the algorithms. One will be the default auto-ARIMA and the other three will be auto-ARIMA specifications with different season lengths. This will give me twelve different ARIMA models (3 recipes x 4 algorithm specifications).\nIn much the same way I will have different sets of recipes and model specifications for the following:\n\nan exponential smoothing algorithm (including a Holt-Winters specification) to allow for weighting the periods exponentially\nan STLM algorithm using an ARIMA engine (STLM: Seasonal Trend Decomposition using Loess with Multiple seasonal periods)\nA feed-forward, auto-regressive neural-net algorithm because these tend to have high predictive ability\n\nSome of these algorithms allow for covariates and others (like exponential smoothing) do not. In the end I will have 90 model specifications to test. In order to do speed up the processing I’ll employ modeltime’s parallel processing feature."
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#building-workflow-objects",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#building-workflow-objects",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Building workflow objects",
    "text": "Building workflow objects\nHere I’ll write the three recipes for the ARIMA models and one for the auto-regressive neural-net model. The exponential smoothing and STLM models will both use the first of the three ARIMA recipes as neither of these algorithms can handle covariates.\nYou might notice in the code below that I exclude some of the demographic variables like “educ_hs_less” for the third ARIMA recipe. I exclude these variables because I found that including all of the variables for a demographic variable group causes the functions to error out much in the same way that including all of the categories of a factor variable would. Initially I did not expect that I would need contrasts considering that the variable values are ratios, but it seems that the fact that the sum of the ratios for a given variable group, e.g., education, is 1 for each row and for each variable group creates a need for contrasts in the regression (this is my guess).\n\nWriting recipes\n\n\n\n\n\n\nCode Explanations\n\n\n\nSome of the code chunks below contain explanations. Lines in explanations will be marked with encircled numbers. Just hover over those numbers to see the corresponding explanation.\n\n\n\n\nCode\n1arima_rec1 &lt;- recipe(hpi ~ date, data = training(econ_splits))\n\n2arima_rec2 &lt;- recipe(\n  hpi ~ date + unemp_rate,\n  data = training(econ_splits)\n) |&gt;\n3  step_center(all_numeric_predictors()) |&gt;\n4  step_scale(all_numeric_predictors()) |&gt;\n5  step_lag(unemp_rate, lag = c(1, 3, 6))\n\n6rec3_dep_vars &lt;- econ_data |&gt;\n  select(date, unemp_rate:population) |&gt;\n  select(!c(educ_hs_less, status_married, age_36_65)) |&gt;\n  names() |&gt;\n  str_flatten(collapse = \" + \")\n\narima_rec3 &lt;- recipe(\n  formula(str_c(\"hpi\", rec3_dep_vars, sep = \" ~ \")),\n  data = training(econ_splits)\n) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  step_lag(unemp_rate, population, lag = c(1, 3, 6))\n\nnnet_rec &lt;- recipe(\n  hpi ~ .,\n  data = training(econ_splits)\n) |&gt;\n7  update_role(city, new_role = \"ID\") |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n8  step_timeseries_signature(date) |&gt;\n9  step_nzv(all_predictors())\n\n\n\n1\n\nWrite a base recipe to regress HPI on the date variable\n\n2\n\nWrite a recipe formula that adds unemployment as an explanatory variable\n\n3\n\nMean-center all numeric predictors (only “unemployment rate” here)\n\n4\n\nScale all numeric predictors to have a mean of “0” and a standard error of 1\n\n5\n\nCreate lag variables for “unemployment rate” and “population”\n\n6\n\nGet the names of all the economic and demographic variables from the data set and concatenate them into a formula string to make the right side of a regression formula, i.e., “var1 + var2 + var3…”\n\n7\n\nBecause the “outcome ~ .” notation is being used, the “city” variable is included in the “.”, meaning it will be treated as an explanatory variable. This step changes the role of that variable from “predictor” to an ID variable, which modeling workflows will exclude from sets of predictors.\n\n8\n\nAdd a set of variables related to the date such as day of the week of the date, day of the month, number of the week of the year, etc.\n\n9\n\nRemove any variables with zero or near-zero variance. An example of this would be a variable indicating the day of the month. Since this is monthly data, all values would be “1”, i.e., the first day of the month.\n\n\n\n\nHere is a reminder of what a recipe looks like. This is the neural-net recipe.\n\n\nCode\nnnet_rec\n\n\nAnd this is the data set that gets created by that recipe.\n\n\nCode\nnnet_rec |&gt; prep() |&gt; juice() |&gt; slice(1:5) |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nunemp_rate\nage_lt_18\nage_18_35\nage_36_65\nage_gt_65\neduc_hs_less\neduc_college\neduc_grad\nstatus_married\nstatus_nev_mar\nstatus_other\nhispanic\npopulation\nhpi\ndate_index.num\ndate_year\ndate_year.iso\ndate_half\ndate_quarter\ndate_month\ndate_month.xts\ndate_month.lbl\ndate_wday\ndate_wday.xts\ndate_wday.lbl\ndate_qday\ndate_yday\ndate_mweek\ndate_week\ndate_week.iso\ndate_week2\ndate_week3\ndate_week4\n\n\n\n\nDallas\n2006-01-01\n-0.5318986\n1.9837898\n1.2244300\n-2.1179446\n-1.4144536\n1.4132488\n-1.9352723\n-1.0007519\n0.7138177\n-1.8445270\n1.9655339\n0.2421831\n-0.4128166\n121.9108\n1136073600\n2006\n2005\n1\n1\n1\n0\nJanuary\n1\n0\nSunday\n1\n1\n5\n1\n52\n1\n1\n1\n\n\nSan Diego\n2006-01-01\n-0.8651452\n-0.2639636\n-0.5419655\n-0.1248023\n0.7104276\n-0.6271533\n1.1018075\n-0.5165046\n-0.8376538\n-0.5633246\n1.4600357\n0.4759810\n-0.8614678\n247.4588\n1136073600\n2006\n2005\n1\n1\n1\n0\nJanuary\n1\n0\nSunday\n1\n1\n5\n1\n52\n1\n1\n1\n\n\nNew York\n2006-01-01\n-0.5652232\n0.4335013\n-0.7101494\n-0.3141128\n0.2439870\n1.6923074\n-1.9655377\n0.3044405\n-0.5241964\n-0.2666740\n0.7951381\n-0.1053321\n1.5314733\n213.4958\n1136073600\n2006\n2005\n1\n1\n1\n0\nJanuary\n1\n0\nSunday\n1\n1\n5\n1\n52\n1\n1\n1\n\n\nDetroit\n2006-01-01\n0.1679193\n1.1233044\n-1.7256753\n-0.3452176\n0.4389330\n1.7816712\n-1.6797175\n-1.2833046\n-0.7472264\n-1.1655961\n2.2179937\n-1.6335110\n-0.6156132\n126.6627\n1136073600\n2006\n2005\n1\n1\n1\n0\nJanuary\n1\n0\nSunday\n1\n1\n5\n1\n52\n1\n1\n1\n\n\nDallas\n2006-02-01\n-0.4985739\n2.4431155\n1.3332951\n-2.2373194\n-1.7310508\n0.7705532\n-0.9746432\n-1.6783801\n1.0008565\n-1.5342766\n1.3033739\n0.4153678\n-0.4092439\n121.3285\n1138752000\n2006\n2006\n1\n1\n2\n1\nFebruary\n4\n3\nWednesday\n32\n32\n5\n5\n5\n1\n2\n1\n\n\n\n\n\n\n\n\n\nSpecifying models\nIn this next step I’ll specify the model objects for each algorithm along with the search grids for their various hyperparameters. I chose to keep only some of the combinations of the exponential smoothing specifications to limit the number of models being run that are likely to fail.\n\n\nCode\n1arima_default &lt;- arima_reg() |&gt; set_engine(\"auto_arima\")\n\n2arima_spec_grid &lt;- tibble(seasonal_period = seq(9, 18, by = 3)) |&gt;\n3  create_model_grid(\n    f_model_spec = arima_reg,\n    engine_name = \"auto_arima\",\n    mode = \"regression\"\n  )\n\nets_default &lt;- exp_smoothing() |&gt; set_engine(\"ets\")\n\n4set.seed(40)\nalpha_vals &lt;- runif(5, 0.01, 0.5) *\n  sample(c(1, 0.1, 0.01, .001), 5, replace = TRUE)\nbeta_vals &lt;- runif(5, 0.01, 0.2) * sample(c(1, 0.1, 0.01), 5, replace = TRUE)\ngamma_vals &lt;- map_dbl(\n  alpha_vals,\n  \\(x) runif(1, 0, 1 - x)\n)\n\n5ets_spec_grid &lt;- expand_grid(\n  smooth_level = alpha_vals[-2],\n  smooth_seasonal = gamma_vals[2:3],\n  seasonal_period = seq(9, 18, by = 3)\n) |&gt;\n  create_model_grid(\n    f_model_spec = exp_smoothing,\n    engine_name = \"ets\",\n    mode = \"regression\",\n    error = \"auto\",\n    trend = \"auto\",\n    season = \"auto\",\n    smooth_trend = beta_vals[3]\n  )\n\nstlm_spec_grid &lt;- expand_grid(\n  seasonal_period_2 = c(NULL, seq(6, 18, by = 3))\n) |&gt;\n  create_model_grid(\n    f_model_spec = seasonal_reg,\n    engine_name = \"stlm_arima\",\n    mode = \"regression\",\n    seasonal_period_1 = 12\n  )\n\nnnet_default &lt;- nnetar_reg() |&gt; set_engine(\"nnetar\")\n\nnnet_spec_grid &lt;- expand_grid(\n  non_seasonal_ar = 1:3,\n  hidden_units = 3:5,\n  num_networks = c(20, 25),\n  penalty = c(0.2, 0.25)\n) |&gt;\n  create_model_grid(\n    f_model_spec = nnetar_reg,\n    engine_name = \"nnetar\",\n    mode = \"regression\",\n    seasonal_period = 12,\n    epochs = 100,\n    seasonal_ar = 1\n  )\n\n\n\n1\n\nSpecify an ARIMA regression model using the “auto_arima” engine with all default arguments\n\n2\n\nCreate a 1-column tibble (data frame) with the seasonal_period argument set to vary as 9, 12, 15, and 18 (months).\n\n3\n\ncreate_model_grid() allows one to set static arguments to combine them with the arguments that vary. This is meant to serve a similar purpose as tune::tune_grid() from Tidymodels.\n\n4\n\nSet a random seed for reproducibility and generate vectors for the smooth_level, smooth_trend, and smooth_seasonal arguments. In forecast::ets() these are alpha, beta, and gamma respectively.\n\n5\n\nUse tidyr::expand_grid() to get all combinations of the included vectors.\n\n\n\n\nBelow is what a model grid looks like.\n\n\nCode\nets_spec_grid |&gt; slice(1:5)\n\n\n# A tibble: 5 × 4\n  smooth_level smooth_seasonal seasonal_period .models  \n         &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt; &lt;list&gt;   \n1       0.0345           0.569               9 &lt;spec[+]&gt;\n2       0.0345           0.569              12 &lt;spec[+]&gt;\n3       0.0345           0.569              15 &lt;spec[+]&gt;\n4       0.0345           0.569              18 &lt;spec[+]&gt;\n5       0.0345           0.763               9 &lt;spec[+]&gt;\n\n\nThis is a glimpse of a model specification.\n\n\nCode\nets_spec_grid |&gt; slice(1) |&gt; pluck(\".models\", 1)\n\n\nExponential Smoothing State Space Model Specification (regression)\n\nMain Arguments:\n  seasonal_period = 9\n  error = auto\n  trend = auto\n  season = auto\n  smooth_level = 0.0344955188313033\n  smooth_trend = 0.0247416579537094\n  smooth_seasonal = 0.569256213388705\n\nComputational engine: ets \n\n\n\n\nBuilding workflowsets\nA workflowset is, you guessed it, a set of workflows. Mechanically it is a very convenient wrapper that can put together different combinations of recipes (pre-processors) and models. I don’t want to mix all of my recipes with all of my models, however. So, in this step I’ll make sets of individuals that combine the appropriate sets of recipes and models then I’ll bind all of them together at the end.\n\n\nCode\n1arima_wfset &lt;- workflow_set(\n2  preproc = list(\n    base_rec = arima_rec1,\n    econ_rec = arima_rec2,\n    demog_rec = arima_rec3\n  ),\n3  models = c(\n    arima_default = list(arima_default),\n    arima_spec = arima_spec_grid$.models\n  ),\n4  cross = TRUE\n)\n\nets_wfset &lt;- workflow_set(\n  preproc = list(base_rec = arima_rec1),\n  models = c(ets_default = list(ets_default), ets_spec = ets_spec_grid$.models),\n  cross = TRUE\n)\n\nstlm_wfset &lt;- workflow_set(\n  preproc = list(base_rec = arima_rec1),\n  models = c(stlm_spec = stlm_spec_grid$.models),\n  cross = TRUE\n)\n\nnnet_wfset &lt;- workflow_set(\n  preproc = list(nnet_rec = nnet_rec),\n  models = c(\n    nnet_default = list(nnet_default),\n    nnet_spec = nnet_spec_grid$.models\n  ),\n  cross = TRUE\n)\n\n\n\n1\n\nCall workflowsets::workflowset()\n\n2\n\nProvide a named list of recipes to use in the workflowset; naming the elements at this stage makes it easier to match up the elements of the final table containing the trained models with the original recipes and model specifications\n\n3\n\nProvide a named list of model specifications by pulling the “.models” column from each grid\n\n4\n\nIndicate that all recipes should be combined with all model specifications\n\n\n\n\nThe individual specifications in the final model table will be named using a combination of the name of the recipe and the name of the model specification given in the respective lists with a count added whenever there are duplicate names. For example, arima_spec_grid has four model specifications in it. The ones that are combined with the first ARIMA recipe (named “base_rec”) will be named BASE_REC_ARIMA_SPEC1, BASE_REC_ARIMA_SPEC2, BASE_REC_ARIMA_SPEC3, and BASE_REC_ARIMA_SPEC4.\nWith all of the model-specific workflow sets created, now it’s time to put them all together.\n\n\nCode\nhpi_wfset &lt;- bind_rows(arima_wfset, ets_wfset, stlm_wfset, nnet_wfset)\n\nhpi_wfset |&gt; slice(1:5)\n\n\n# A workflow set/tibble: 5 × 4\n  wflow_id               info             option    result    \n  &lt;chr&gt;                  &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n1 base_rec_arima_default &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n2 base_rec_arima_spec1   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n3 base_rec_arima_spec2   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n4 base_rec_arima_spec3   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n5 base_rec_arima_spec4   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n\n\nThat’s all the heavy lifting. Now it’s time to let the computer processors do their part and run the models."
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#fitting-the-models",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#fitting-the-models",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Fitting the models",
    "text": "Fitting the models\n\n\nCode\n1avail_cores &lt;- unname(parallelly::availableCores() - 1)\nmodulo_vec &lt;- sapply(avail_cores:2, \\(x) nrow(hpi_wfset) %% x)              \n2num_cores &lt;- max(\n  max(which(modulo_vec == max(modulo_vec))),\n    max(which(modulo_vec == 0))\n) + 1\n\n3cl &lt;- parallel::makePSOCKcluster(num_cores)\ndoParallel::registerDoParallel(cl)\ninvisible(\n  parallel::clusterCall(cl, function(x) .libPaths(x), .libPaths())\n)\nparallel::clusterEvalQ(cl, set.seed(500))\n\nhpi_mods_start &lt;- proc.time()\n\n4hpi_mods &lt;- hpi_wfset |&gt;\n5  modeltime_fit_workflowset(\n6    data = training(econ_splits),\n7    control = control_fit_workflowset(verbose = FALSE, allow_par = TRUE)\n  )\n\nhpi_mods_end &lt;- proc.time()\n\n8parallel::stopCluster(cl)\n\n\n\n1\n\nGet the number of cores available and leave one out for common tasks like running the operating system\n\n2\n\nCalculate the number of cores to use in order to maximize efficiency\n\n3\n\nSet up the cluster for parallel processing\n\n4\n\nCall the workflowset object containing all the workflows to run\n\n5\n\nUse modeltime::modeltime_fit_workflowset() to fit all of the models\n\n6\n\nUse the training portion of the split object to train the models\n\n7\n\nSet some controls for the model fitting function\n\n8\n\nStop the parallel cluster from running\n\n\n\n\n\n\nCode\nfit_time &lt;- (hpi_mods_end - hpi_mods_start) |&gt;\n  enframe(\"time_metric\", \"seconds\") |&gt;\n  drop_na(seconds)\n\nfit_time |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ntime_metric\nseconds\n\n\n\n\nuser.self\n0.97\n\n\nsys.self\n0.23\n\n\nelapsed\n74.36\n\n\n\n\n\n\n\nAbout 74.36 seconds elapsed during model-fitting with parallel processing. Not too bad. Unfortunately, though, some of the models failed (I suppressed error printing). So, before calibrating the models I’m going to drop those from the model output data set. But first I’ll investigate the model failures a little."
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#investigating-failed-models",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#investigating-failed-models",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Investigating failed models",
    "text": "Investigating failed models\nHere’s a look at some of the models that failed.\n\n\nCode\nmod_fail_df &lt;- hpi_mods |&gt;\n  mutate(mod_fail = map_lgl(.model, \\(x) is.null(x))) |&gt;\n  select(!.model)\n\nmod_fail_df |&gt;\n  group_by(mod_fail) |&gt;\n  slice_min(.model_id, n = 3) |&gt;\n  gt(groupname_col = NULL) |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\n.model_id\n.model_desc\nmod_fail\n\n\n\n\n1\nBASE_REC_ARIMA_DEFAULT\nFALSE\n\n\n2\nBASE_REC_ARIMA_SPEC1\nFALSE\n\n\n3\nBASE_REC_ARIMA_SPEC2\nFALSE\n\n\n33\nBASE_REC_ETS_SPEC17\nTRUE\n\n\n34\nBASE_REC_ETS_SPEC18\nTRUE\n\n\n35\nBASE_REC_ETS_SPEC19\nTRUE\n\n\n\n\n\n\n\nNow I want to see the distinct names of the recipes and algorithms (model specifications) that failed.\n\n\nCode\nfailure_spec_df &lt;- mod_fail_df |&gt;\n  filter(mod_fail) |&gt;\n  separate_wider_delim(\n    .model_desc,\n    delim = \"_\",\n    names = c(\"rec_name\", \"rec\", \"algo\", \"spec\")\n  ) |&gt;\n  separate_wider_regex(spec, patterns = c(spec = \"SPEC\", spec_num = \"\\\\d+\"))\n\nfailure_spec_df |&gt; distinct(rec_name, algo, spec) |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\nrec_name\nalgo\nspec\n\n\n\n\nBASE\nETS\nSPEC\n\n\n\n\n\n\n\nSince the failures occurred with only one algorithm they’ll be easy to isolate. Now I want to get the numbers of the specifications that failed so that I can pull up the details on those specifications.\n\n\nCode\nfailures &lt;- failure_spec_df |&gt;\n  pull(spec_num) |&gt;\n  as.numeric()\n\nets_failed &lt;- ets_spec_grid |&gt;\n  slice(failures) |&gt;\n  select(!.models)\n\nets_failed |&gt;\n  count(smooth_level) |&gt;\n  gt() |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\nsmooth_level\nn\n\n\n\n\n6.680868e-05\n8\n\n\n\n\n\n\n\nOverall there were 8 out of 32 failed exponential smoothing models. That’s not terrible. I’m going to remove these specifications from the trained model data set and move on to model evaluation."
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#model-evaluation",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#model-evaluation",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Model evaluation",
    "text": "Model evaluation\nTo evaluate the models first I need to calibrate them. I’ll remove the failed models, calibrate the successful ones, and store accuracy tables here. Below are the 10 best-performing global models, i.e., the ones that performed best on the batched series with all four cities. I defined “best” as the models with the lowest SMAPE (Scaled Mean Average Percent Error) values.\n\n\nCode\nhpi_mods &lt;- hpi_mods |&gt; slice(which(!mod_fail_df$mod_fail))\n\nhpi_mod_calib &lt;- hpi_mods |&gt;\n  modeltime_calibrate(testing(econ_splits), id = \"city\", quiet = TRUE)\n\n\n\n\nCode\nhpi_mod_accuracy &lt;- hpi_mod_calib |&gt;\n  modeltime_accuracy() |&gt;\n  drop_na(.type)\n\nhpi_local_mod_accuracy &lt;- hpi_mod_calib |&gt;\n  modeltime_accuracy(acc_by_id = TRUE) |&gt;\n  drop_na(.type)\n\nbest_global_table &lt;- hpi_mod_accuracy |&gt;\n  slice_min(smape, n = 10)\n\nbest_local_table &lt;- hpi_local_mod_accuracy |&gt;\n  group_by(city) |&gt;\n  slice_min(smape, n = 3)\n\nbest_global_table |&gt;\n  table_modeltime_accuracy(\n    .interactive = FALSE,\n    .title = \"Global Accuracy\"\n  ) |&gt;\n  gt_bold_head() |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_title()\n  )\n\n\n\n\n\n\n\n\nGlobal Accuracy\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n16\nBASE_REC_ETS_DEFAULT\nTest\n19.29\n6.22\n0.18\n6.59\n28.08\n0.96\n\n\n1\nBASE_REC_ARIMA_DEFAULT\nTest\n20.79\n7.14\n0.19\n7.37\n29.50\n0.95\n\n\n3\nBASE_REC_ARIMA_SPEC2\nTest\n20.79\n7.14\n0.19\n7.37\n29.50\n0.95\n\n\n30\nBASE_REC_ETS_SPEC14\nTest\n20.81\n7.48\n0.19\n7.60\n28.89\n0.95\n\n\n22\nBASE_REC_ETS_SPEC6\nTest\n20.80\n7.48\n0.19\n7.60\n28.87\n0.95\n\n\n46\nBASE_REC_ETS_SPEC30\nTest\n22.16\n7.53\n0.21\n7.82\n31.18\n0.95\n\n\n42\nBASE_REC_ETS_SPEC26\nTest\n22.31\n7.77\n0.21\n8.00\n31.01\n0.95\n\n\n53\nBASE_REC_STLM_SPEC5\nTest\n30.98\n9.75\n0.29\n10.59\n43.56\n0.94\n\n\n49\nBASE_REC_STLM_SPEC1\nTest\n31.20\n9.81\n0.29\n10.68\n43.87\n0.93\n\n\n52\nBASE_REC_STLM_SPEC4\nTest\n32.10\n10.02\n0.30\n10.94\n45.27\n0.93\n\n\n\n\n\n\n\nNext is a table with the best 3 models calibrated to each city.\n\n\nCode\nbest_local_table|&gt;\n  table_modeltime_accuracy(\n    .interactive = FALSE,\n    .title = \"Accuracy by City\"\n  ) |&gt;\n  gt_bold_head() |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_title()\n  ) |&gt;\n  tab_style(\n    style = list(cell_text(weight = \"bold\"), cell_fill(color = \"lightgray\")),\n    locations = cells_group()\n  ) |&gt;\n  tab_style(\n    style = cell_borders(\n      sides = c(\"top\", \"bottom\"),\n      weight = px(2),\n      style = (\"solid\")\n    ),\n    locations = cells_group()\n  )\n\n\n\n\n\n\n\n\nAccuracy by City\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\nDallas\n\n\n12\nDEMOG_REC_ARIMA_SPEC1\nTest\n13.99\n5.18\n2.65\n5.32\n15.79\n0.80\n\n\n14\nDEMOG_REC_ARIMA_SPEC3\nTest\n14.19\n5.24\n2.69\n5.39\n15.97\n0.80\n\n\n22\nBASE_REC_ETS_SPEC6\nTest\n20.59\n7.34\n3.93\n7.72\n24.76\n0.85\n\n\nDetroit\n\n\n72\nNNET_REC_NNET_SPEC18\nTest\n2.70\n1.63\n1.53\n1.65\n3.64\n0.89\n\n\n74\nNNET_REC_NNET_SPEC20\nTest\n2.82\n1.71\n1.60\n1.72\n3.69\n0.88\n\n\n66\nNNET_REC_NNET_SPEC12\nTest\n2.86\n1.74\n1.63\n1.74\n3.61\n0.88\n\n\nNew York\n\n\n1\nBASE_REC_ARIMA_DEFAULT\nTest\n3.09\n1.16\n1.30\n1.16\n4.41\n0.94\n\n\n3\nBASE_REC_ARIMA_SPEC2\nTest\n3.09\n1.16\n1.30\n1.16\n4.41\n0.94\n\n\n30\nBASE_REC_ETS_SPEC14\nTest\n3.18\n1.19\n1.34\n1.17\n5.06\n0.95\n\n\nSan Diego\n\n\n51\nBASE_REC_STLM_SPEC3\nTest\n23.42\n6.49\n3.10\n6.34\n28.83\n0.61\n\n\n12\nDEMOG_REC_ARIMA_SPEC1\nTest\n35.62\n9.29\n4.85\n9.84\n39.31\n0.71\n\n\n14\nDEMOG_REC_ARIMA_SPEC3\nTest\n36.77\n9.58\n5.01\n10.16\n40.32\n0.71\n\n\n\n\n\n\n\nThere’s a pretty good mix of models across the two tables including ARIMA, ETS (lots of this one), STLM, and neural-net. Among the ARIMA models there were some with the basic recipe and some with the recipe that included both economic and demographic variables.\nBelow are plots of forecasts of the test data period for each city using the best 3 global models.\n\n\nCode\ntop_3_global &lt;- best_global_table |&gt; slice_min(smape, n = 3) |&gt; pull(.model_id)\n\nhpi_mod_calib |&gt;\n  filter(.model_id %in% top_3_global) |&gt;                            \n  modeltime_forecast(                            \n    new_data = testing(econ_splits),      \n    actual_data = econ_data,                     \n    conf_by_id = TRUE,                           \n    keep_data = TRUE                             \n  ) |&gt;\n  group_by(city) |&gt;                             \n  plot_modeltime_forecast(                      \n    .interactive = FALSE,\n    .title = \"Forecast of Test Data\",\n    .facet_ncol = 2\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nJust for fun, here are the 5 worst-performing models (global).\n\n\nCode\nhpi_mod_accuracy |&gt;\n  slice_max(smape, n = 5, with_ties = FALSE) |&gt;\n  gt() |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n2\nBASE_REC_ARIMA_SPEC1\nTest\n82.07379\n28.40303\n0.7639937\n32.64595\n100.22516\n0.0237270639\n\n\n4\nBASE_REC_ARIMA_SPEC3\nTest\n81.85927\n28.93424\n0.7619969\n32.63484\n99.20593\n0.0005985376\n\n\n17\nBASE_REC_ETS_SPEC1\nTest\n81.34845\n28.53501\n0.7572419\n32.31949\n99.36593\nNA\n\n\n19\nBASE_REC_ETS_SPEC3\nTest\n81.34845\n28.53501\n0.7572419\n32.31949\n99.36593\nNA\n\n\n20\nBASE_REC_ETS_SPEC4\nTest\n81.34845\n28.53501\n0.7572419\n32.31949\n99.36593\nNA"
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#forecasting-with-global-models",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#forecasting-with-global-models",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Forecasting with global models",
    "text": "Forecasting with global models\nBelow are forecasts for each city using the top 3 global models.\n\n\nCode\nhpi_refit &lt;- hpi_mod_calib |&gt;\n  filter(.model_id %in% top_3_global) |&gt;\n  modeltime_refit(data = econ_data) \n\nhpi_future &lt;- econ_data |&gt;\n  group_by(city) |&gt;\n  future_frame(.length_out = 12, .bind_data = FALSE, .date_var = date)\n\nhpi_future_forecast &lt;- hpi_refit |&gt;\n  modeltime_forecast(\n    new_data = hpi_future,\n    actual_data = econ_data,\n    conf_by_id = TRUE\n  )\n\nhpi_future_forecast |&gt;\n  group_by(city) |&gt;\n  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"1 Year Forecast into the Future - Global\",\n    .facet_ncol = 1\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#choosing-models-for-iterative-process",
    "href": "posts/20240220-forecasting-with-modeltime-part-4/forecasting-with-modeltime-part-4.html#choosing-models-for-iterative-process",
    "title": "Forecasting with {modeltime} - Part IV",
    "section": "Choosing models for iterative process",
    "text": "Choosing models for iterative process\nFor iterative modeling I’m going to use the 10 best global models and the 3 best local models for each city. The specifications that I’m going to use for iterative modeling in the next post are:\n\n\nCode\nbest_global_table |&gt;\n  slice_min(smape, n = 5) |&gt;\n  bind_rows(best_local_table) |&gt;\n  distinct(.model_desc) |&gt;\n  gt() |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\n.model_desc\n\n\n\n\nBASE_REC_ETS_DEFAULT\n\n\nBASE_REC_ARIMA_DEFAULT\n\n\nBASE_REC_ARIMA_SPEC2\n\n\nBASE_REC_ETS_SPEC14\n\n\nBASE_REC_ETS_SPEC6\n\n\nDEMOG_REC_ARIMA_SPEC1\n\n\nDEMOG_REC_ARIMA_SPEC3\n\n\nNNET_REC_NNET_SPEC18\n\n\nNNET_REC_NNET_SPEC20\n\n\nNNET_REC_NNET_SPEC12\n\n\nBASE_REC_STLM_SPEC3"
  },
  {
    "objectID": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html",
    "href": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html",
    "title": "Forecasting with {modeltime} - Part II",
    "section": "",
    "text": "Last time on Forecasting with {modeltime} - Part I I wrote about gathering the data for this project from various sources. Here I’ll go into the first part of the analysis which will include visualizing the dependent variable, checking and correcting for non-stationarity, identifying seasonality, and identifying the terms (AR, MA) to use in a S-/ARIMA model.\nBesides the usual tidyverse (Wickham et al. 2019), in this post I’ll be using the timetk (Dancho and Vaughan 2023) package – also from the folks at Business Science – for performing my data analysis.\nCode\nlibrary(tidyverse)\nlibrary(timetk)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")"
  },
  {
    "objectID": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#a-quick-look-at-the-case-shiller-hpi",
    "href": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#a-quick-look-at-the-case-shiller-hpi",
    "title": "Forecasting with {modeltime} - Part II",
    "section": "A quick look at the Case-Shiller HPI",
    "text": "A quick look at the Case-Shiller HPI\nIn this post I’ll only be looking at the Case-Shiller Home Price Index (HPI) for the four cities in my data – San Diego, Dallas, Detroit, and New York City – between January 2006 and December 2022. Here’s a quick peek at the data:\n\n\nCode\necon_data |&gt;\n  group_by(city) |&gt;\n  slice_min(date, n = 3) |&gt;\n  ungroup() |&gt;\n  gt() |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\n\n\n\n\nDallas\n2006-01-01\n121.9108\n\n\nDallas\n2006-02-01\n121.3285\n\n\nDallas\n2006-03-01\n121.5217\n\n\nDetroit\n2006-01-01\n126.6627\n\n\nDetroit\n2006-02-01\n126.3886\n\n\nDetroit\n2006-03-01\n126.1218\n\n\nNew York\n2006-01-01\n213.4958\n\n\nNew York\n2006-02-01\n214.4696\n\n\nNew York\n2006-03-01\n214.3266\n\n\nSan Diego\n2006-01-01\n247.4588\n\n\nSan Diego\n2006-02-01\n247.8891\n\n\nSan Diego\n2006-03-01\n248.0924\n\n\n\n\n\n\n\nNow I’ll look visualize the data in a line plot.\n\n\nCode\necon_data |&gt;\n  ggplot(aes(date, hpi, color = city)) +\n  geom_line() +\n  labs(\n    x = NULL,\n    y = \"HPI\",\n    color = NULL,\n    title = \"Case-Shiller HPI from 2006 through 2022\"\n  ) +\n  theme_timetk +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nHere one can see that Dallas’s HPI did not drop as much during the Great Recession and has had a fast rise relative to those of the other cities ever since. It’s also pretty clear that NYC’s HPI has experienced a relatively mild increase over the years covered. There’s also more volatility between 2010 and 2015 in the HPIs for NYC, Dallas, and Detroit than in San Diego’s."
  },
  {
    "objectID": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#time-series-analysis",
    "href": "posts/20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.html#time-series-analysis",
    "title": "Forecasting with {modeltime} - Part II",
    "section": "Time series analysis",
    "text": "Time series analysis\nThe goal of this initial analysis is to find the parameters to use in an ARIMA model, which means that I’ll need to ensure my data are stationary. I’ll try to get there through the following operations and analyses.\n\nPreparing the data\nBefore performing any analysis I’d like to add one column with a lagged first-difference and another with a lagged twelfth-difference. To accomplish this I’ll use the tk_augment_differences() from the timetk package. I’ll store this data set in a new variable called “hpi_series”. I chose these two lagged differences because the lagged first-difference generally any stationarity rooted in a trend and the lagged twelfth-difference should deal with seasonality as my data are monthly observations and I’m operating on the assumption that the housing market cycle is annual.\nBelow is a snippet of the data.\n\n\nCode\nhpi_series &lt;- econ_data |&gt;\n  select(city, date, hpi) |&gt;\n  group_by(city) |&gt;\n  tk_augment_differences(\n    .value = hpi,\n    .lags = 1,\n    .differences = c(1, 12),\n    .log = FALSE\n  ) |&gt;\n  ungroup()\n\nhead(hpi_series, 15) |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\nhpi_lag1_diff1\nhpi_lag1_diff12\n\n\n\n\nDallas\n2006-01-01\n121.9108\nNA\nNA\n\n\nDallas\n2006-02-01\n121.3285\n-0.58234\nNA\n\n\nDallas\n2006-03-01\n121.5217\n0.19324\nNA\n\n\nDallas\n2006-04-01\n122.3996\n0.87788\nNA\n\n\nDallas\n2006-05-01\n123.2863\n0.88666\nNA\n\n\nDallas\n2006-06-01\n124.4985\n1.21228\nNA\n\n\nDallas\n2006-07-01\n125.3672\n0.86869\nNA\n\n\nDallas\n2006-08-01\n125.7007\n0.33342\nNA\n\n\nDallas\n2006-09-01\n125.1859\n-0.51474\nNA\n\n\nDallas\n2006-10-01\n124.5813\n-0.60463\nNA\n\n\nDallas\n2006-11-01\n123.8517\n-0.72958\nNA\n\n\nDallas\n2006-12-01\n123.6697\n-0.18205\nNA\n\n\nDallas\n2007-01-01\n122.6361\n-1.03360\n-161.2924\n\n\nDallas\n2007-02-01\n122.7292\n0.09314\n188.2084\n\n\nDallas\n2007-03-01\n123.1562\n0.42699\n-251.1343\n\n\n\n\n\n\n\nAnd now I can look at line plots of the three different series with the original on the left ; the lagged first-difference in the middle; and the lagged twelfth-difference on the right with a the cities in the different rows. From left to right the plots go from being highly regular (seasonal and trending) to being less regular in the middle (only seasonal) and highly irregular on the right. That irregularity is indicative of stationarity, which is what I want to see. This also provides a sneak peak into what the following analyses will give us.\n\n\nCode\nhpi_series |&gt;\n  pivot_longer(starts_with(\"hpi\"), names_to = \"series\", values_to = \"value\") |&gt;\n  group_by(city, series) |&gt;\n  plot_time_series(\n    date,\n    value,\n    .facet_ncol = 3,\n    .title = \"Case-Shiller HPI from 2006 through 2022\",\n    .interactive = FALSE\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nOn might ask “why not use a lagged, log difference?” The below plot shows that using a lagged, log first- and twelfth-differenced series don’t change anything compared to the lagged first- and twelfth-differenced series except for generating data with much higher relative amplitude and possibly showing outliers. This might actually introduce heteroskedasticity, which I’d like to avoid.\n\n\nCode\necon_data |&gt;\n  tk_augment_differences(\n    .value = hpi,\n    .lags = 1,\n    .differences = c(1, 12),\n    .log = TRUE\n  ) |&gt;\n  pivot_longer(starts_with(\"hpi\"), names_to = \"series\", values_to = \"value\") |&gt;\n  group_by(city, series) |&gt;\n  plot_time_series(\n    date,\n    value,\n    .facet_ncol = 3,\n    .title = \"Case-Shiller HPI from 2006 through 2022\",\n    .interactive = FALSE\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nTime series decomposition\nThe following plot shows the Seasonal-Trend decomposition using Loess (STL), which is a common tool to determine how much of a time series’ movement is due to a trend versus that which is due to it’s seasonality. Again, here I’ll assume an annual seasonal cycle.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_stl_diagnostics(\n    date,\n    hpi,\n    .feature_set = c(\"observed\", \"season\", \"trend\", \"remainder\"),\n    .interactive = FALSE,\n    .frequency = 12\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nAccording to this plot most of the movement in the HPI is determined by the trend and very little by seasonality.\n\n\nAugmented Dickey-Fuller Test\nThe Augmented Dickey-Fuller Test (ADF) is commonly run to evaluate whether a time series is stationary or not. The null hypothesis of the ADF is that the time series is non-stationary, so want to reject the null hypothesis to be able to assume the alternative, i.e., that the series is stationary. The following table shows that one can reject the null hypothesis for every city but San Diego with just a lagged first-difference, but with the lagged twelfth-difference all four series can be thought of as stationary.\n\n\nCode\nhpi_adf &lt;- hpi_series |&gt;\n  pivot_longer(!c(city, date), names_to = \"variable\") |&gt;\n  group_by(city, variable) |&gt;\n  nest(data = c(date, value)) |&gt;\n  mutate(\n    adf_test = map(\n      data,\n      \\(x) drop_na(x, value) |&gt;\n        pull(value) |&gt;\n        tseries::adf.test() |&gt;\n        broom::tidy() |&gt;\n        select(statistic, p.value)\n    )\n  ) |&gt;\n  select(-data) |&gt;\n  unnest(adf_test) |&gt;\n  ungroup(variable)\n\nhpi_adf |&gt;\n  group_by(city) |&gt;\n  group_nest() |&gt;\n  mutate(data_gt = map(data, make_adf_gt)) |&gt;\n  select(!data) |&gt;\n  pivot_wider(names_from = city, values_from = data_gt) |&gt;\n  gt() |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDallas\nDetroit\nNew York\nSan Diego\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−0.688\n0.970\n\n\nhpi_lag1_diff1\n−3.985\n0.011\n\n\nhpi_lag1_diff12\n−21.100\n0.010\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−2.927\n0.188\n\n\nhpi_lag1_diff1\n−6.023\n0.010\n\n\nhpi_lag1_diff12\n−21.563\n0.010\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−0.256\n0.990\n\n\nhpi_lag1_diff1\n−6.495\n0.010\n\n\nhpi_lag1_diff12\n−24.809\n0.010\n\n\n\n\n\n\n\n\nvariable\nstatistic\np.value\n\n\n\n\nhpi\n−2.368\n0.422\n\n\nhpi_lag1_diff1\n−2.982\n0.165\n\n\nhpi_lag1_diff12\n−27.132\n0.010\n\n\n\n\n\n\n\n\n\n\n\n\n\nACF / PACF plots\nThe Autocorrelation Function (ACF) calculates the correlation of a value, e.g., the most recent value of the HPI, with previous values in the same series. Knowing this is very useful for determining how many Moving Average (the “MA” in ARIMA) lags to use in an ARIMA mode. The Partial Autocorrelation Function (PACF) calculates the correlation of a value with previous values in the same series after accounting for intervening correlations. This is useful to help us determine how many lags to use for the Autoregressive (the “AR” in ARIMA) part of an ARIMA model. Below are the ACF and PACF plots for the HPI series (no lagged differences) for all four cities. The timetk::plot_acf_diagnostics() function plots ACF on top and PACF on bottom, which is conventional. Values outside of the red, dashed lines indicate statistically significant correlations.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_acf_diagnostics(\n    date,\n    hpi,\n    .lags = 60,\n    .title = \"HPI Lag Diagnostics\",\n    .interactive = FALSE,\n    .white_noise_line_color = \"red\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nIt’s clear from the ACF plot that the trend is causing a lot of the autocorrelation in each city. Next is the same plot, but for the first-differenced values.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_acf_diagnostics(\n    date,\n    hpi_lag1_diff1,\n    .lags = 60,\n    .title = \"First-Differenced HPI Lag Diagnostics\",\n    .interactive = FALSE,\n    .white_noise_line_color = \"red\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThe trend’s influence is significantly decreased here, but there are still some strong seasonal patterns in the data. Below is a look at the twelfth-differenced series.\n\n\nCode\nhpi_series |&gt;\n  group_by(city) |&gt;\n  plot_acf_diagnostics(\n    date,\n    hpi_lag1_diff12,\n    .lags = 60,\n    .title = \"Twelfth-Differenced HPI Lag Diagnostics\",\n    .interactive = FALSE,\n    .white_noise_line_color = \"red\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nLooking at this plot over 36 months it still appears that there is a small seasonal effect in Dallas, San Diego, and Detroit with Detroit’s possibly being a shorter season. Considering the infinite number of things that could affect one of these data series one could look at hundreds of plots like this with different combinations of lags and differences for any one series and never find a perfect one. This plot is pretty good. From this plot it looks like I’m going to us an AR(3) model. San Diego’s PACF suggests that I should use two for that city, but I’m looking for one value for all four cities. The MA component is a little more challenging. It looks like I would use MA lags of 5 or 6 for Dallas, 8 for San Diego, 4 for NYC, and 4 or 5 for Detroit. I’m going to use MA(5) for my ARIMA.\n\n\nSeasonality\nIt’s been pretty clear up to this point that seasonality is an important consideration with this data series and it’s something that I will expect to handle in the modeling portion of the analysis in a later post. If I wanted to handle seasonality in a conventional model I might run a SARIMA model, but I’m going to keep this part simple and only run ARIMA knowing that I’m partially ignoring the seasonality and only dealing with it by differencing. With that said, I’ll show two more plots here to demonstrate the timetk::plot_seasonal_diagnostics() . The first plot shows seasonal diagnostics for the unadjusted series and the second one for the twelfth-differenced series.\n\n\nCode\nhpi_series |&gt;\n  plot_seasonal_diagnostics(\n    date,\n    hpi,\n    .facet_vars = city,\n    .interactive = FALSE,\n    .title = \"HPI Seasonal Diagnostics\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nThis plot shows a clear pattern of HPI values being lowest in the first quarter across all four cities, highest in the second and third quarters for San Diego, and highest in the quarter for New York and December. From this data it seems that the peak in Dallas is December. This seasonality can obscure the findings of an ARIMA model.\n\n\nCode\nhpi_series |&gt;\n  plot_seasonal_diagnostics(\n    date,\n    hpi_lag1_diff12,\n    .facet_vars = city,\n    .interactive = FALSE,\n    .title = \"Twelfth-Differenced HPI Seasonal Diagnostics\"\n  ) +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\nIn this plot the patterns are generally gone across the quarterly plots. Even the trend is mostly gone from the annual plot.\nIn the next post I’ll use the AR and MA values from this post to run ARIMA models for all four cities using a global modeling process and an iterative modeling process with modeltime."
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html",
    "href": "posts/20240125-topic-modeling/topic-modeling.html",
    "title": "Topic Modeling with R",
    "section": "",
    "text": "For this project I’ll be following an example of performing topic modeling with R using the Tidytext format introduced by Julia Silge and David Robinson in their “Text Mining with R! A Tidy Approach”. And I’ll be using abstracts submitted by Johns Hopkins University for funding to the NIH HEAL Initiative. The list of funded projects can be found at https://heal.nih.gov/funding/awarded. A benefit of using this data is that it doesn’t require much cleaning as Twitter text might. For the purposes of this demonstration I won’t do any data cleaning. The objective will be to find the optimal number of topics to which to attribute the abstracts. I’ll be using the tidyverse and tidytext packages for wrangling, the stm package for modeling, the janitor package for cleaning column names (but I won’t load it because I’m only going to use it once), the knitr package for table formatting, and the pacman package to load my packages (also only used once).\n\n\nCode\npacman::p_load(tidyverse, tidytext, stm, knitr, gt)"
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#introduction",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#introduction",
    "title": "Topic Modeling with R",
    "section": "",
    "text": "For this project I’ll be following an example of performing topic modeling with R using the Tidytext format introduced by Julia Silge and David Robinson in their “Text Mining with R! A Tidy Approach”. And I’ll be using abstracts submitted by Johns Hopkins University for funding to the NIH HEAL Initiative. The list of funded projects can be found at https://heal.nih.gov/funding/awarded. A benefit of using this data is that it doesn’t require much cleaning as Twitter text might. For the purposes of this demonstration I won’t do any data cleaning. The objective will be to find the optimal number of topics to which to attribute the abstracts. I’ll be using the tidyverse and tidytext packages for wrangling, the stm package for modeling, the janitor package for cleaning column names (but I won’t load it because I’m only going to use it once), the knitr package for table formatting, and the pacman package to load my packages (also only used once).\n\n\nCode\npacman::p_load(tidyverse, tidytext, stm, knitr, gt)"
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#downloading-the-abstract-data",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#downloading-the-abstract-data",
    "title": "Topic Modeling with R",
    "section": "Downloading the abstract data",
    "text": "Downloading the abstract data\nI’ll first download the data from the NIH’s Funded Projects page and filter for John’s Hopkins. The code for this is below, though, I will not run it because the data change as proposals are submitted. Instead I’ll download the filtered data to my hard drive and the ensuing analysis will be based on the data available as of this writing (January 25, 2024).\n\n\nCode\n# Not run\nread_csv(\n  \"https://heal.nih.gov/funding/awarded/export?combine=johns%20hopkins&_format=csv\"\n) |&gt;\n  write_csv(file.path(data_dir, \"jh-data.csv\"))\n\n\nNow I’ll read that data in and just look at the column names to start familiarizing myself with the data\n\n\nCode\njh_data &lt;- read_csv(file.path(data_dir, \"jh-data.csv\"))\nglimpse(jh_data)\n\n\nRows: 30\nColumns: 10\n$ `Project #`           &lt;chr&gt; \"1R01DA059473-01\", \"1RF1NS134549-01\", \"5U54DA049…\n$ `Project Title`       &lt;chr&gt; \"Sleep and Circadian Rhythm Phenotypes and Mecha…\n$ `Research Focus Area` &lt;chr&gt; \"New Strategies to Prevent and Treat Opioid Addi…\n$ `Research Program`    &lt;chr&gt; \"Sleep Dysfunction as a Core Feature of Opioid U…\n$ `Administering IC(s)` &lt;chr&gt; \"NIDA\", \"NINDS\", \"NIDA\", \"NIAMS\", \"NIDA\", \"NIDA\"…\n$ `Institution(s)`      &lt;chr&gt; \"JOHNS HOPKINS UNIVERSITY\", \"JOHNS HOPKINS UNIVE…\n$ `Investigator(s)`     &lt;chr&gt; \"HUHN, ANDREW S (contact); RABINOWITZ, JILL ALEX…\n$ `Location(s)`         &lt;chr&gt; \"Baltimore, MD\", \"Baltimore, MD\", \"Baltimore, MD…\n$ `Year Awarded`        &lt;dbl&gt; 2023, 2023, 2023, 2023, 2022, 2022, 2022, 2022, …\n$ Summary               &lt;chr&gt; \"Chronic opioid use has well known effects on sl…\n\n\nIt looks like there are 30 columns in the data. I’m going to clean up the column names and select only the Project # (to use as an identifier), Project Title, and Summary.\n\n\nCode\njh_clean &lt;- jh_data |&gt;\n  janitor::clean_names() |&gt;\n  select(project_number, project_title, summary)\n\nhead(jh_clean, 3) |&gt; gt::gt() # kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\nproject_number\nproject_title\nsummary\n\n\n\n\n1R01DA059473-01\nSleep and Circadian Rhythm Phenotypes and Mechanisms Associated With Opioid Use Disorder Treatment Outcomes\nChronic opioid use has well known effects on sleep quality, including disordered breathing during sleep and other abnormalities related to circadian rhythms. However, little is known about the relationship between sleep-related symptoms and non-medical opioid use among individuals being treated for opioid use disorder. This longitudinal study aims to identify biological pathways that may account for these associations. The research will first determine associations of sleep and proxy measures of circadian rhythms with non-medical opioid use. Second, they will investigate emotional processes associated with sleep/circadian symptoms and opioid treatment outcomes.\n\n\n1RF1NS134549-01\nValidation of a New Large-Pore Channel as a Novel Target for Neuropathic Pain\nActivation of immune cells (microglia) in the central nervous system and neuroinflammation have emerged as key drivers of neuropathic pain. These processes can be triggered by release of ATP, the compound that provides energy to many biochemical reactions. The source and mechanism of ATP release are poorly understood but could be targets of novel treatment approaches for neuropathic pain. This project will use genetic, pharmacological, and electrophysiological approaches to determine whether a large pore channel called Swell 1 that spans the cell membrane is the source of ATP release and resulting neuropathic pain and thus could be a treatment target.\n\n\n5U54DA049110-04\nData Center for Acute to Chronic Pain Biosignatures\nUnderstanding the mechanisms underlying the transition to chronic pain is key to mitigating the dual epidemics of chronic pain and opioid use in the United States. As part of the National Institutes of Health-funded Acute to Chronic Pain Signatures Program, the Data Integration and Resource Center aims to This project will support a post-doctoral trainee to develop the skills and knowledge needed to pursue a successful career in clinical pain research. The research will involve integrating imaging, physiology, -omics, behavioral, and clinical data to develop biosignatures for the transition from acute to chronic pain, toward understanding how the nervous and immune systems affect post-surgical pain and opioid use."
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#tidy-the-data",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#tidy-the-data",
    "title": "Topic Modeling with R",
    "section": "Tidy the data",
    "text": "Tidy the data\nIn this next step I’m going to tidy the data and remove stop words. Then I’ll take a look at word frequencies and TF-IDF metrics to get an idea of what I might expect from my model.\nI’m first going to use the tidytext::unnest_tokens() function to tokenize the data by single word then remove stop words from the text. I’ll also get word frequencies in this one step. Also, I’m going to drop the project title for the time being, but I’ll bring it back later.\n\n\nCode\njh_tidy &lt;- jh_clean |&gt;\n  mutate(summary = str_c(project_title, summary, sep = \" \")) |&gt;\n  select(-project_title) |&gt;\n  unnest_tokens(word, summary, token = \"words\") |&gt;\n  anti_join(stop_words, by = \"word\") |&gt;\n  count(project_number, word)\n\njh_tidy |&gt; arrange(-n) |&gt; head(10) |&gt; gt()\n\n\n\n\n\n\n\n\nproject_number\nword\nn\n\n\n\n\n1RF1NS113883-01\npain\n9\n\n\n3R01MD009063-05S1\npain\n9\n\n\n1UG3NS115718-01\nmrgprx1\n8\n\n\n1U01HL150568-01\nsleep\n7\n\n\n1U01HL150835-01\nsleep\n7\n\n\n5U54DA049110-04\npain\n7\n\n\n1R01DA059473-01\nopioid\n6\n\n\n1R01DA059473-01\nsleep\n6\n\n\n1R61AT012279-01\nshoulder\n6\n\n\n1U01HL150835-01\nstress\n6\n\n\n\n\n\n\n\nThe words “pain” is one of the most frequently occurring word in documents 1RF1NS113883-01 and 5U54DA049110-04 while the words “pain” and “sleep” seem to be 2 of the most frequent across documents. Let’s see how these words do in terms of how important they are in distinguishing a document from other documents.\nI’m going to add TF-IDF metrics using the tidytext::bind_tf_idf() function.\n\n\nCode\njh_tfidf &lt;- jh_tidy |&gt; bind_tf_idf(word, project_number, n)\n\njh_tfidf |&gt;\n  arrange(-tf_idf) |&gt;\n  head(15) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nproject_number\nword\nn\ntf\nidf\ntf_idf\n\n\n\n\n3U24TR001609-04S1\ntin\n1\n0.33333333\n3.401197\n1.1337325\n\n\n3U24TR001609-04S1\nsummary\n1\n0.33333333\n2.708050\n0.9026834\n\n\n3U24TR001609-04S1\nsupplement\n1\n0.33333333\n2.302585\n0.7675284\n\n\n1R61HL156248-01\nintranasal\n1\n0.12500000\n3.401197\n0.4251497\n\n\n1R61HL156248-01\nleptin\n1\n0.12500000\n3.401197\n0.4251497\n\n\n1R61HL156248-01\ninduced\n1\n0.12500000\n2.708050\n0.3385063\n\n\n1R61HL156248-01\nrespiratory\n1\n0.12500000\n2.708050\n0.3385063\n\n\n1R61HL156248-01\nsummary\n1\n0.12500000\n2.708050\n0.3385063\n\n\n1R61HL156248-01\ndepression\n1\n0.12500000\n2.302585\n0.2878231\n\n\n1R61AT012279-01\nshoulder\n6\n0.08333333\n3.401197\n0.2834331\n\n\n1RF1AG068997-01\nbone\n3\n0.06976744\n3.401197\n0.2372928\n\n\n1R01DA057655-01\nharm\n4\n0.06666667\n3.401197\n0.2267465\n\n\n1R01DA057655-01\nreduction\n4\n0.06666667\n3.401197\n0.2267465\n\n\n1UG3NS115718-01\nmrgprx1\n8\n0.06557377\n3.401197\n0.2230293\n\n\n1R01DA059473-01\nsleep\n6\n0.10169492\n2.014903\n0.2049054\n\n\n\n\n\n\n\nIt’s very strange that 2 documents have the term “summary” as the only term in the summary. This is indicated by a term frequency (tf) of 1. Let’s take a look at the original text for these documents.\n\n\nCode\njh_clean |&gt;\n  filter(\n    project_number %in% c(\"1R61HL156248-01\", \"3U24TR001609-04S1\")\n  ) |&gt;\n  select(project_number, summary) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nproject_number\nsummary\n\n\n\n\n1R61HL156248-01\nno summary\n\n\n3U24TR001609-04S1\nno summary\n\n\n\n\n\n\n\nThere was no summary for either of these documents! The word “summary” had a really high TF-IDF because it was the only word (“no” is a stop word and was dropped) in these documents and it likely isn’t used very frequently in other documents. This is one of the benefits of doing this kind of check before going into modeling. Interestingly, though, neither “pain” nor “stress” are in the terms with the top 15 TF-IDF values. I imagine it’s because these words occur in a lot of the documents and, therefore, would not be very useful in distinguishing one project from another. So I’ll drop the two project with no summaries and go into the modeling.\n\n\nCode\njh_tidy &lt;- jh_tidy |&gt;\n  filter(\n    !project_number %in% c(\"1R61HL156248-01\", \"3U24TR001609-04S1\")\n  )"
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#modeling",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#modeling",
    "title": "Topic Modeling with R",
    "section": "Modeling",
    "text": "Modeling\nTo be able to perform text mining of any kind, including structural topic modeling, I first need to put my data into a sparse matrix. For this I’ll use the tidytext::cast_sparse() function.\n\n\nCode\njh_sparse &lt;- jh_tidy |&gt; cast_sparse(project_number, word, n)\n\ndim(jh_sparse)\n\n\n[1]   28 1023\n\n\nI now have a matrix of 28 and 1023 columns. There is now one column for each distinct word in my corpus of data. I’m going to create a structural model with 3 topics (number chosen arbitrarily) just to get an idea of how I might evaluate a model. I’m going to use the stm::stm() function and use the LDA algorithm to generate my model.\n\n\nCode\njh_lda_3 &lt;- jh_sparse |&gt;\n  stm(K = 3, init.type = \"LDA\", seed = 212, verbose = FALSE)\n\n\nNow I want to see what some of the most frequently occurring words are within each topic. For this I’m going to simply use the tidy method from stm on the model. By default, the tidy method extracts values of “beta” which tells us the probability that a word comes from a given topic.\n\n\nCode\njh_lda_3 |&gt;\n  tidy() |&gt; \n  group_by(topic) |&gt;\n  slice_max(beta, n = 10, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  labs(y = \"Beta\", x = NULL) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe word “pain” is in the top 2 most frequently occurring words in each topic and the word “opioid” occurs in the top 10 words of 2 of the topics. It looks like the model’s not doing a great job of separating topics based on this.\nAnother interesting metric to look at is “gamma” which is the probability that a given document is related to a given topic and this is also provided by our model output, which we can access by specifying the matrix = \"gamma\" argument in our tidy() function.\n\n\nCode\njh_lda_3 |&gt;\n  tidy(matrix = \"gamma\") |&gt;\n  mutate(topic = str_c(\"Topic\", topic, sep = \" \") |&gt; factor()) |&gt;\n  ggplot(aes(document, gamma, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(breaks = 1:nrow(distinct(jh_tidy, project_number))) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 1) +\n  labs(x = \"Document\", y = \"Gamma\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIt looks like each document except for Document 16 has a high probability of coming from only 1 topic. This is pretty good considering what the beta values showed.\nNext I’ll see if there’s a better number of topics to separate these documents into."
  },
  {
    "objectID": "posts/20240125-topic-modeling/topic-modeling.html#choosing-a-number-of-topics",
    "href": "posts/20240125-topic-modeling/topic-modeling.html#choosing-a-number-of-topics",
    "title": "Topic Modeling with R",
    "section": "Choosing a number of topics",
    "text": "Choosing a number of topics\nFirst I’m going to build a set of LDA models with values of K (number of topics) ranging from 2 to 25. I’m also going to create a hold-out data set to calculate the heldout-likelihood metric later. This is somewhat akin to using a cross-validation holdout set in other machine learning methodologies.\n\n\nCode\njh_tune_models &lt;- tibble(k = 2:24) |&gt;\n  mutate(\n    lda_mod = map(\n      k,\n      \\(x) stm(\n        jh_sparse,\n        K = x,\n        init.type = \"LDA\",\n        seed = 212,\n        verbose = FALSE\n      )\n    )\n  )\n\nheldout &lt;- make.heldout(jh_sparse)\n\n\nNow I’m going to extract the metrics that I want to use to evaluate my model. I’m going to focus on the approach of find the value of K with high held-out likelihood and semantic coherence metrics, but I’m also interested in looking at the residual and exclusivity metrics, so I’ll keep those as well.\n\n\nCode\njh_tune_results &lt;- jh_tune_models |&gt;\n  mutate(\n    exclusivity = map(lda_mod, exclusivity),\n    semantic_coherence = map(\n      lda_mod,\n      semanticCoherence,\n      documents = jh_sparse,\n      M = 10\n    ),\n    eval_heldout = map(lda_mod, eval.heldout, heldout$missing),\n    residual = map(lda_mod, checkResiduals, jh_sparse)\n  )\n\n\n\n\nCode\njh_tune_results |&gt;\n  transmute(\n    k,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\"),\n    Exclusivity = map_dbl(exclusivity, mean)\n  ) |&gt;\n  pivot_longer(-k, names_to = \"metrics\", values_to = \"value\") |&gt;\n  ggplot(aes(k, value, color = metrics)) +\n  geom_line(show.legend = FALSE) +\n  labs(x = \"Number of Topics\", y = NULL) +\n  facet_wrap(~ metrics, scales = \"free_y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe highest held-out likelihood score comes from the model with K = 17 and the highest semantic coherence comes from the model with K = 20, so there’s a little bit of a trade-off. In this case I’m going to go with parsimony and choose the model with K = 17. This model also has relatively high exclusivity. Notably, exclusivity and the residual seem to flatten out and semantic coherence increases dramatically at K &gt; 10.\nNow I’m going to look at both the K = 17 and K = 20 models to see whether one is clearly better. I’m going to start by looking at how easily each model would predict each document to belong to a specific topic. For that, I’m going to look at the maximum values of gamma for each document. For comparison I’m going to show the same plot for the approach of maximizing semantic coherence (K = 21).\n\n\nCode\njh_tune_models |&gt; \n  filter(k %in% c(17, 20)) |&gt;\n  mutate(\n    max_gammas = map(\n      lda_mod,\n      \\(x) tidy(x, matrix = \"gamma\") |&gt;\n        group_by(document) |&gt;\n        summarise(gamma = max(gamma), .groups = \"drop\")\n    ),\n    k_label = str_c(\"Number of Topics:\", k, sep = \" \") |&gt; fct_reorder(k)\n  ) |&gt;\n  select(-lda_mod) |&gt;\n  unnest(max_gammas) |&gt;\n  ggplot(aes(document, gamma, fill = factor(k))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(breaks = 1:24) +\n  labs(x = \"Document\", y = \"Maximum Gamma\") +\n  facet_wrap(~ k_label, ncol = 1) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIt looks like both models yield relatively high probabilities that each document corresponds to one specific topic across the board with only documents 23 and 24 having maximum gammas below 0.75, but the model with K = 17 generally performs a bit better.\nFor the purposes of visualization I’ll look at the top 5 words defining the topics for the 17-topic model.\n\n\nCode\njh_tune_models |&gt; \n  filter(k == 17) |&gt;\n  pluck(\"lda_mod\", 1) |&gt;\n  tidy() |&gt;\n  group_by(topic) |&gt;\n  slice_max(beta, n = 5, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  mutate(\n    topic = str_c(\"Topic\", topic, sep = \" \") |&gt;\n      factor(levels = str_c(\"Topic\", 1:17, sep = \" \"))\n  ) |&gt;\n  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  labs(y = \"Beta\", x = NULL) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThe word “pain” appears in the top 5 words of 7 topics; in further analysis I might consider making this a stop word along with words like “study” and “research”. But topic 11 seems to be about joint issues given the prevalence of the words “osteoarthritis” and “knee”; topic 12 seems to be about demographics with words like “american” and “urban” in the top 5 words; and topic 17 seems to be clearly about digestion issues with words like “pain”, “ibs”, and “abdominal” in the top 5 words.\nFinally, I’ll look at the titles grouped by each project’s maximum gamma value. You be the judge.\n\n\nCode\njh_tune_models |&gt; \n  filter(k == 17) |&gt;\n  pluck(\"lda_mod\", 1) |&gt;\n  tidy(matrix = \"gamma\") |&gt;\n  nest(data = -document) |&gt;\n  mutate(\n    data = map(data,\\(x) slice_max(x, gamma, n = 1, with_ties = FALSE)),\n    project_number = rownames(jh_sparse)\n  ) |&gt;\n  unnest(data) |&gt;\n  left_join(\n    jh_clean |&gt; select(project_number, project_title),\n    by = \"project_number\"\n  ) |&gt;\n  select(topic, project_title, gamma) |&gt;\n  mutate(gamma = round(gamma, 3)) |&gt;\n  arrange(topic, gamma) |&gt;\n  gt()\n\n\n\n\n\n\n\n\ntopic\nproject_title\ngamma\n\n\n\n\n1\nValidation of peripheral CGRP signaling as a target for the treatment of pain in chronic pancreatitis\n0.991\n\n\n1\nDevelopment of MRGPRX1 positive allosteric modulators as non-addictive therapies for neuropathic pain\n0.994\n\n\n2\nDEVELOPMENT &amp; MALLEABILITY FROM CHILDHOOD TO ADULTHOOD\n0.993\n\n\n3\nData Center for Acute to Chronic Pain Biosignatures\n0.991\n\n\n3\nSocial Networks among Native American caregivers participating in an evidence-based and culturally informed intergenerational intervention\n0.993\n\n\n4\nHEALthy ORCHARD: Developing plans for a Baltimore site of the HEALthy BCD study\n0.989\n\n\n5\nSubchondral Bone Cavities in Osteoarthritis Pain\n0.985\n\n\n5\nImplementing and Evaluating the Impact of Novel Mobile Harm Reduction Services on Overdose Among Women who use Drugs: The SHOUT Study\n0.989\n\n\n6\nImproving Function and Reducing Opioid Use for Patients with Chronic Low Back Pain in Rural Communities Through Improved Access to Physical Therapy Using Telerehabilitation\n0.992\n\n\n6\nUNDERSTANDING THE INTERSECTION BETWEEN OPIOIDS AND SUICIDE THROUGH THE SOUTHWEST HUB\n0.993\n\n\n7\nEvaluating the Role of the Orexin System in Circadian Rhythms of Sleep and Stress in Persons on Medication-Assisted Treatments for Opioid Use Disorder\n0.993\n\n\n7\nEvaluating Suvorexant for Sleep Disturbance in Opioid Use Disorder\n0.993\n\n\n8\nValidation of a New Large-Pore Channel as a Novel Target for Neuropathic Pain\n0.987\n\n\n9\nThe Short and Long-Term Dynamics of Opioid/Stimulant Use: Mixed Methods to Inform Overdose Prevention and Treatment Related to Polysubstance Use\n0.992\n\n\n9\nSympathetic-mediated sensory neuron cluster firing as a novel therapeutic target for neuropathic pain\n0.995\n\n\n10\n7/24 Healthy Brain and Child Development National Consortium\n0.990\n\n\n11\nA sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain\n0.989\n\n\n11\nA sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain\n0.990\n\n\n11\nIncreasing Participant Diversity in a 'Sequenced-Strategy to Improve Outcomes in People with Knee Osteoarthritis Pain (SKOAP)\n0.991\n\n\n11\nMentorship of Junior Investigators on HEAL-SKOAP\n0.992\n\n\n11\nA sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain\n0.994\n\n\n13\nDC Research Infrastructure Building &amp; Initiative to Reach, Engage, and Retain in MOUD Patients with OUD\n0.754\n\n\n13\nDC Research Infrastructure Building &amp; Initiative to Reach, Engage, and Retain in MOUD Patients with OUD\n0.754\n\n\n14\nQuantifying and Treating Myofascial Dysfunction in Post Stroke Shoulder Pain\n0.990\n\n\n14\nETHNIC DIFFERENCES IN ENDOGENOUS PAIN REGULATION: PET IMAGING OF OPIOID RECEPTORS\n0.993\n\n\n15\nSleep and Circadian Rhythm Phenotypes and Mechanisms Associated With Opioid Use Disorder Treatment Outcomes\n0.988\n\n\n16\nEffects of experimental sleep disruption and fragmentation on cerebral Mu-opioid receptor function, Mu-opioid receptor agonist analgesia, and abuse liability.\n0.991\n\n\n17\nHome-based transcutaneous electrical acustimulation for abdominal pain\n0.991"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "",
    "text": "In any binary classification analysis it is common to have to deal with heavily unbalanced data, i.e., the frequency of the majority class in the dependent variable is overwhelmingly greater than that of the minority class. One of the main reasons this can be problematic is that many classification algorithms are biased toward the majority class. If 95% of the observations belong to the majority class in the training data and the algorithm always predicts the majority class, then it will have achieved 95% accuracy.\nThere are many ways of dealing with unbalanced data including changing from a classification algorithm to an anomaly detection algorithm like an isoforest or a one-class SVM, using SMOTE to re-balance the data sets in pre-processing, up- or down-sampling, and changing the classification threshold. Here I will only deal with up-sampling and changing the classification threshold to see how it affects classification metrics by way of going through an exercise. I’ll first create and tune a base model then do the same for an up-sampled model using Tidymodels (Kuhn and Wickham 2020). Hyperparameter tuning will be done with the finetune package (Kuhn 2023a). I’ll then find the optimal threshold for each using the probably package (Kuhn, Vaughan, and Ruiz 2023). Finally I’ll show a comparison of their respective classification metrics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(themis)\nlibrary(finetune)\nlibrary(probably)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#introduction",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#introduction",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "",
    "text": "In any binary classification analysis it is common to have to deal with heavily unbalanced data, i.e., the frequency of the majority class in the dependent variable is overwhelmingly greater than that of the minority class. One of the main reasons this can be problematic is that many classification algorithms are biased toward the majority class. If 95% of the observations belong to the majority class in the training data and the algorithm always predicts the majority class, then it will have achieved 95% accuracy.\nThere are many ways of dealing with unbalanced data including changing from a classification algorithm to an anomaly detection algorithm like an isoforest or a one-class SVM, using SMOTE to re-balance the data sets in pre-processing, up- or down-sampling, and changing the classification threshold. Here I will only deal with up-sampling and changing the classification threshold to see how it affects classification metrics by way of going through an exercise. I’ll first create and tune a base model then do the same for an up-sampled model using Tidymodels (Kuhn and Wickham 2020). Hyperparameter tuning will be done with the finetune package (Kuhn 2023a). I’ll then find the optimal threshold for each using the probably package (Kuhn, Vaughan, and Ruiz 2023). Finally I’ll show a comparison of their respective classification metrics.\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(themis)\nlibrary(finetune)\nlibrary(probably)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#lending-club-data",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#lending-club-data",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Lending Club Data",
    "text": "Lending Club Data\nFor this project I’ll be using the lending_club data set from the modeldata (Kuhn 2023b) package. The data contain the Class variable which indicates whether a loan was “good” or bad”. For the purposes of this analysis I also will consider the “bad” outcome as the event to predict."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#data-exploration",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#data-exploration",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Data Exploration",
    "text": "Data Exploration\nThe first step is to load the data using the readr package from the Tidyverse (Wickham et al. 2019) and summarize it using the skimr package (Waring et al. 2022).\n\n\nCode\ndata(\"lending_club\", package = \"modeldata\")\n\nskim(lending_club) |&gt; select(-complete_rate)\n\n\n\nData summary\n\n\nName\nlending_club\n\n\nNumber of rows\n9857\n\n\nNumber of columns\n23\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n17\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\nordered\nn_unique\ntop_counts\n\n\n\n\nterm\n0\nFALSE\n2\nter: 7047, ter: 2810\n\n\nsub_grade\n0\nFALSE\n35\nC1: 672, B5: 624, A1: 612, B3: 607\n\n\naddr_state\n0\nFALSE\n50\nCA: 1324, TX: 900, NY: 767, FL: 731\n\n\nverification_status\n0\nFALSE\n3\nSou: 3742, Not: 3434, Ver: 2681\n\n\nemp_length\n0\nFALSE\n12\nemp: 3452, emp: 893, emp: 769, emp: 733\n\n\nClass\n0\nFALSE\n2\ngoo: 9340, bad: 517\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nfunded_amnt\n0\n15683.56\n8879.11\n1000.00\n8500.00\n15000.00\n21000.00\n40000.00\n▆▇▅▃▂\n\n\nint_rate\n0\n12.53\n4.89\n5.32\n8.49\n11.99\n15.31\n28.99\n▇▇▃▂▁\n\n\nannual_inc\n0\n80320.36\n53450.17\n0.00\n50000.00\n68900.00\n96000.00\n960000.00\n▇▁▁▁▁\n\n\ndelinq_2yrs\n0\n0.33\n0.89\n0.00\n0.00\n0.00\n0.00\n22.00\n▇▁▁▁▁\n\n\ninq_last_6mths\n0\n0.58\n0.88\n0.00\n0.00\n0.00\n1.00\n5.00\n▇▁▁▁▁\n\n\nrevol_util\n0\n51.73\n24.38\n0.00\n33.20\n51.80\n70.40\n144.30\n▅▇▇▂▁\n\n\nacc_now_delinq\n0\n0.01\n0.08\n0.00\n0.00\n0.00\n0.00\n2.00\n▇▁▁▁▁\n\n\nopen_il_6m\n0\n2.75\n2.93\n0.00\n1.00\n2.00\n3.00\n32.00\n▇▁▁▁▁\n\n\nopen_il_12m\n0\n0.74\n1.01\n0.00\n0.00\n0.00\n1.00\n20.00\n▇▁▁▁▁\n\n\nopen_il_24m\n0\n1.62\n1.70\n0.00\n0.00\n1.00\n2.00\n30.00\n▇▁▁▁▁\n\n\ntotal_bal_il\n0\n35286.92\n41923.62\n0.00\n9450.00\n23650.00\n46297.00\n585583.00\n▇▁▁▁▁\n\n\nall_util\n0\n60.31\n20.28\n0.00\n47.00\n62.00\n75.00\n198.00\n▂▇▂▁▁\n\n\ninq_fi\n0\n0.93\n1.47\n0.00\n0.00\n0.00\n1.00\n15.00\n▇▁▁▁▁\n\n\ninq_last_12m\n0\n2.19\n2.44\n0.00\n0.00\n2.00\n3.00\n32.00\n▇▁▁▁▁\n\n\ndelinq_amnt\n0\n12.17\n565.47\n0.00\n0.00\n0.00\n0.00\n42428.00\n▇▁▁▁▁\n\n\nnum_il_tl\n0\n8.64\n7.52\n0.00\n4.00\n7.00\n11.00\n82.00\n▇▁▁▁▁\n\n\ntotal_il_high_credit_limit\n0\n45400.75\n45103.21\n0.00\n16300.00\n34375.00\n60786.00\n554119.00\n▇▁▁▁▁\n\n\n\n\n\nThe above data summary shows some important things. First, there are 9,857 observations and 23 variables with no missing values. Also, the data has 6 factor variables and 17 numeric variables. It shows that most of the numeric variables are left skewed. The “sub_grade” and “addr_state” factor variables have 35 and 50 levels respectively. With so many unique values it’s very likely that some of the classes in each of those variables become overwhelmed by majority classes. While there are feature engineering steps that I can take such as consolidating infrequent classes or splitting up sub_grade into constituent parts, I’ll just allow the feature selection step to handle it. Finally, the “Class” factor variable (dependent variable) has two levels with the majority class – “good” – having 9,340 observations and the minority class – “bad” – having 517 observations.\nHere I’ll just look at the proportions of the Class variable.\n\n\nCode\nlending_club |&gt;\n  ggplot(aes(y = Class)) +\n  geom_bar(aes(x = (..count..) / sum(..count..))) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Proportion\",\n    y = NULL,\n    title = \"Frequency of loan outcome categories (Class)\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nHere we see a sever class unbalance with only about 5% of loans having a “bad” outcome."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#create-data-splits-and-other-common-elements",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#create-data-splits-and-other-common-elements",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Create Data Splits and Other Common Elements",
    "text": "Create Data Splits and Other Common Elements\nFor the analysis I’ll split the data into training (75%) and test (25%) sets stratified by the Class variable. I’ll then create 100 bootstraps of the training data for model tuning, also stratified by the Class variable. Stratification will ensure that both cuts of the data and the bootstraps retain the class unbalance as close to the proportions of the original data.\nHere I also create a list of features to control the model tuning process.\n\n\nCode\n# Create training/test split\nset.seed(95)\ntt_split &lt;- initial_split(lending_club, prop = 0.75, strata = Class)\n\n# Create the bootstrap object\nset.seed(386)\nbstraps &lt;- bootstraps(training(tt_split), times = 100, strata = Class)\n\n# Create the model object for the workflows\nlr_mod &lt;- logistic_reg(mode = \"classification\", engine = \"glm\")\n\n# Create a list of control options\nrace_control &lt;- control_race(\n  verbose = FALSE,\n  allow_par = FALSE,\n  burn_in = 3,\n  save_workflow = TRUE,\n  save_pred = TRUE\n)"
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-base-model",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-base-model",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Train Base Model",
    "text": "Train Base Model\nThe base recipe will perform the following steps:\n\nRemove variables with near-zero variance\nScale and center all numeric variables\nConvert categorical variables to dummy variables\nSelect variables based on mRMR using the colino package (Pawley, Kuhn, and Jacques-Hamilton 2023); the percentile threshold used to decide which variables to keep will be chosen through model tuning\n\nThe base recipe will be combined with a logistic regression model specification to put into a workflow. This workflow then goes through a Bayesian hyperparameter tuning process to find the best mRMR threshold as mentioned above. “Best” is defined as the full model specification that yields the highest area under the receiver operator curve (ROC-AUC) because I’ll be choosing the optimal probability threshold on the basis of this curve, even though it would be preferable to choose on the basis of the area under the precision-recall curve (PR-AUC) with unbalanced data. (Rosenberg 2022)\n\n\nCode\n# Create a base recipe\nbase_rec &lt;- recipe(Class ~ ., data = training(tt_split)) |&gt;\n  step_nzv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt;\n  step_select_mrmr(\n    all_predictors(),\n    threshold = tune(),\n    outcome = \"Class\"\n  )\n\n# First tune the base model to get the best set of variables with MRMR\nbase_wflow &lt;- workflow() |&gt; add_recipe(base_rec) |&gt; add_model(lr_mod)\n\nset.seed(40)\nbase_tuned &lt;- tune_race_win_loss(\n  base_wflow,\n  resamples = bstraps,\n  control = race_control,\n  grid = 10,\n  metrics = metric_set(pr_auc, roc_auc, accuracy)\n)\n\n# Get the workflow with the best MRMR threshold\nbest_base &lt;- select_best(base_tuned, \"roc_auc\")\n\n# Get the MRMR threshold from the best base model\nmrmr_threshold &lt;- best_base |&gt; pull(threshold)\n\n\nThe mRMR threshold associated with the best model is 0.9478018 ."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-upsampled-model",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#train-upsampled-model",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Train Upsampled Model",
    "text": "Train Upsampled Model\nThe upsampled model will only add the pre-processing step of upsampling the minority class to the same proportion as the majority class (or close to it) by resampling observations with a Class value from the minority class using the themis package (Hvitfeldt 2023). To ensure that this is the only difference, I’ll take the mRMR threshold that was tuned for the base model directly from the best base model and specify it as the threshold for the upsampling model.\n\n\nCode\n# Build a recipe for upsampling by first updating the MRMR step from the base recipe with the threshold from the tuning process then adding upsampling\nupsample_rec &lt;- base_rec\nselect_mrmr_step &lt;- tidy(upsample_rec) |&gt;\n  filter(type %in% \"select_mrmr\") |&gt;\n  pull(number)\n\nupsample_rec$steps[[select_mrmr_step]] &lt;- update(\n  upsample_rec$steps[[select_mrmr_step]],\n  threshold = mrmr_threshold\n)\n\nupsample_rec &lt;- upsample_rec |&gt;\n  step_upsample(Class, over_ratio = tune())\n\n# Create the upsample workflow object\nupsample_wflow &lt;- workflow() |&gt; add_recipe(upsample_rec) |&gt; add_model(lr_mod)\n\n# Tune the workflow containing the upsample recipe\nset.seed(40)\nupsample_tuned &lt;- tune_race_win_loss(\n  upsample_wflow,\n  resamples = bstraps,\n  control = race_control,\n  grid = 10,\n  metrics = metric_set(pr_auc, roc_auc, accuracy)\n)\n\n# Get the workflow with the best MRMR threshold\nbest_upsample &lt;- select_best(upsample_tuned, \"roc_auc\")\n\n\nThe best over-sampling ratio is 1.1065577 ."
  },
  {
    "objectID": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#compare-model-classification-metrics",
    "href": "posts/20230215-class-thresh-vs-upsampling/class-thresh-vs-upsampling.html#compare-model-classification-metrics",
    "title": "Optimal Threshold vs. Upsampling",
    "section": "Compare Model Classification Metrics",
    "text": "Compare Model Classification Metrics\nNow that I’ve run both models and gotten the best hyperparameters for each, I’ll finalize both models and ensure that I have the same sets of variables in the models.\n\n\nCode\n# Get evaluation datasets for both workflows\nset.seed(75)\neval_base &lt;- base_wflow |&gt;\n  finalize_workflow(best_base) |&gt;\n  last_fit(tt_split)\n\n# Build the evaluation tibble for the upsample case\nset.seed(212)\neval_upsample &lt;- upsample_wflow |&gt;\n  finalize_workflow(select_best(upsample_tuned, \"roc_auc\")) |&gt;\n  last_fit(tt_split)\n\neval_base |&gt;\n  extract_fit_engine() |&gt;\n  tidy() |&gt;\n  select(base = term) |&gt;\n  bind_cols(\n    eval_upsample |&gt;\n      extract_fit_engine() |&gt;\n      tidy() |&gt;\n      select(upsample = term)\n  ) |&gt;\n  kable(booktabs = TRUE) |&gt; \n  kable_styling(\"striped\")\n\n\n\n\n\nbase\nupsample\n\n\n\n\n(Intercept)\n(Intercept)\n\n\nint_rate\nint_rate\n\n\ninq_last_6mths\ninq_last_6mths\n\n\nopen_il_12m\nopen_il_12m\n\n\nsub_grade_G4\nsub_grade_G4\n\n\naddr_state_SD\naddr_state_SD\n\n\nverification_status_Verified\nverification_status_Verified\n\n\n\n\n\n\n\nThe above table shows that both models ended up with the same sets of variables. Next we’ll find the optimal threshold cut-offs for each set of predictions on the test data using Youden’s J-Index.\n\n\nCode\nbase_preds &lt;- tibble(\n  truth = collect_predictions(eval_base) |&gt; pull(Class),\n  estimate = collect_predictions(eval_base) |&gt; pull(.pred_bad),\n  pred_class = collect_predictions(eval_base) |&gt; pull(.pred_class)\n)\n\n# Find the optimal classification threshold for the base model\nopt_thresh_base &lt;- base_preds |&gt;\n  threshold_perf(truth, estimate, seq(0, 1, length.out = 1000)) |&gt;\n  filter(.metric %in% \"j_index\") |&gt;\n  slice_max(.estimate, with_ties = FALSE) |&gt;\n  pull(.threshold)\n\nupsample_preds &lt;- tibble(\n  truth = collect_predictions(eval_upsample) |&gt; pull(Class),\n  estimate = collect_predictions(eval_upsample) |&gt; pull(.pred_bad),\n  pred_class = collect_predictions(eval_upsample) |&gt; pull(.pred_class)\n)\n\n# Find the optimal classification threshold for the upsample model\nopt_thresh_upsample &lt;- upsample_preds |&gt;\n  threshold_perf(truth, estimate, seq(0, 1, length.out = 1000))|&gt;\n  filter(.metric %in% \"j_index\") |&gt;\n  slice_max(.estimate, with_ties = FALSE) |&gt;\n  pull(.threshold)\n\n\nThe optimal probability threshold for classifying a loan as bad with the base model is 0.0550551 and that for the upsampling model is 0.5525526\nEach table of predictions currently has 3 columns showing the actual class for a given observation (truth), the prediction probability associated with that observation being a bad loan (estimate), and a predicted class based on a probability threshold of 0.5 (pred_class). The table below shows the first few predictions from the base model.\n\n\nCode\nbase_preds |&gt;\n  slice_head(n = 10) |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling(\"striped\")\n\n\n\n\n\ntruth\nestimate\npred_class\n\n\n\n\ngood\n0.0350714\ngood\n\n\ngood\n0.0107027\ngood\n\n\ngood\n0.0297951\ngood\n\n\ngood\n0.0432709\ngood\n\n\ngood\n0.0335889\ngood\n\n\ngood\n0.0310299\ngood\n\n\ngood\n0.0452632\ngood\n\n\ngood\n0.0226798\ngood\n\n\ngood\n0.0170578\ngood\n\n\ngood\n0.0286983\ngood\n\n\n\n\n\n\n\nNow I’d like to add to each of these tables another column containing the predicted class based on the optimal probability threshold for each respective table. The below table shows a few cases for which the predicted class using a 0.5 probability threshold does not match the predicted class using the optimal probability threshold of 0.5525526 .\n\n\nCode\nbase_preds &lt;- base_preds |&gt;\n  mutate(\n    opt_pred_class = if_else(estimate &gt;= opt_thresh_base, \"bad\", \"good\") |&gt;\n      factor(levels = c(\"bad\", \"good\"))\n  )\n\nupsample_preds &lt;- upsample_preds |&gt;\n  mutate(\n    opt_pred_class = if_else(\n      estimate &gt;= opt_thresh_upsample,\n      \"bad\",\n      \"good\"\n    ) |&gt;\n      factor(levels = c(\"bad\", \"good\"))\n  )\n\nupsample_preds |&gt; \n  filter(opt_pred_class != pred_class) |&gt;\n  slice_head(n = 10) |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling(\"striped\")\n\n\n\n\n\ntruth\nestimate\npred_class\nopt_pred_class\n\n\n\n\ngood\n0.5030688\nbad\ngood\n\n\nbad\n0.5284232\nbad\ngood\n\n\ngood\n0.5373165\nbad\ngood\n\n\ngood\n0.5007787\nbad\ngood\n\n\ngood\n0.5173375\nbad\ngood\n\n\ngood\n0.5463947\nbad\ngood\n\n\nbad\n0.5297278\nbad\ngood\n\n\ngood\n0.5227147\nbad\ngood\n\n\ngood\n0.5284232\nbad\ngood\n\n\ngood\n0.5284232\nbad\ngood\n\n\n\n\n\n\n\nNext I’ll plot confusion matrices for each set of predictions.\n\n\nCode\nconf_mats &lt;- list(base = base_preds, upsample = upsample_preds) |&gt;\n  map(\n    \\(x) list(\n      conf_mat(x, truth = truth, estimate = pred_class),\n      conf_mat(x, truth = truth, estimate = opt_pred_class)\n    ) |&gt;\n      set_names(\"standard\", \"optimal\")\n  ) |&gt;\n  list_flatten()\n\nconf_mat_plots &lt;- conf_mats |&gt;\n  imap(\n    \\(x, y) {\n      plot_title &lt;- str_replace(y, \"_\", \" \") |&gt;\n        str_to_title()\n      \n      autoplot(x, type = \"heatmap\") +\n        ggtitle(plot_title) +\n        theme(plot.title = element_text(hjust = 0.5))\n    }\n  )\n\ncowplot::plot_grid(plotlist = conf_mat_plots)\n\n\n\n\n\n\n\n\n\nThe above confusion matrices show that both changing the probability threshold for classification as a bad loan and upsampling have a significant impact on the number of positive predictions (“bad” loan), even though most of those are predicted incorrectly with this model. It’s also interesting to note that doing both – changing the probability threshold and upsampling – shows very little improvement over doing just one or the other. Below are some classification metrics for each model specification.\n\n\nCode\nconf_mats |&gt;\n  imap(\n    \\(x, y) summary(x) |&gt;\n      select(-.estimator) |&gt;\n      set_names(\"metric\", str_replace(y, \"_\", \" \") |&gt; str_to_title())\n  ) |&gt;\n  reduce(left_join, by = \"metric\") |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling(\"striped\")\n\n\n\n\n\nmetric\nBase Standard\nBase Optimal\nUpsample Standard\nUpsample Optimal\n\n\n\n\naccuracy\n0.9436105\n0.7083164\n0.6908722\n0.7452333\n\n\nkap\n0.0103118\n0.1137115\n0.1086066\n0.1316040\n\n\nsens\n0.0073529\n0.6470588\n0.6691176\n0.6176471\n\n\nspec\n0.9982825\n0.7118935\n0.6921426\n0.7526836\n\n\nppv\n0.2000000\n0.1159420\n0.1126238\n0.1272727\n\n\nnpv\n0.9451220\n0.9718640\n0.9728425\n0.9711911\n\n\nmcc\n0.0285977\n0.1775336\n0.1757144\n0.1909560\n\n\nj_index\n0.0056355\n0.3589523\n0.3612602\n0.3703306\n\n\nbal_accuracy\n0.5028177\n0.6794762\n0.6806301\n0.6851653\n\n\ndetection_prevalence\n0.0020284\n0.3079108\n0.3277890\n0.2677485\n\n\nprecision\n0.2000000\n0.1159420\n0.1126238\n0.1272727\n\n\nrecall\n0.0073529\n0.6470588\n0.6691176\n0.6176471\n\n\nf_meas\n0.0141844\n0.1966480\n0.1927966\n0.2110553\n\n\n\n\n\n\n\nThe first thing that one might notices from the above is how high the accuracy is for the base model. It’s important to remember, though, that when a data set is so unbalanced all a model has to do is predict the majority class all the time and it will have high accuracy. That model, though, has a very low sensitivity and high specificity. The high specificity, again, is due to the fact that the actual data has an overwhelming proportion of the majority class. Looking across at the metrics for the other models one sees drops in both accuracy and specificity, but much more significant increases in sensitivity from the base model. It’s also useful to note that other metrics like balanced accuracy, kappa, mcc, and f_measure also improve. This is further evidence of the effect that the imbalance in classes has on the base model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "I’m an economist turned consultant/data scientist with strong interests in the divergence between inferential modeling and predictive modeling. Over the last few years I’ve started to really enjoy programming in the Tidyverse style and have even begun to work quite a bit with Tidymodels.\n\nI hold a B.S. in Applied Economics from Ithaca College and an M.S. in Economics from the City University of New York - City College. I enjoy projects that allow me to learn and implement new techniques ranging from different types of regression analysis to more advanced machine learning algorithms.\n\n\n\nOne of the most fun personal projects of mine has been to create the cepumd package. You can read more about it here.\nI also enjoy tutoring econometrics, data science, R, and statistics on Wyzant. Check out my profile!\nI built this website using some great, open-source tools including R, RStudio, GitHub, and Quarto. I’m very grateful to all the folks that work on these tools for what they do. I’m also grateful to Sam Csik for her blog on how to add a blog to a Quarto website."
  },
  {
    "objectID": "ce-dashboards.html",
    "href": "ce-dashboards.html",
    "title": "Consumer Expenditure Surveys Dashboards",
    "section": "",
    "text": "I built the Interactive CE Visualization Tool while working as an economist at the U.S. Bureau of Labor Statistics Consumer Expenditure Surveys program. The motivation was to help Public-Use Microdata (PUMD) users better understand some of the methods the program employs to generate annual expenditure estimates. The dashboard helps to answer questions about why it’s not statistically sound practice to generate mean expenditure estimates for certain cross-sections of the data, for example.\n\n\n\nInteractive CE Visualization Tool - 2015 Data"
  },
  {
    "objectID": "ce-dashboards.html#interactive-ce-visualization-tool",
    "href": "ce-dashboards.html#interactive-ce-visualization-tool",
    "title": "Consumer Expenditure Surveys Dashboards",
    "section": "",
    "text": "I built the Interactive CE Visualization Tool while working as an economist at the U.S. Bureau of Labor Statistics Consumer Expenditure Surveys program. The motivation was to help Public-Use Microdata (PUMD) users better understand some of the methods the program employs to generate annual expenditure estimates. The dashboard helps to answer questions about why it’s not statistically sound practice to generate mean expenditure estimates for certain cross-sections of the data, for example.\n\n\n\nInteractive CE Visualization Tool - 2015 Data"
  },
  {
    "objectID": "ce-dashboards.html#interactive-ce-dictionary",
    "href": "ce-dashboards.html#interactive-ce-dictionary",
    "title": "Consumer Expenditure Surveys Dashboards",
    "section": "Interactive CE Dictionary",
    "text": "Interactive CE Dictionary\nThe purpose of the Interactive CE Dictionary is to help data users interactively search for variables, codes, and other elements of the CE PUMD rather than read through various PDF documents.\n\n\n\nInteractive CE Dictionary"
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "",
    "text": "[Image from “Interpretable Machine Learning” by Christopher Molnar](https://christophm.github.io/interpretable-ml-book/shapley.html)\n\n\nRecently I worked on a project which required explanation of ML algorithms more precisely than with precision (I’ve heard you should always open with a bad joke). Being fairly new to the world of ML, I wasn’t aware of many ways to do that, but in my research I found DALEX developed by MI2DataLab. I was impressed, surprised, and excited. I very quickly developed some code for my project and even built a modelStudio dashboard as a proof of concept. Some of the advantages of DALEX are that it…\n\nis model agnostic with comparable results across models\nhas an easy, consistent interface\nproduces easily interpretable visualizations\ncan generates HTML widgets/pages through modelStudio\nprovides Various ways to look at contributions of each observation to model predictions\n\nUpon presenting my proof of concept, though, I learned about some other project requirements that highlighted three disadvantages of DALEX. The R package does not…\n\nrun in parallel\nprovide SHAP values\nwork natively (or easily) with Tidymodels objects\n\nThese disadvantages were enough to have me go back to the drawing board. I wondered if there were other ways to get the same types of visualizations and model break-downs through or from other R packages. I found…\n\nfastshap for computing Shapley values\npdp for generating partial dependence plots\nvip for generating variable importance using different methods\nbreakDown for explaining variable contributions to individual predictions\n\nSince I was using Shapley values as a basis on which to explain everything I used only fastshap and pdp, but found that vip would work well if I wanted to use another method for computing variable importance like permutation. I found it a bit challenging to use breakDown with Tidymodels objects, otherwise, it was pretty easy to work with.\nI’m going to demonstrate how I used these packages to “open the black box” of ML algorithms. In the example below I’ll show how I did it using an XG Boost and a linear model algorithm since ‘lm’ is very standard and XG Boost can present some challenging data formatting requirements. I’ll be using the Ames Housing Data included in the modeldata package. I’m using this version rather than that from the AmesHousing package because the good folks at RStudio who work on Tidymodels did a great job of cleaning up the data a bit (thank you!) in their version and I didn’t want to spend a lot of time doing that for this demo. Also, I chose this data set because it has some things in common with the data that I used in the aforementioned project that prompted this work such as having mixed data types (discrete and numeric) in the explanatory variables, a large enough number of observations for training (and tuning) and testing a model, and some problems with multicollinearity (or redundancy) in the explanatory variables. As a quick aside, I’m also very grateful to the folks that work on the Tidyverse of packages; I use these in nearly every R session."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#introduction-motivation",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#introduction-motivation",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "",
    "text": "[Image from “Interpretable Machine Learning” by Christopher Molnar](https://christophm.github.io/interpretable-ml-book/shapley.html)\n\n\nRecently I worked on a project which required explanation of ML algorithms more precisely than with precision (I’ve heard you should always open with a bad joke). Being fairly new to the world of ML, I wasn’t aware of many ways to do that, but in my research I found DALEX developed by MI2DataLab. I was impressed, surprised, and excited. I very quickly developed some code for my project and even built a modelStudio dashboard as a proof of concept. Some of the advantages of DALEX are that it…\n\nis model agnostic with comparable results across models\nhas an easy, consistent interface\nproduces easily interpretable visualizations\ncan generates HTML widgets/pages through modelStudio\nprovides Various ways to look at contributions of each observation to model predictions\n\nUpon presenting my proof of concept, though, I learned about some other project requirements that highlighted three disadvantages of DALEX. The R package does not…\n\nrun in parallel\nprovide SHAP values\nwork natively (or easily) with Tidymodels objects\n\nThese disadvantages were enough to have me go back to the drawing board. I wondered if there were other ways to get the same types of visualizations and model break-downs through or from other R packages. I found…\n\nfastshap for computing Shapley values\npdp for generating partial dependence plots\nvip for generating variable importance using different methods\nbreakDown for explaining variable contributions to individual predictions\n\nSince I was using Shapley values as a basis on which to explain everything I used only fastshap and pdp, but found that vip would work well if I wanted to use another method for computing variable importance like permutation. I found it a bit challenging to use breakDown with Tidymodels objects, otherwise, it was pretty easy to work with.\nI’m going to demonstrate how I used these packages to “open the black box” of ML algorithms. In the example below I’ll show how I did it using an XG Boost and a linear model algorithm since ‘lm’ is very standard and XG Boost can present some challenging data formatting requirements. I’ll be using the Ames Housing Data included in the modeldata package. I’m using this version rather than that from the AmesHousing package because the good folks at RStudio who work on Tidymodels did a great job of cleaning up the data a bit (thank you!) in their version and I didn’t want to spend a lot of time doing that for this demo. Also, I chose this data set because it has some things in common with the data that I used in the aforementioned project that prompted this work such as having mixed data types (discrete and numeric) in the explanatory variables, a large enough number of observations for training (and tuning) and testing a model, and some problems with multicollinearity (or redundancy) in the explanatory variables. As a quick aside, I’m also very grateful to the folks that work on the Tidyverse of packages; I use these in nearly every R session."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#getting-started",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#getting-started",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Getting Started",
    "text": "Getting Started\nPrior to diving into the data, first I loaded packages that I would use throughout the code.\n\n\nCode\npacman::p_load(tidyverse, tidymodels, yardstick, fastshap, kableExtra)\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()\n\n\nNext I loaded a script containing some functions that I defined (available on GitHub).\n\n\nCode\nsource(\"./tidy-ml-evaluation-funs.R\")\n\n\nAt this point I was ready to start with data exploration."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#data-exploration",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#data-exploration",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Data Exploration",
    "text": "Data Exploration\nFirst I loaded the data and made some small adjustments, for example, I learned in my tinkering that there are some unused levels in the “Neighborhood” variable.\n\n\nCode\n# Load the Ames data set\ndata(ames)\n\n# Convert month variable to a factor and set the largest category of each\n# factor to that factor's base level (for contrast matrix for LM)\nhousing &lt;- ames |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    mo_sold = as_factor(mo_sold),\n    across(where(is.factor), \\(x) fct_drop(x) |&gt; fct_infreq())\n  )\n\nneighborhoods &lt;- table(ames$Neighborhood) |&gt; \n  enframe(name = \"Neighborhood\", value = \"Count\") |&gt; \n  arrange(Count) |&gt;\n  slice(1:6)\n\nhl_neigh_rows &lt;- which(neighborhoods$Count == 0)\n\nneighborhoods |&gt;\n  kable(booktabs = TRUE) |&gt;\n  kable_styling() |&gt;\n  row_spec(hl_neigh_rows, bold = T, color = \"black\", background = \"yellow\")\n\n\n\n\n\nNeighborhood\nCount\n\n\n\n\nHayden_Lake\n0\n\n\nLandmark\n1\n\n\nGreen_Hills\n2\n\n\nGreens\n8\n\n\nBlueste\n10\n\n\nNorthpark_Villa\n23\n\n\n\n\n\n\n\nNext I just wanted to check whether there were any missing values in the data (there shouldn’t be).\n\n\nCode\nany(is.na(housing))\n\n\n[1] FALSE\n\n\nThen I wanted to see how many categories there were in each discrete variable to determine whether there might be any near-zero variance issues. When there are too many categories in a discrete variable, that variable would not do a good job of grouping (how informative is a category of size 1?) and if there are too few variables relative to the observation count then it’s likely that a small number of categories will be too dominant and possibly mask the effects of other categories.\n\n\nCode\nselect(housing, -sale_price) |&gt;\n  select_if(is.factor) |&gt;\n  summarise(across(everything(), \\(x) n_distinct(x, na.rm = TRUE))) |&gt;\n  pivot_longer(\n    everything(), \n    names_to = \"variable\", \n    values_to = \"num_categories\"\n  ) |&gt;\n  ggplot(aes(x = num_categories, y = fct_reorder(variable, num_categories))) +\n  geom_bar(stat = \"identity\", width = 0.8) +\n  labs(\n    y = \"Variable\",\n    x = \"Number of Categories\",\n    title = \"Number of Categories in Each Factor Variable\"\n  ) +\n  ml_eval_theme()\n\n\n\n\n\n\n\n\n\nThere are a few concerning variables, so next I’ll do a quick check for near-zero variance using the parameters from the recipes::step_nzv() function.\n\n\nCode\n# Check for near-zero variance\nuniqueCut &lt;- select(housing, -sale_price) |&gt;\n  select_if(is.factor) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"category\") |&gt; \n  group_by(variable) |&gt;\n  summarise(uniqueCut = (n_distinct(category) * 100) / n(), .groups = \"drop\")\n\nfreqCut &lt;- select(housing, -sale_price) |&gt;\n  select_if(is.factor) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"category\") |&gt;\n  count(variable, category, name = \"count\") |&gt;\n  group_by(variable) |&gt;\n  slice_max(count, n = 2, with_ties = FALSE) |&gt;\n  mutate(rank = c(\"first\", \"second\")) |&gt;\n  ungroup() |&gt;\n  select(-category) |&gt;\n  pivot_wider(names_from = rank, values_from = count) |&gt;\n  mutate(freqCut = first / second)\n\nhousing_nzv &lt;- left_join(freqCut, uniqueCut, by = \"variable\") |&gt;\n  mutate(nzv = as.numeric(uniqueCut &lt; 10 & freqCut &gt; 19))\n\nhousing_nzv_counts &lt;- housing_nzv |&gt;\n  count(nzv)\n\nhl_nzv_rows &lt;- which(housing_nzv_counts$nzv == 1)\n\nkable(slice(housing_nzv, 1:6), booktabs = TRUE) |&gt; kable_styling()\n\n\n\n\n\nvariable\nfirst\nsecond\nfreqCut\nuniqueCut\nnzv\n\n\n\n\nalley\n2732\n120\n22.766667\n0.1023891\n1\n\n\nbldg_type\n2425\n233\n10.407725\n0.1706485\n0\n\n\nbsmt_cond\n2616\n122\n21.442623\n0.2047782\n1\n\n\nbsmt_exposure\n1906\n418\n4.559809\n0.1706485\n0\n\n\nbsmt_fin_type_1\n859\n851\n1.009401\n0.2389078\n0\n\n\nbsmt_fin_type_2\n2499\n106\n23.575472\n0.2389078\n1\n\n\n\n\n\n\n\nCode\nkable(housing_nzv_counts, booktabs = TRUE) |&gt; \n  kable_styling() |&gt;\n  row_spec(hl_nzv_rows, bold = T, color = \"black\", background = \"yellow\")\n\n\n\n\n\nnzv\nn\n\n\n\n\n0\n28\n\n\n1\n13\n\n\n\n\n\n\n\nNext I want to look at the distributions of numeric data with violin plots from ggplot2.\n\n\nCode\nselect(housing, where(is.numeric)) |&gt;\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"value\") |&gt;\n  ggplot(aes(x = variable, y = value)) +\n  geom_violin(fill = \"gray\") +\n  facet_wrap(~ variable, scales = \"free\", ncol = 5) +\n  labs(\n    y = NULL,\n    x = NULL,\n    title = \"Distributions of Numeric Variables\"\n  ) +\n  ml_eval_theme() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\nWhile it looks like some of the numeric variables are multimodal or have skewed distributions, the real concern is that sales_price, the target variable, looks a bit right skewed (as financial data often are). I’ll normalize the numeric explanatory variables in my recipe later and perform a Yeo-Johnson transformation on the target variable for modeling.\nNow that I have an idea of what the variables look like individually, I’m going to see if there are any relationships that might suggest multicollinearity or redundancy. I’ll start with the discrete variables.\n\n\nCode\nfactor_names &lt;- select(housing, -sale_price, where(is.factor)) |&gt;  names()\n\nchi_sq_dat &lt;- crossing(var1 = factor_names, var2 = factor_names) |&gt;\n  mutate(\n    chi_sq_results = map2(\n      var1,\n      var2,\n      \\(x, y) select(housing, any_of(c(x, y))) |&gt;\n        table() |&gt;\n        chisq.test() |&gt;\n        broom::tidy()\n    )\n  ) |&gt;\n  unnest(chi_sq_results) |&gt;\n  select(var1, var2, p.value) |&gt;\n  pivot_wider(names_from = var2, values_from = p.value) |&gt;\n  column_to_rownames(\"var1\")\n\nchi_sq_dat[!upper.tri(chi_sq_dat)] &lt;- NA\n\nchi_sq_dat |&gt;\n  rownames_to_column(\"var1\") |&gt;\n  pivot_longer(-var1, names_to = \"var2\", values_to = \"p.value\") |&gt;\n  drop_na(p.value) |&gt;\n  ggplot(aes(fct_rev(var2), var1, color = p.value)) +\n  geom_point(size = 3) +\n  scale_color_viridis_c(direction = -1) +\n  labs(title = \"Chi-square Plot of Categorical Variables\", color = \"P-value\") +\n  ml_eval_theme() +\n  theme(\n    axis.title = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.border = element_blank(),\n    axis.line = element_line()\n  )\n\n\n\n\n\n\n\n\n\nA low p-value from a chi-square test indicates non-independence between 2 variables and this plot shows that there seems to be quite a lot of interdependence among these categorical variables. Next I’ll look at the relationships among the numeric variables with a correlation plot.\n\n\nCode\nselect(housing, -sale_price) |&gt;\n  select_if(is.numeric) |&gt;\n  corrr::correlate(method = \"spearman\", use = \"pairwise.complete.obs\") |&gt;\n  corrr::rearrange(absolute = FALSE) |&gt;\n  corrr::shave() |&gt;\n  corrr::rplot(colors = c(\"red\", \"white\", \"blue\")) +\n  labs(title = \"Correlation Plot of Numeric Variables\", color = \"Correlation\") +\n  ml_eval_theme() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.border = element_blank(),\n    axis.line = element_line()\n  )\n\n\n\n\n\n\n\n\n\nThere seems to be less of an interdependence problem among the numeric variables. To deal with this problem in both types of variables I’ll use the colino::step_mrmr() function step in my pre-processing recipe.\nNow I’ll move on to preparing the data for modeling."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling-prep",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling-prep",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Modeling Prep",
    "text": "Modeling Prep\nFirst I’ll split the data for training and testing and further split the training data into cross-validation folds.\n\n\nCode\n# Set the random number seed\nset.seed(485)\n\n# Split the data\nhousing_split &lt;- initial_split(housing, prop = 0.75)\nhousing_train &lt;- training(housing_split)\nhousing_test &lt;- testing(housing_split)\n\n# Create the CV dataframe\nhousing_folds &lt;- vfold_cv(housing_train, v = 10)\n\n\nNext I’ll write my pre-processing recipes; one for each type of model. The only difference between the two recipes is that in the linear model recipe I set one_hot = FALSE to keep the reference level out of the contrast matrix. The steps are as follows:\n\nDefine roles for the outcome and predictor variables\nRemove near-zero-variance variables\nNormalize all numeric predictors\nPerform a Yeo-Johnson transformation on the target variable\nApply minimum Redundancy Maximum Relevance (mRMR) feature selection\nConvert discrete variables to dummy variables\n\n\n\nCode\n# XGBoost recipe\nxgb_rec &lt;- recipe(housing_train) |&gt;\n  update_role(sale_price, new_role = \"outcome\") |&gt;\n  update_role(-has_role(\"outcome\"), new_role = \"predictor\") |&gt;\n  step_nzv(all_predictors()) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_YeoJohnson(all_outcomes()) |&gt;\n  colino::step_select_mrmr(\n    all_predictors(),\n    outcome = \"sale_price\",\n    threshold = 0.9,\n    skip = TRUE\n  ) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\n# Linear Model recipe\nnum_steps &lt;- length(xgb_rec$steps)\nlm_rec &lt;- xgb_rec\nlm_rec$steps[[num_steps]] &lt;- update(lm_rec$steps[[num_steps]], one_hot = FALSE)\n\n\nNext I want to generate a tuning grid for the XGB model. To limit the number of features selected randomly at each node I’ll first get the number of features that will be in the data after the pre-processing recipe has been applied. I also set the range for the learning rate a bit arbitrarily because I found that the default range resulted in the model learning too quickly and not keeping enough information from early iterations.\n\n\nCode\n# Get the number of features in the training data for XGB tuning grid\nn_features &lt;- bake(prep(xgb_rec), new_data = housing_train) |&gt;\n  ncol() |&gt;\n  magrittr::subtract(1) |&gt;\n  sqrt()\n\n# Create a tuning grid for XGB\nset.seed(55)\nxgb_grid &lt;- grid_random(\n  mtry(c(1, floor(n_features))),   # Range of number of features to try\n  trees(),   # Range of number of trees\n  learn_rate(range = c(-7, 1)),   # Learning rate\n  loss_reduction(),\n  size = 50\n)\n\n\nNow I’ll create the workflow objects that I’ll use in model tuning and evaluation.\n\n\nCode\n# Create model objects for both XGB and LM\nxgb_mod &lt;- boost_tree(\n  mtry = tune(), \n  trees = tune(), \n  learn_rate = tune(),\n  loss_reduction = tune(),\n  mode = \"regression\"\n) |&gt;\n  set_engine(\"xgboost\")\n\nlm_mod &lt;- linear_reg(mode = \"regression\") |&gt; set_engine(\"lm\")\n\n# Create workflow objects for tuning\nxgb_wflow &lt;- workflow() |&gt; add_recipe(xgb_rec) |&gt; add_model(xgb_mod)\nlm_wflow &lt;- workflow() |&gt; add_recipe(lm_rec) |&gt; add_model(lm_mod)"
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#modeling",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Modeling",
    "text": "Modeling\n\nTuning the Models\nBecause model tuning can be quite computationally intensive (and time consuming) even for a small example like the one I’m working with, I first set up a parallel back end to run simple processes simultaneously. Some important things to note are that I had to ensure that the colino package was available to each processor and that I set the same random seed on each processor for reproducibility. Once that’s done, I tune the models.\n\n\nCode\n# Register and set up the parallel backend\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE) - 1)\ndoParallel::registerDoParallel(cl)\ninvisible(parallel::clusterEvalQ(cl, set.seed(853)))\n\n# Tune both models\nxgb_tune &lt;- tune_grid(\n  xgb_wflow,\n  grid = xgb_grid,\n  resamples = housing_folds,\n  metrics = yardstick::metric_set(rmse, rsq_trad),\n  control = control_grid(allow_par = TRUE, parallel_over = \"everything\")\n)\n\nlm_tune &lt;- fit_resamples(\n  lm_wflow,\n  resamples = housing_folds,\n  metrics = metric_set(rmse, rsq_trad),\n  control = control_grid(allow_par = TRUE, parallel_over = \"resamples\")\n)\n\n# Close the connection to the cluster\ndoParallel::stopImplicitCluster()\n\n\n\n\nModel Selection and Fitting\nNow that the model hyperparameters have been tuned (at least for XGB) I can pull the models that performed best from our cross-validation process.\n\n\nCode\n# Get the best models from each tuning process\nbest_models &lt;- list(xgb = xgb_tune, lm = lm_tune) |&gt;\n  map(select_best, metric = \"rmse\")\n\n# Collect tuning metrics for each tuning process\ntune_metrics &lt;- list(xgb = xgb_tune, lm = lm_tune) |&gt;\n  map(collect_metrics)\n\n# Fit the models with the best parameters to the entire training data set\nfinal_wflows &lt;- map2(\n  list(xgb = xgb_wflow, lm = lm_wflow),\n  best_models,\n  \\(x, y) finalize_workflow(x, y) |&gt; fit(data = housing_train)\n)\n\n\n\n\nModel Evaluation\nWith the best performing models from cross-validation now fit to the entire training data set, I can evaluate these models against the test data.\n\n\nCode\n# Evaluate each model with the test data and store 'last_fit' objects\nset.seed(485)\nfinal_wflow_evals &lt;- map(\n  final_wflows,\n  \\(x) last_fit(\n    x,\n    split = housing_split,\n    metrics = metric_set(rmse, mae, mape, rsq_trad)\n  )\n)\n\n\nNow I can see how each model performed on the test data.\n\n\nCode\n# Generate a dataframe of model metrics to compare the two models\nmodel_metrics &lt;- final_wflow_evals |&gt;\n  map2_df(\n    names(final_wflow_evals),\n    ~ collect_metrics(.x) |&gt; \n      select(.metric, .estimate) |&gt; \n      pivot_wider(names_from = .metric, values_from = .estimate) |&gt;\n      mutate(algorithm = .y) |&gt;\n      relocate(algorithm)\n  )\n\nkable(model_metrics, booktabs = TRUE) |&gt; kable_styling()\n\n\n\n\n\nalgorithm\nrmse\nmae\nmape\nrsq_trad\n\n\n\n\nxgb\n0.1365643\n0.0954771\n0.8758136\n0.8525943\n\n\nlm\n0.1518358\n0.1056075\n0.9692341\n0.8177832\n\n\n\n\n\n\n\nIt looks like the XGB model outperformed the Linear Model on every metric, but not by very much. Both seem to have performed pretty well at predicting home sale prices. It’s important to keep in mind here that the data were transformed, which is why the RMSE and MAE are so small. Now let’s go ahead and open these “black boxes”."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#opening-the-black-box",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#opening-the-black-box",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Opening the “Black Box”",
    "text": "Opening the “Black Box”\n\nGenerate SHAP Values\nAs stated earlier, this part of the analysis will be based on SHAP values. This explanation from Christoph Molnar’s book Interpretable Machine Learning is a great primer on what SHAP values are and how they can help someone understand the effect of a given feature/variable on a prediction.\nWith that in mind, now that the models have been tuned, trained, and evaluated, I can generate SHAP values to understand what’s going on under the hood (kinda). Note that this is still being run in parallel until the end of this portion.\n\n\nCode\n# Get matrices of training features for each model\ntraining_features &lt;- final_wflows |&gt; \n  map(\\(x) pull_workflow_mold(x) |&gt; pluck(\"predictors\"))\n\n# Register and set up the parallel backend\ncl &lt;- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE) - 1)\ndoParallel::registerDoParallel(cl)\ninvisible(parallel::clusterEvalQ(cl, set.seed(44)))\n\nshap &lt;- final_wflows |&gt; map(get_shap)\n\n# Get SHAP variable importance features\nvar_imp &lt;- shap |&gt; map(get_shap_imp)\n\n# Close the connection to the cluster\nparallel::stopCluster(cl)\n\n\n\n\nPlot Relationships Between SHAP and the Training Data\nI now have my SHAP values and can begin to visualize a few different things. The first visualization will be a SHAP summary plot, which I also found in Christoph Molnar’s Interpretable Machine Learning. When I saw this plot I was amazed at how much information it actually contains once you grasp the dimensionality.\n\n\nCode\n# Pull data from the outermost point on the SHAP summary plot\npoint_dat_xgb &lt;- get_shap_summary(\n  var_imp$xgb, \n  shap$xgb, \n  training_features$xgb, \n  max_features = 1\n) |&gt;\n  mutate(shap_value = as.numeric(shap_value)) |&gt;\n  slice_max(shap_value, with_ties = FALSE)\n\npoint_dat_lm &lt;- get_shap_summary(\n  var_imp$lm, \n  shap$lm, \n  training_features$lm, \n  max_features = 1\n) |&gt;\n  mutate(shap_value = as.numeric(shap_value)) |&gt;\n  slice_max(shap_value, with_ties = FALSE)\n\n# Put the data from that point into a string object\npoint_annotation_xgb &lt;- str_glue(\n  \"Row = {point_dat_xgb$id}\n  Variable = {point_dat_xgb$variable}\n  SHAP Value = {round(point_dat_xgb$shap_value, 3)}\n  Feature Value = {round(point_dat_xgb$feature_value, 3)}\"\n)\n\npoint_annotation_lm &lt;- str_glue(\n  \"Row = {point_dat_lm$id}\n  Variable = {point_dat_lm$variable}\n  SHAP Value = {round(point_dat_lm$shap_value, 3)}\n  Feature Value = {round(point_dat_lm$feature_value, 3)}\"\n)\n\n# Create SHAP summaries containing data frames and plots\nshap_summary_plots &lt;- list(\n  var_imp, \n  shap, \n  training_features\n) |&gt;\n  pmap(get_shap_summary, max_features = 10) %&gt;%\n  map2(c(\"XG Boost\", \"Linear Regression\"), plot_shap_summary)\n\ntotal_bsmt_sf_pdp &lt;- final_wflows |&gt;\n  map2(c(\"XG Boost\", \"Linear Regression\"), plot_pdp, pred_var = total_bsmt_sf)\n\n\n\n\nCode\n# Generate the SHAP summary plot with an annotation for the outermost point\nset.seed(23)\nshap_summary_plots$xgb + \n  geom_segment(\n    aes(x = 0.5, xend = 0.575, y = 8, yend = 10.01), \n    color = \"black\",\n    arrow = arrow(length = unit(0.03, \"npc\"))\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 0.46, \n    y = 6.8, label = point_annotation_xgb, \n    hjust = \"center\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Generate the SHAP summary plot with an annotation for the outermost point\nset.seed(23)\nshap_summary_plots$lm + \n  geom_segment(\n    aes(x = 0.79, xend = 0.9, y = 8, yend = 10), \n    color = \"black\",\n    arrow = arrow(length = unit(0.03, \"npc\"))\n  ) +\n  annotate(\n    geom = \"text\", \n    x = 0.8, \n    y = 6.8, label = point_annotation_lm, \n    hjust = \"center\"\n  )\n\n\n\n\n\n\n\n\n\nThe first plot above shows that row 1693 has a positive SHAP value of 0.5879337 for the gr_liv_area (0.095) in the XGB model, which had an above average value of 4.1097005 in the training data. When considering all the observations collectively the feature value is ideally positively related with the SHAP value. Next I’ll look at partial dependence plots to show the effect that the gr_liv_area has on sale_price according to each model.\n\n\nCode\ngr_liv_area_pdp &lt;- final_wflows |&gt;\n  map2(c(\"XG Boost\", \"Linear Regression\"), plot_pdp, pred_var = gr_liv_area)\n\ngr_liv_area_pdp$xgb\n\n\n\n\n\n\n\n\n\nCode\ngr_liv_area_pdp$lm\n\n\n\n\n\n\n\n\n\nAccording to both models the gr_liv_area variable positively effects the predicted sale price of a home in the training data, which seems consistent with general knowledge about home prices given that gr_liv_area is a measure of the above ground living area (more commonly known as “square footage”). It’s important to note, however, that, unlike the coefficient in a regression model, one should not make inferences from a partial dependency beyond the dimensions of the training data. A partial dependency plot is meant only to provide an overview on the broad relationship between an explanatory variable and the target variable.\nLastly, I can look at how each feature affects a particular observation with a contribution plot. I’ll consider the house observed in row 2049 of the training data set.\n\n\nCode\nobs_2409_contrib &lt;- shap |&gt;\n  map2(\n    c(\"XG Boost\", \"Linear Regression\"), \n    get_contributions, \n    rnum = 2049, \n    nfeat = 15 \n  )\n\n\n\n\nCode\nobs_2409_contrib$xgb$contrib_plot\n\n\n\n\n\n\n\n\n\n\n\nCode\nobs_2409_contrib$lm$contrib_plot\n\n\n\n\n\n\n\n\n\nThe above XG Boost contribution plot shows that total_bsmt_sf had the largest positive effect on the predicted sale price of the sale in observation 2049 and having garage_finish_Unf (having an unfinished garage) had the largest negative effect. The Linear Regression contribution plot shows that gr_liv_area had the largest positive effect and neighborhood_Brookside had the largest negative effect on the predicted sale price. However, it’s interesting to note that garage_finish_Fin, the opposite of garage_finish_Unf, had a negative effect in the Linear Regression."
  },
  {
    "objectID": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#final-thoughts",
    "href": "posts/20210829-open-blackbox-tidy/open-blackbox-tidy.html#final-thoughts",
    "title": "Opening the ML ‘Black Box’ with Shapley Values",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nWhile these methods of looking into machine learning algorithms are not designed for inference, they do go a long way in helping to elucidate the relationships or patterns that machine learning algorithms find. They help answer the questions about why some predictions come out the way that they do, whether they make sense or not."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html",
    "title": "Introduction to cepumd",
    "section": "",
    "text": "The purpose of cepumd is to make working with Consumer Expenditure Surveys (CE) Public-Use Microdata (PUMD) easier toward calculating mean, weighted, annual expenditures (henceforth “mean expenditures”). The challenges cepumd seeks to address deal primarily with pulling together the necessary data toward this end. Some of the overarching ideas underlying the package are as follows:\n\n\n\nUse a Tidyverse framework for most operations and be (hopefully) generally Tidyverse friendly\nBalance the effort to make the end user’s experience with CE PUMD easier while being flexible enough to allow that user to perform any analysis with the data they wish\nOnly designed to help users calculate mean expenditures on and of the consumer unit (CU), i.e., not income, not assets, not liabilities, not gifts."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#motivation",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#motivation",
    "title": "Introduction to cepumd",
    "section": "",
    "text": "The purpose of cepumd is to make working with Consumer Expenditure Surveys (CE) Public-Use Microdata (PUMD) easier toward calculating mean, weighted, annual expenditures (henceforth “mean expenditures”). The challenges cepumd seeks to address deal primarily with pulling together the necessary data toward this end. Some of the overarching ideas underlying the package are as follows:\n\n\n\nUse a Tidyverse framework for most operations and be (hopefully) generally Tidyverse friendly\nBalance the effort to make the end user’s experience with CE PUMD easier while being flexible enough to allow that user to perform any analysis with the data they wish\nOnly designed to help users calculate mean expenditures on and of the consumer unit (CU), i.e., not income, not assets, not liabilities, not gifts."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#overview-of-the-ce-and-ce-pumd",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#overview-of-the-ce-and-ce-pumd",
    "title": "Introduction to cepumd",
    "section": "Overview of the CE and CE PUMD",
    "text": "Overview of the CE and CE PUMD\nFirst a little history…\nThe first Consumer Expenditure Survey happened in 1888 (https://www.bls.gov/opub/hom/cex/history.htm), it was first used to revise CPI weights in 1972-1973, and it has been collected on a monthly basis since 1979. For a little bit more detail on the history of the CE, check out the slide deck of a presentation delivered by Steve Henderson (former Chief of the Branch of Information and Analysis) and Adam Safir (current Division Chief of CE) called 130 Years of the Consumer Expenditure Surveys (CE): 1888 - 2018\nFrom the CE home page:\n\n“The Consumer Expenditure Surveys (CE) program provides data on expenditures, income, and demographic characteristics of consumers in the United States. The CE program provides these data in tables, LABSTAT database, news releases, reports, and public use microdata files.\n\n\nCE data are collected by the Census Bureau for BLS in two surveys, the Interview Survey for major and/or recurring items and the Diary Survey for more minor or frequently purchased items. CE data are primarily used to revise the relative importance of goods and services in the market basket of the Consumer Price Index. The CE is the only Federal household survey to provide information on the complete range of consumers’ expenditures and incomes. Here is an overview of the CE program and its methods.”\n\nSome important things to note are that expenditure data are collected through two different survey instruments (Diary and Interview), expenditure categories are organized hierarchically, and data are stored across thousands of files to which the CE provides access through their website. Also, given the length of the program, it would be difficult to harmonize data across all those years and files, so there are some inconsistencies in the way data are stored, which cepumd seeks to address (more on this further down).\nPlease visit the following pages to learn more about the CE program overall and CE PUMD more specifically.\n\nCE homepage: (https://www.bls.gov/cex/)\nCE PUMD page: (https://www.bls.gov/cex/pumd.htm)\nCE PUMD Getting Started Guide: https://www.bls.gov/cex/pumd-getting-started-guide.htm\nCE Dictionary for Interview and Diary Surveys (XLSX download) (https://www.bls.gov/cex/pumd/ce_pumd_interview_diary_dictionary.xlsx)\nCE PUMD published tables: (https://www.bls.gov/cex/tables.htm)\nCE PUMD Handbook of Methods: https://www.bls.gov/opub/hom/cex/\nCE Frequently Asked Questions: https://www.bls.gov/cex/csxfaqs.htm"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#challenges-addressed-by-cepumd",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#challenges-addressed-by-cepumd",
    "title": "Introduction to cepumd",
    "section": "Challenges addressed by cepumd",
    "text": "Challenges addressed by cepumd\ncepumd seeks to address challenges in three categories: data gathering/organization; managing data inconsistencies; and calculating weighted, annual metrics.\n\nData wrangling\n\nConvert hierarchical grouping (HG) files to data tables using ce_hg()\nHelp the user identify the Universal Classification Codes (UCCs) related to their analysis using a combination of ce_hg() and ce_uccs()\nCombine all required files and variables using ce_prepdata()\n\nManaging data inconsistencies\n\nProvide the ability to re-code variable categories using the CE Dictionary for Interview and Diary Surveys\nResolve some inconsistencies such as differences code definitions between the Interview and Diary (check the definitions of the “FAM_TYPE” variable categories in 2015 for an example)\nProvide useful errors or warnings when there are multiple categories of something the user is trying to access, e.g., some titles in the hierarchical grouping files (“stub” or “HG” files) repeat and requires more careful selection of UCCs\n\nCalculating weighted, annual metrics\n\nCalculate a mean expenditure with ce_mean() or expenditure quantile with ce_quantile()\nAccount for the factor (annual vs. quarterly expenditure)\nAccount for the “months in scope” of a given consumer unit (CU)\nAnnualize expenditures for either Diary or Interview expenditures\nIntegrate Interview and Diary data as necessary\n\n\nSource code and other package information is available at https://github.com/arcenis-r/cepumd"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#cautions-and-recommendations",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#cautions-and-recommendations",
    "title": "Introduction to cepumd",
    "section": "Cautions and recommendations",
    "text": "Cautions and recommendations\n\nEstimates produced using PUMD, which is top-coded by the CE and has some records suppressed to protect respondent confidentiality, will not match the published estimates released by the CE in most cases. The CE’s published estimates are based on confidential data that are not top-coded nor have records suppressed. You can learn more at CE Protection of Respondent Confidentiality.\nWhen calculating estimates for sub-samples or cross-sections of data it is best to stick to the combinations of variables that the CE uses in it’s publication tables, e.g., income, geography, composition of CU, size of CU. This is because CE data are collected using a stratified, random sample (a.k.a., “representative sample”) and only analyses conducted using the stratification variables are statistically valid. Using other variables can be helpful to understand spending across different groups, but unweighted estimates are likely more useful for this. cepumd currently does not support unweighted estimates, but data for such an analysis can be prepared using ce_prepdata().\nQuantiles should only be generated using data from 1 survey instrument as the samples for the Interview and Diary are different.\nCheck the expenditure category in the appropriate HG file to ensure that it is the category for which you intend to generate an estimate.\nStore an HG object in the environment and call that directly in ce_prepdata()."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#installation",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#installation",
    "title": "Introduction to cepumd",
    "section": "Installation",
    "text": "Installation\nYou can install the development version of cepumd from its GitHub repo.\n\n\nCode\npak::pkg_install(\"arcenis-r/cepumd\")"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#key-cepumd-functions",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#key-cepumd-functions",
    "title": "Introduction to cepumd",
    "section": "Key cepumd functions",
    "text": "Key cepumd functions\n\nThe workhorse of cepumd is ce_prepdata(). It merges the household characteristics file (FMLI/-D) with the corresponding expenditure tabulation file (MTBI/EXPD) for a specified year, adjusts weights for months-in-scope and the number of collection quarters, adjusts some cost values by their periodicity factor (some cost categories are represented as annual figures and others as quarterly). With the recent update it only requires the first 3 arguments to function: the year, the survey type, and one or more valid UCCs. ce_prepdata() now creates all of the other necessary objects within the function if not provided.\nThere are two functions for wrangling hierarchical grouping data into more usable formats:\n\nce_hg() pulls the requested type of HG file (Interview, Diary, or Integrated) for a specified year.\nce_uccs() filters the HG file for the specified expenditure category and returns either a data frame with only that section of the HG file or the Universal Classification Codes (UCCs) that make up that expenditure category.\n\nThere are two functions that the user can use to calculate CE summary statistics:\n\nce_mean() calculates a mean expenditure, standard error of the mean, coefficient of variation, and an aggregate expenditure.\nce_quantiles() calculates weighted expenditure quantiles. It is important to note that calculating medians for integrated expenditures is not recommended because the calculation involves using weights from both the Diary and Survey instruments."
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#example-workflows",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#example-workflows",
    "title": "Introduction to cepumd",
    "section": "Example workflows",
    "text": "Example workflows\nThe following are a few sample workflows that show how cepumd can be used. Before jumping into those I’ll first install and load the necessary packages and store some CEPUMD files. I’ll keep the path to those files in a variable called ce_data_dir.\n\n\nCode\npacman::p_load(knitr, readxl, tidyverse, cepumd)\n\n\n\nSimple workflow: Integrated pet expenditures\nThe following is an example of how someone might go about using cepumd to calculate a 2021 annual, weighted estimate of mean expenditures on pets for all of the U.S. using CE integrated data. This is just a quick and easy calculation.\n\n\nCode\ninteg21_hg &lt;- ce_hg(\n  2021,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\nce_prepdata(\n  2021,\n  integrated,\n  integ21_hg,\n  uccs = ce_uccs(integ21_hg, expenditure = \"Pets\", ucc_group = \"PETS\"),\n  dia_zp = file.path(ce_data_dir, \"diary21.zip\"),\n  int_zp = c(\n    file.path(ce_data_dir, \"intrvw20.zip\"),\n    file.path(ce_data_dir, \"intrvw21.zip\")\n  )\n) |&gt;\n  ce_mean() |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nagg_exp\nmean_exp\nse\ncv\n\n\n\n\n130886736176\n981.3035\n53.5767\n5.459748\n\n\n\n\n\nYup… that’s all it takes. I simply ran ce_hg to get the hierarchical grouping (stub) file for integrated expenditures for 2021; then ran ce_prepdata() with the year, the survey type, the stub file, uccs I needed, and the file paths to where I downloaded the data files; then I piped that directly into ce_mean(). An important thing to notice is that I provided two file paths to the int_zp argument. I did this because calculating integrated CE annual estimates actually requires 5 quarters of data from the Interview survey. Some of the data for calculating 2021 estimates is provided in the 2020 Interview data.This is one of the reasons it’s important to be familiar with CE methodology and how it changes over time when working with CE PUMD. Prior to 2020, file storing practices were different as stated in the Getting Started Guide Interview Survey section.\n\n\nSlightly more advanced workflow: Used Car & Truck Expenditures by Urbanicity\nIn this example I’ll calculate estimated annual expenditures on new and used cars by urbanicity also for 2021. Once the data are prepped with ce_data() I’ll just nest the data by urbanicity and run ce_means() and ce_quantiles() on the nested data sets. Since the overwhelming number of reports of vehicle purchases occur in the Interview survey, I’ll only use Interview data.\nFirst I’ll get the stub file and filter it for categories involving new or used cars.\n\n\nCode\nint21_hg &lt;- ce_hg(\n  2021,\n  interview,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\nint21_hg |&gt;\n  filter(str_detect(title, \"[C|c]ars\")) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n4\nCars and trucks, new\nNEWCARS\nG\n1\n\n\n5\nNew cars\n450110\nI\n1\n\n\n4\nCars and trucks, used\nUSEDCARS\nG\n1\n\n\n5\nUsed cars\n460110\nI\n1\n\n\n\n\n\nSo there’s one UCC for “New cars” and one for “Used cars”. I’ll use the code above to grab those UCCs and prepare my data.\n\n\nCode\ncar_data &lt;- ce_prepdata(\n  2021,\n  interview,\n  int21_hg,\n  uccs = int21_hg |&gt;\n    filter(str_detect(title, \"[C|c]ars\"), !is.na(as.numeric(ucc))) |&gt;\n    pull(ucc),\n  bls_urbn,  # &lt;------- this is the variable for urbanicity\n  int_zp = c(\n    file.path(ce_data_dir, \"intrvw20.zip\"),\n    file.path(ce_data_dir, \"intrvw21.zip\")\n  ),\n  recode_variables = TRUE,\n  dict_path = file.path(ce_data_dir, \"ce-data-dict.xlsx\")\n)\n\ncar_data |&gt;\n  group_nest(bls_urbn) |&gt;\n  mutate(ce_ann_means = map(data, ce_mean)) |&gt;\n  select(-data) |&gt;\n  unnest(ce_ann_means) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nbls_urbn\nagg_exp\nmean_exp\nse\ncv\n\n\n\n\nUrban\n168458799558\n1341.7184\n106.2495\n7.918912\n\n\nRural\n4756065697\n591.5108\n155.0020\n26.204421\n\n\n\n\n\nGetting the annual, weighted estimate of the median (or another quantile) would be just as easy. Since I’m using interview data only here (it would be bad practice to try to calculate quantiles with integrated data), this would be a good example. I’ll calculate the first percentile and the median along with the 0.991 through 0.999 quantiles for the overall sample rather than breaking it down by urbanicity.\n\n\nCode\nce_quantiles(\n  car_data,\n  probs = c(0.01, 0.5, 0.95, seq(0.99, 0.999, by = 0.001))\n) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nprobs\nquantile\n\n\n\n\n1.0%\n0\n\n\n50.0%\n0\n\n\n95.0%\n0\n\n\n99.0%\n8300\n\n\n99.1%\n10000\n\n\n99.2%\n12434\n\n\n99.3%\n15000\n\n\n99.4%\n17850\n\n\n99.5%\n20000\n\n\n99.6%\n22948\n\n\n99.7%\n26593\n\n\n99.8%\n30000\n\n\n99.9%\n40000\n\n\n\n\n\nAt least 95% of households in the Interview survey did not report expenditures on cars in 2021, which explains why the mean is so low.\n\n\nVery advanced workflow: Inflation adjusted food away from home expenditures by household size\nIn this last example I’m going to assume very little knowledge about the CE. I’d like to compare mean annual expenditures on food away from home between 2010 and 2020 by household size and I want to convert expenditures to 2023 dollars using the CPI. First I’d go to the CE PUMD Data Files page and download the files that I need for both years. I’ll also go to the CE PUMD Documentation page to download the hierarchical grouping files to get the UCCs for “Food away from home” and the CE Dictionary to find out what variable has data on the household size.\nWith all that done, now I want to look at the hierarchical grouping files for 2010 and 2020 for integrated data as they relate to “Food away from home”.\n\n\nCode\ninteg10_hg &lt;- ce_hg(\n  2010,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\ninteg20_hg &lt;- ce_hg(\n  2020,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\n\nFirst I’ll explore the titles of the hierarchical grouping files to see if any of them mention “food away”\n\n\nCode\ninteg10_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"food away\")) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nFood away from home\nFOODAWAY\nG\n1\n\n\n\n\n\nNow I’ll do the same for 2020.\n\n\nCode\ninteg20_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"food away\")) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nFood away from home\nFOODAW\nG\n1\n\n\n\n\n\nHere I’ll take note of the title, which is the same in both years (“Food away from home”). I’ll use that to get the UCCs for both years.\n\n\nCode\nfood_away_uccs_10 &lt;- integ10_hg |&gt;\n  ce_uccs(expenditure = \"Food away from home\")\n\nfood_away_uccs_20 &lt;- integ20_hg |&gt;\n  ce_uccs(expenditure = \"Food away from home\")\n\n\nHere’s a quick look at the UCCs from 2010.\n\n\nCode\nfood_away_uccs_10\n\n\n [1] \"190111\" \"190112\" \"190113\" \"190114\" \"190211\" \"190212\" \"190213\" \"190214\"\n [9] \"190311\" \"190312\" \"190313\" \"190314\" \"190321\" \"190322\" \"190323\" \"190324\"\n[17] \"190901\" \"190902\" \"190903\" \"790430\" \"800700\"\n\n\nNow the 2020 UCCs.\n\n\nCode\nfood_away_uccs_10\n\n\n [1] \"190111\" \"190112\" \"190113\" \"190114\" \"190211\" \"190212\" \"190213\" \"190214\"\n [9] \"190311\" \"190312\" \"190313\" \"190314\" \"190321\" \"190322\" \"190323\" \"190324\"\n[17] \"190901\" \"190902\" \"190903\" \"790430\" \"800700\"\n\n\nThe vectors of UCCs look identical, but I’ll keep both just to be cautious.\nNext I’ll turn to finding the variable for household size in the CE data dictionary. It’s important to remember that the dictionary is stored as an “XLSX” file. I’ll use functions from the readxl package to work with the dictionary.\n\n\nCode\nexcel_sheets(file.path(ce_data_dir, \"ce-data-dict.xlsx\"))\n\n\n[1] \"Cover\"     \"Variables\" \"Codes \"   \n\n\nNow I’ll see what variables contain anything about the number of household members. To do that I’ll have to load the sheet from the dictionary containing the variable definitions. I also want to filter the variable data to only the FMLI where the “Last year” column is missing, i.e., the variable definition is still in use.\n\n\nCode\nce_variables &lt;- read_excel(\n  file.path(ce_data_dir, \"ce-data-dict.xlsx\"),\n  sheet = \"Variables\"\n)\n\nce_variables |&gt;\n  filter(\n    str_detect(File, \"FMLI\"),\n    str_detect(\n      tolower(`Variable description`), \"number of members\"\n    )\n  ) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey\nFile\nVariable Name\nVariable description\nFormula\nFlag name\nSection number\nSection description\nSection part\nFirst year\nFirst Quarter\nLast quarter\nLast year\nComment\n\n\n\n\nINTERVIEW\nFMLI\nAS_COMP5\nNumber of members under age 2 in CU\nCOUNT (AGE &lt; 2)\nAS_C_MP5\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1984\n1\nNA\nNA\nNA\n\n\nINTERVIEW\nFMLI\nAS_COMP5\nNumber of members under age 2 in CU\nNA\nAS_C_MP5\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1980\n1\n4\n1981\nNA\n\n\nINTERVIEW\nFMLI\nFAM_SIZE\nNumber of Members in CU\nNA\nFAM__IZE\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1984\n1\nNA\nNA\nNA\n\n\nINTERVIEW\nFMLI\nFAM_SIZE\nNumber of Members in CU\nNA\nFAM__IZE\nNA\nCU characteristics, income, weights, and summary level expenditures.\nNA\n1980\n1\n4\n1981\nNA\n\n\n\n\n\nIt looks like FAM_SIZE is the variable I want. I can see that this variable was used from 1980 through 1981 then was dropped and re-introduced in 1984 and has been in use since. So it looks like it’s available for my 2 years of interest. Next I’ll check whether the FAM_SIZE variable has any value codes associated with it. I’ll have to pull in the “Codes” sheet. (Check your spelling here.)\n\n\nCode\nce_codes &lt;- read_excel(\n  file.path(ce_data_dir, \"ce-data-dict.xlsx\"),\n  sheet = \"Codes \"\n)\n\nce_codes |&gt;\n  filter(File %in% \"FMLI\", Variable %in% \"FAM_SIZE\") |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurvey\nFile\nVariable\nCode value\nCode description\nFirst year\nFirst quarter\nLast year\nLast quarter\nComment\n…11\n\n\n\n\n\n\n\nIt looks like FAM_SIZE is not a coded variable (no observations in the “Codes” sheet), so it must be numeric. With all that, I’m ready to prepare my data. I’ll start by preparing the 2010 data and getting a summary of the FAM_SIZE variable since it is a continuous variable.\n\n\nCode\nfood_away_data_10 &lt;- ce_prepdata(\n  2010,\n  integrated,\n  integ10_hg,\n  food_away_uccs_10,\n  dia_zp = file.path(ce_data_dir, \"diary10.zip\"),\n  int_zp = file.path(ce_data_dir, \"intrvw10.zip\"),\n  fam_size\n)\n\nsummary(food_away_data_10$fam_size)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   2.000   2.666   4.000  14.000 \n\n\nSince some households have as many as 14 people, I’ll create a FAM_SIZE label with any number greater than 4 taking on the value “5+”. Next, I’ll prepare the 2020 data and row bind it with the 2010 data as well as create the “fam_size_label” variable. I’m also going to convert “ref_mo” and “ref_yr” to character to make it compatible with the CPI data that I’ll get later. I’ll also take a look at just a snippet of the data.\n\n\nCode\nfood_away_data_20 &lt;- ce_prepdata(\n  2020,\n  integrated,\n  integ10_hg,\n  food_away_uccs_20,\n  dia_zp = file.path(ce_data_dir, \"diary20.zip\"),\n  int_zp = c(\n    file.path(ce_data_dir, \"intrvw19.zip\"),\n    file.path(ce_data_dir, \"intrvw20.zip\")\n  ),\n  fam_size\n)\n\nfood_away_comp_data &lt;- food_away_data_10 |&gt;\n  mutate(year = \"2010\") |&gt;\n  bind_rows(food_away_data_20 |&gt; mutate(year = \"2020\")) |&gt;\n  mutate(\n    fam_size_label = if_else(fam_size &gt; 4, \"5+\", as.character(fam_size)),\n    ref_yr = as.character(ref_yr)\n  )\n\nfood_away_comp_data |&gt;\n  select(survey, year, newid, finlwt21, cost, ucc, ref_yr, ref_mo) |&gt;\n  filter(!is.na(ucc)) |&gt;\n  group_by(year, survey) |&gt;\n  slice_sample(n = 3) |&gt;\n  ungroup() |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nsurvey\nyear\nnewid\nfinlwt21\ncost\nucc\nref_yr\nref_mo\n\n\n\n\nD\n2010\n01047471\n43881.11\n2.30971\n190321\n2010\n1\n\n\nD\n2010\n01077871\n24204.65\n66.30000\n190111\n2010\n5\n\n\nD\n2010\n01116361\n62846.94\n18.33000\n190313\n2010\n8\n\n\nI\n2010\n02265533\n12788.96\n144.00000\n790430\n2010\n10\n\n\nI\n2010\n02309802\n22657.97\n33.33333\n790430\n2010\n7\n\n\nI\n2010\n02247652\n15588.84\n52.00000\n790430\n2010\n3\n\n\nD\n2020\n04630171\n33468.71\n523.51000\n190211\n2020\n11\n\n\nD\n2020\n04591042\n60503.51\n137.67000\n190311\n2020\n8\n\n\nD\n2020\n04436801\n49119.67\n156.00000\n190112\n2020\n2\n\n\nI\n2020\n04316443\n16358.45\n52.00000\n800700\n2020\n6\n\n\nI\n2020\n04227663\n27787.28\n100.00000\n190901\n2020\n1\n\n\nI\n2020\n04234823\n28645.27\n92.00000\n790430\n2020\n1\n\n\n\n\n\nI’ll now get CPI data for the years in the analysis and for 2023 to set as a base using the blsR package (https://github.com/groditi/blsR). I’m going to use the “All Urban Consumers (Current Series)” series, which has series ID “CUUR0000SA0”.\n\n\nCode\ncpi_data &lt;- blsR::get_series(\n  \"CUUR0000SA0\",\n  start_year = 2010,\n  end_year = 2023\n) |&gt;\n  pluck(\"data\") |&gt;\n  map(\n    \\(x) list_flatten(x) |&gt;\n      enframe() |&gt;\n      filter(!name %in% \"footnotes\") |&gt;\n      unnest(value) |&gt;\n      pivot_wider(values_from = value, names_from = name)\n  ) |&gt;\n  list_rbind() |&gt;\n  rename(cpi = \"value\") |&gt;\n  mutate(month = match(periodName, month.name))\n\ncpi_base &lt;- cpi_data |&gt; filter(year %in% \"2023\", month %in% \"12\")\n\ncpi_data &lt;- cpi_data |&gt; filter(year %in% unique(food_away_comp_data$ref_yr))\n\ncpi_data |&gt; slice(1:10) |&gt; kable(booktabs = TRUE)\n\n\n\n\n\nyear\nperiod\nperiodName\ncpi\nmonth\n\n\n\n\n2021\nM12\nDecember\n278.802\n12\n\n\n2021\nM11\nNovember\n277.948\n11\n\n\n2021\nM10\nOctober\n276.589\n10\n\n\n2021\nM09\nSeptember\n274.310\n9\n\n\n2021\nM08\nAugust\n273.567\n8\n\n\n2021\nM07\nJuly\n273.003\n7\n\n\n2021\nM06\nJune\n271.696\n6\n\n\n2021\nM05\nMay\n269.195\n5\n\n\n2021\nM04\nApril\n267.054\n4\n\n\n2021\nM03\nMarch\n264.877\n3\n\n\n\n\n\nThe base that I’m going to covert to is December 2023.\n\n\nCode\ncpi_base |&gt; kable(booktabs = TRUE)\n\n\n\n\n\nyear\nperiod\nperiodName\ncpi\nmonth\n\n\n\n\n2023\nM12\nDecember\n306.746\n12\n\n\n\n\n\nNext I’m going to join the CPI data to the CE data and adjust the “cost” variable for inflation. Note that I replace resulting missing values in the “cost” variable with “0”. Missing values will result when I multiply a cost of “0” by an adjustment factor and ce_mean() will not function with missing values.\n\n\nCode\nfood_away_comp_data &lt;- food_away_comp_data |&gt;\n  left_join(\n    select(cpi_data, year, month, cpi),\n    by = c(\"ref_yr\" = \"year\", \"ref_mo\" = \"month\")\n  ) |&gt;\n  mutate(\n    base_cpi = pull(cpi_base, cpi),\n    across(c(base_cpi, cpi), as.numeric),\n    cost = cost * (base_cpi / cpi) |&gt; replace_na(0)\n  )\n\nfood_away_comp_data |&gt;\n  select(survey, year, newid, finlwt21, cost, ucc, ref_yr, ref_mo) |&gt;\n  filter(!is.na(ucc)) |&gt;\n  group_by(year, survey) |&gt;\n  slice_sample(n = 3) |&gt;\n  ungroup() |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\nsurvey\nyear\nnewid\nfinlwt21\ncost\nucc\nref_yr\nref_mo\n\n\n\n\nD\n2010\n01062182\n31911.373\n632.33499\n190111\n2010\n4\n\n\nD\n2010\n01132291\n42762.166\n75.39493\n190312\n2010\n9\n\n\nD\n2010\n01151261\n34508.670\n71.78432\n190321\n2010\n11\n\n\nI\n2010\n02222433\n3456.019\n33.02286\n790430\n2010\n2\n\n\nI\n2010\n02202583\n14948.874\n233.51876\n190903\n2010\n2\n\n\nI\n2010\n02183355\n15016.399\n703.51037\n190903\n2010\n7\n\n\nD\n2020\n04433811\n74006.452\n63.35846\n190311\n2020\n2\n\n\nD\n2020\n04438592\n32749.351\n64.59171\n190211\n2020\n2\n\n\nD\n2020\n04636492\n41910.471\n306.47607\n190211\n2020\n11\n\n\nI\n2020\n04291913\n70403.081\n356.52248\n190903\n2020\n3\n\n\nI\n2020\n04386872\n20595.360\n1181.86902\n800700\n2020\n2\n\n\nI\n2020\n04683051\n33054.288\n280.67215\n800700\n2020\n12\n\n\n\n\n\nThe next step is to calculate means, for which I’ll use some more Tidyverse functions.\n\n\nCode\nfood_away_means &lt;- food_away_comp_data |&gt;\n  group_nest(year, fam_size_label, .key = \"data\") |&gt;\n  mutate(ce_mn_df = map(data, ce_mean)) |&gt;\n  select(-data) |&gt; \n  unnest(ce_mn_df) |&gt;\n  mutate(lower = mean_exp - cv, upper = mean_exp + cv)\n\nfood_away_means |&gt; kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nfam_size_label\nagg_exp\nmean_exp\nse\ncv\nlower\nupper\n\n\n\n\n2010\n1\n77335813000\n2207.926\n87.23315\n3.950909\n2203.975\n2211.877\n\n\n2010\n2\n137538943234\n3479.533\n102.54849\n2.947192\n3476.585\n3482.480\n\n\n2010\n3\n70833802054\n4028.171\n187.60152\n4.657238\n4023.514\n4032.829\n\n\n2010\n4\n78621552689\n4994.423\n196.31421\n3.930669\n4990.492\n4998.354\n\n\n2010\n5+\n60764779535\n4668.419\n252.13159\n5.400792\n4663.018\n4673.820\n\n\n2020\n1\n67173826416\n1713.043\n84.53129\n4.934569\n1708.108\n1717.977\n\n\n2020\n2\n122787510310\n2825.619\n131.55436\n4.655771\n2820.963\n2830.275\n\n\n2020\n3\n61184160903\n3170.855\n203.25085\n6.409971\n3164.445\n3177.265\n\n\n2020\n4\n68948170493\n4210.973\n262.49920\n6.233695\n4204.739\n4217.206\n\n\n2020\n5+\n48767026459\n3737.376\n327.18302\n8.754351\n3728.622\n3746.130\n\n\n\n\n\nPlotting these data would be pretty straightforward, as well.\n\n\nCode\nfood_away_means |&gt;\n  ggplot(aes(x = fam_size_label, y = mean_exp, fill = year, group = year)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", width = 0.8) +\n  geom_errorbar(\n    aes(ymin = lower, ymax = upper),\n    width = 0.4,\n    position = position_dodge(0.75)\n  ) +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    title =\n      \"Estimated annual mean food away from home expenditures by CU size\",\n    x = \"CU size\",\n    y = \"Estimated, weighted, annual mean expenditure\",\n    fill = \"Year\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nHere we can see that on an inflation-adjusted basis, households of all sizes had higher expenditures on food away from home in 2010 than they did in 2020.\nNow I’ll generate a plot of the expenditures at each weighted, annual, estimated quantile (from 0.01 through 0.99, by 0.01) for the same years, but only using Diary data, since most of the UCCs (16 out of 21) in the “food away from home” category come from the Diary.\n\n\nCode\nfood_away_comp_quantiles &lt;- map2(\n  c(2010, 2020),\n  c(\n    file.path(ce_data_dir, \"diary10.zip\"),\n    file.path(ce_data_dir, \"diary20.zip\")\n  ),\n  \\(x, y) {\n    dia_hg &lt;- ce_hg(\n      x,\n      diary,\n      hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n    )\n    \n    food_uccs &lt;- ce_uccs(dia_hg, expenditure = \"Food away from home\")\n    \n    ce_prepdata(\n      x,\n      diary,\n      dia_hg,\n      food_uccs,\n      dia_zp = y\n    ) |&gt;\n      mutate(year = x, ref_yr = as.character(ref_yr))\n  }\n) |&gt;\n  list_rbind() |&gt;\n  left_join(\n    select(cpi_data, year, month, cpi),\n    by = c(\"ref_yr\" = \"year\", \"ref_mo\" = \"month\")\n  ) |&gt;\n  mutate(year = factor(year)) |&gt;\n  nest(data = -year) |&gt;\n  mutate(\n    fa_qtile = map(data, ce_quantiles, probs = c(seq(0, 0.95, by = 0.05), 0.99))\n  ) |&gt;\n  select(-data) |&gt;\n  unnest(fa_qtile) |&gt;\n  mutate(probs = parse_number(probs) / 100)\n\nfood_away_comp_quantiles |&gt;\n  ggplot(aes(x = probs, y = quantile, group = year, color = year)) +\n  geom_line() +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_x_continuous(labels = scales::percent) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    title =\n      \"Estimated, annual food away from home expenditure quantiles\",\n    x = \"Quantile\",\n    y = \"Estimated, weighted, annual expenditure\",\n    color = \"Year\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nInterestingly the expenditures don’t appear to have changed much between 2010 and 2020 across quantiles on an inflation-adjusted basis, but we can see that across all quantiles, CU’s spent less in 2020 than they did in 2010 on food away from home, which is consistent with the means that we calculated above. There are a lot of 0-value reported expenditures, though, in the CE on food away from home. Unfortunately, I can’t perform an analysis using only respondents that did have expenditures in this category, i.e., dropping the 0’s, because whether someone had an expenditure on food away from home is not one of the variables used for generating the survey weights. In other words, the analysis can be done, but it would not be statistically valid and I definitely wouldn’t be able to infer from it. This is just another cautionary note to anyone using this package who might use it in a way that does not follow statistically sound practices. Please visit the CE’s website and read the CE PUMD Getting Started Guide for more information.\n\n\nDealing with inconsistent code definitions\nIn this workflow I’m going to calculate estimated mean utilities expenditures for 2015 using integrated data by CU composition using the FAM_TYPE variable. In this case I’m going to start by looking at the codes for that variable to show how one might run into an inconsistency in code definitions across survey instruments. First I’m going to set up a sub-directory in my temporary directory and store what I’ll need to get started.\nFirst, I’ll look at code descriptions for the “FAM_TYPE” variable in the dictionary and I’m going to focus on the code values of 3, 5, and 7. I still have the ce_codes object in memory, so I’ll just use that.\n\n\nCode\nce_codes |&gt;\n  janitor::clean_names() |&gt;\n  filter(\n    variable %in% \"FAM_TYPE\",\n  first_year &lt;= 2015,\n  (last_year &gt;= 2015 | is.na(last_year)),\n  code_value %in% c(\"3\", \"5\", \"7\")\n  ) |&gt;\n  select(survey, code_value, code_description) |&gt;\n  arrange(code_value, survey) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\nsurvey\ncode_value\ncode_description\n\n\n\n\nDIARY\n3\nMarried couple, own children only, oldest child &gt; 6, &lt; 18\n\n\nINTERVIEW\n3\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n\n\nDIARY\n5\nAll other Married couple families\n\n\nINTERVIEW\n5\nAll other Husband and wife families\n\n\nDIARY\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\nINTERVIEW\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\n\n\n\nThe code descriptions for these 3 code values are different across instruments. To resolve this I’m going to create a table containing only codes from the Interview survey.\n\n\nCode\nfam_type_codes &lt;- ce_codes |&gt;\n  janitor::clean_names() |&gt;\n  filter(\n    variable %in% \"FAM_TYPE\",\n    first_year &lt;= 2015,\n    (last_year &gt;= 2015 | is.na(last_year))\n  )\n\ncodes2keep &lt;- fam_type_codes |&gt;\n  filter(survey %in% \"INTERVIEW\") |&gt;\n  select(code_value, code_description)\n\nfam_type_codes &lt;- fam_type_codes |&gt;\n  select(-code_description) |&gt;\n  left_join(codes2keep, by = \"code_value\") |&gt;\n  relocate(code_description, .after = code_value)\n\nfam_type_codes |&gt;\n  filter(code_value %in% c(\"3\", \"5\", \"7\")) |&gt;\n  select(survey, code_value, code_description) |&gt;\n  arrange(code_value, survey) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\nsurvey\ncode_value\ncode_description\n\n\n\n\nDIARY\n3\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n\n\nINTERVIEW\n3\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n\n\nDIARY\n5\nAll other Husband and wife families\n\n\nINTERVIEW\n5\nAll other Husband and wife families\n\n\nDIARY\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\nINTERVIEW\n7\nOne parent, female, own children, at least one age &lt; 18\n\n\n\n\n\nNow the codes are consistent across survey instruments and I can use this code-book in my call to ce_prepdata() using the “own_code-book” argument. Then I’ll pass that to ce_mean() per usual.\nNext I’ll get some information about how utilities expenditures are organized using the stub file.\n\n\nCode\ninteg15_hg &lt;- ce_hg(\n  2015,\n  integrated,\n  hg_zip_path = file.path(ce_data_dir, \"stubs.zip\")\n)\n\ninteg15_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"utilities\")) |&gt;\n  kable(bookmarks = TRUE)\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nUtilities, fuels, and public services\nUTILS\nG\n1\n\n\n\n\n\nThe expenditure category associated with utilities is “Utilities, fuels, and public services”. I’ll store that title to work with later and narrow down the section of the stub file that includes only these expenditures.\n\n\nCode\nutilities_title &lt;- integ15_hg |&gt;\n  filter(str_detect(str_to_lower(title), \"utilities\")) |&gt;\n  pull(title)\n\nutilities_hg &lt;- ce_uccs(\n  integ15_hg,\n  expenditure = utilities_title,\n  uccs_only = FALSE\n)\n\nutilities_hg |&gt; kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nlevel\ntitle\nucc\nsurvey\nfactor\n\n\n\n\n3\nUtilities, fuels, and public services\nUTILS\nG\n1\n\n\n4\nNatural gas\nNATRLG\nG\n1\n\n\n5\nUtility-natural gas (renter)\n260211\nI\n1\n\n\n5\nUtility-natural gas (owned home)\n260212\nI\n1\n\n\n5\nUtility-natural gas (owned vacation)\n260213\nI\n1\n\n\n5\nUtility-natural gas (rented vacation)\n260214\nI\n1\n\n\n4\nElectricity\nELECTR\nG\n1\n\n\n5\nElectricity (renter)\n260111\nI\n1\n\n\n5\nElectricity (owned home)\n260112\nI\n1\n\n\n5\nElectricity (owned vacation)\n260113\nI\n1\n\n\n5\nElectricity (rented vacation)\n260114\nI\n1\n\n\n4\nFuel oil and other fuels\nOTHRFU\nG\n1\n\n\n5\nFuel oil\nFUELOI\nG\n1\n\n\n6\nFuel oil (renter)\n250111\nI\n1\n\n\n6\nFuel oil (owned home)\n250112\nI\n1\n\n\n6\nFuel oil (owned vacation)\n250113\nI\n1\n\n\n6\nFuel oil (rented vacation)\n250114\nI\n1\n\n\n5\nCoal, wood, and other fuels\nCLWDOT\nG\n1\n\n\n6\nCoal, wood, other fuels (renter)\n250911\nI\n1\n\n\n6\nCoal, wood, other fuels (owned home)\n250912\nI\n1\n\n\n6\nCoal, wood, other fuels (owned vacation)\n250913\nI\n1\n\n\n6\nCoal, wood, other fuels (rented vacation)\n250914\nI\n1\n\n\n5\nBottled gas\nBOTTLG\nG\n1\n\n\n6\nGas, btld/tank (renter)\n250211\nI\n1\n\n\n6\nGas, btld/tank (owned home)\n250212\nI\n1\n\n\n6\nGas, btld/tank (owned vacation)\n250213\nI\n1\n\n\n6\nGas, btld/tank (rented vacation)\n250214\nI\n1\n\n\n4\nTelephone services\nPHONE\nG\n1\n\n\n5\nResidential phone service, VOIP, and phone cards\nRESPHO\nG\n1\n\n\n6\nPhone cards\n270104\nI\n1\n\n\n6\nResidential telephone including VOIP\n270106\nI\n1\n\n\n5\nCellular phone service\n270102\nI\n1\n\n\n4\nWater and other public services\nWATER\nG\n1\n\n\n5\nWater and sewerage maintenance\nSEWER\nG\n1\n\n\n6\nWater/sewer maint. (renter)\n270211\nI\n1\n\n\n6\nWater/sewer maint. (owned home)\n270212\nI\n1\n\n\n6\nWater/sewer maint. (owned vacation)\n270213\nI\n1\n\n\n6\nWater/sewer maint. (rented vacation)\n270214\nI\n1\n\n\n5\nTrash and garbage collection\nTRASH\nG\n1\n\n\n6\nTrash/garb. coll. (renter)\n270411\nI\n1\n\n\n6\nTrash/garb. coll. (owned home)\n270412\nI\n1\n\n\n6\nTrash/garb. coll. (owned vacation)\n270413\nI\n1\n\n\n6\nTrash/garb. coll. (rented vacation)\n270414\nI\n1\n\n\n5\nSeptic tank cleaning\nSEPTAN\nG\n1\n\n\n6\nSeptic tank clean. (renter)\n270901\nI\n1\n\n\n6\nSeptic tank clean. (owned home)\n270902\nI\n1\n\n\n6\nSeptic tank clean. (owned vacation)\n270903\nI\n1\n\n\n6\nSeptic tank clean. (rented vacation)\n270904\nI\n1\n\n\n\n\n\nI also want to know what survey instruments the expenditures are collected through for published estimates. My stub file is the integrated stub file, so I should see both “I” and “D” in the survey column of the stub file if expenditures are collected through both instruments.\n\n\nCode\nutilities_hg |&gt; distinct(survey) |&gt; kable(booktabs = TRUE)\n\n\n\n\n\nsurvey\n\n\n\n\nG\n\n\nI\n\n\n\n\n\nIt seems utilities expenditures are collected only through the Interview survey, so I’ll only need to use Interview data files to calculate estimates.\n\n\nCode\nfam_type_utilities &lt;- ce_prepdata(\n  2015,\n  interview,\n  utilities_hg,\n  uccs = ce_uccs(utilities_hg, expenditure = utilities_title),\n  fam_type, \n  int_zp = file.path(ce_data_dir, \"intrvw15.zip\"),\n  recode_variables = TRUE,\n  dict_path = file.path(ce_data_dir, \"ce-data-dict.xlsx\")\n) |&gt;\n  group_nest(fam_type) |&gt;\n  mutate(ce_mean_df = map(data, ce_mean)) |&gt;\n  select(-data) |&gt;\n  unnest(ce_mean_df)\n\nfam_type_utilities |&gt;\n  arrange(fam_type) |&gt;\n  kable(booktabs = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\nfam_type\nagg_exp\nmean_exp\nse\ncv\n\n\n\n\nMarried Couple only\n122623482952\n4378.377\n76.25699\n1.741673\n\n\nMarried Couple, own children only, oldest child &lt; 6\n21592090354\n4073.529\n241.48233\n5.928087\n\n\nMarried Couple, own children only oldest child &gt;= 6, &lt;= 17\n73899626502\n5136.781\n132.53306\n2.580080\n\n\nMarried Couple, own children only, oldest child &gt; 17\n53687574363\n5585.027\n190.15164\n3.404668\n\n\nAll other Husband and wife families\n26767356778\n5699.177\n331.83382\n5.822487\n\n\nOne parent, male, own children at least one age &lt; 18\n4647315390\n3887.863\n532.21229\n13.689069\n\n\nOne parent, female, own children, at least one age &lt; 18\n22862016917\n3684.480\n237.12762\n6.435851\n\n\nSingle consumers\n88002147354\n2348.182\n40.59815\n1.728919\n\n\nOther families\n84986387660\n3942.345\n122.97865\n3.119429\n\n\n\n\n\nAnd finally, a quick lollipop plot.\n\n\nCode\nfam_type_utilities |&gt;\n  mutate(fam_type = fct_reorder(fam_type, mean_exp)) |&gt;\n  ggplot(aes(x = mean_exp, y = fam_type, mean_exp)) +\n  geom_segment(aes(x = 0, xend = mean_exp, yend = fam_type)) +\n  geom_point(color = \"red\", size = 5) +\n  scale_y_discrete(labels = function(x) str_wrap(x, width = 25)) +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    y = \"CU composition (FAM_TYPE)\",\n    x = \"Estimated, weighted, annual mean expenditure\",\n    title =\n      \"Estimated annual mean utilities expenditures by CU composition\"\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "posts/20240124-cepumd-intro/cepumd-intro.html#conclusion",
    "href": "posts/20240124-cepumd-intro/cepumd-intro.html#conclusion",
    "title": "Introduction to cepumd",
    "section": "Conclusion",
    "text": "Conclusion\nThat wraps up this introduction to cepumd. Thank you for taking a look. If you find any bugs, please report them on the Github repo issues section."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "",
    "text": "Image by [Images Money](https://www.flickr.com/photos/59937401@N07/)\n\n\nI learned about the modeltime (Dancho 2023) package – developed by the folks at Business Science – in 2021 and hadn’t had much chance to use it except for a few small coding exercises. With U.S. housing prices have being all over the news in recent times I thought about trying to forecast a housing price metric. My curiosity about housing price trends presented the perfect opportunity to take on a small project using modeltime. My goal for this project will be to model multiple time series together using various algorithms through a global modeling process then select the best models from that process and use those algorithms to run an iterative modeling process on the time series to try to get the best forecasts.\nI’ve broken this project into five parts:\n\nGathering Data\nTime series analysis\nARIMA with global and iterative modeling processes\nGlobal forecasting models\nIterative forecasting models\n\nThis post will cover the first part: Gathering Data."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#introduction",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#introduction",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "",
    "text": "Image by [Images Money](https://www.flickr.com/photos/59937401@N07/)\n\n\nI learned about the modeltime (Dancho 2023) package – developed by the folks at Business Science – in 2021 and hadn’t had much chance to use it except for a few small coding exercises. With U.S. housing prices have being all over the news in recent times I thought about trying to forecast a housing price metric. My curiosity about housing price trends presented the perfect opportunity to take on a small project using modeltime. My goal for this project will be to model multiple time series together using various algorithms through a global modeling process then select the best models from that process and use those algorithms to run an iterative modeling process on the time series to try to get the best forecasts.\nI’ve broken this project into five parts:\n\nGathering Data\nTime series analysis\nARIMA with global and iterative modeling processes\nGlobal forecasting models\nIterative forecasting models\n\nThis post will cover the first part: Gathering Data."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#choosing-house-price-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#choosing-house-price-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Choosing house price data",
    "text": "Choosing house price data\nTo perform this analysis, obviously the first thing I would need is a data set. I wanted data that had sufficient observations with enough granularity to be able to identify seasonality and level changes. I also wanted to compare time series across different regions or cities of the U.S. This meant getting data that was available at a regional/metropolitan level that went back at least 10 years with very few missing values (ideally none). I looked at the data available through the Federal Reserve Bank of St. Louis which hosts one of the largest repositories of U.S. economic data available by using the fredr (Boysel and Vaughan 2021) package and narrowed my search for data series down to either median home prices or the Case-Shiller Home Price Index. I chose the Case-Shiller HPI because I could get the data at the level of metro area rather than just U.S. region for a longer period of time, though there is the trade-off that this index only accounts for existing home sales. For my purposes this wasn’t a problem.\nThe Case-Shiller HPI data was available through 2023, but population data (discussed below) was available only through 2022 and there were some other issues with data from before 2006, so I got data starting in January 2006 and ending in December 2022.\nOnce I chose a data source, the next step was to choose cities. I wanted to compare cities from different regions of the U.S. that were different in other characteristics as well, e.g., population, climate, etc., to see how forecasts might differ. I chose San Diego, Dallas, Detroit, and New York City."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#additional-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#additional-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Additional data",
    "text": "Additional data\nOne of the benefits of using modeltime it has functions for running models beyond ARIMA such as Prophet and eXtreme Gradient Boosting (XGB) for time series. To take advantage of this I wanted to add some features to my data. I chose to add unemployment rates, average gas prices, variables relating to demographic composition, and population for each city at the monthly level. I got the unemployment rate and average gas prices through fredr, population data from the U.S. Census’s American Community Survey (ACS) using the tidycensus (Walker and Herman 2024) package, and other demographic data from the Current Population Survey (CPS) (which is a joint program of the U.S. Census and the U.S. Bureau of Labor Statistics) using the cpsR (Saenz 2023) package.\n\n\n\n\n\n\nImportant\n\n\n\nBoth tidycensus and cpsR require a U.S. Census API key. This post will not go into detail on how to obtain a key or how to set it up for use with the relevant packages.\n\n\n\n\n\n\n\n\nStatistical Area Definitions\n\n\n\n\n\nSome of these data are at the metropolitan statistical area (MSA) level and others are at the core-based statistical area (CBSA) level. While these broadly overlap, which is why I didn’t make a big fuss about mixing them, they are technically different. The U.S. Census provides some definitions here.\n\n\n\nAs usual for me, I performed data importing and wrangling using various tidyverse (Wickham et al. 2019)package functions. I also used multidplyr (Wickham 2023) to perform some operations in parallel."
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#loading-packages",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#loading-packages",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Loading Packages",
    "text": "Loading Packages\nTo begin with, I first loaded my packages. I’ll include the gt package for displaying tables.\n\n\nCode\nlibrary(tidyverse)\nlibrary(fredr)\nlibrary(tidycensus)\nlibrary(cpsR)\nlibrary(gt)\nlibrary(multidplyr)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")\nconflicted::conflict_prefer(\"between\", \"dplyr\")"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-case-shiller-hpi-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-case-shiller-hpi-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting Case-Shiller HPI data",
    "text": "Getting Case-Shiller HPI data\nI started by getting the appropriate Case-Shiller HPI series IDs for my cities of interest.\n\n\nCode\nhpi_ids &lt;- fredr_series_search_text(\"Case-Shiller\") |&gt;\n  filter(\n    str_detect(\n      title,\n      \"CA-San Diego|TX-Dallas|NY-New York|MI-Detroit Home Price Index\"\n    ),\n    seasonal_adjustment_short %in% \"NSA\",\n    frequency %in% \"Monthly\"\n  ) |&gt;\n  mutate(city = str_extract(title, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  select(city, id)\n\ngt(hpi_ids) |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\nid\n\n\n\n\nDallas\nDAXRNSA\n\n\nSan Diego\nSDXRNSA\n\n\nNew York\nNYXRNSA\n\n\nDetroit\nDEXRNSA\n\n\n\n\n\n\n\nNext I read in the Case-Shiller data and kept only the necessary variables.\n\n\nCode\nhpi_data &lt;- hpi_ids |&gt;\n  mutate(\n    data = map(\n      id,\n      \\(x) fredr(\n        series_id = x,\n        observation_start = ymd(\"2006-01-01\"),\n        observation_end = ymd(\"2023-12-31\"),\n        frequency = \"m\"\n      )\n    )\n  ) |&gt;\n  select(-id) |&gt;\n  unnest(data) |&gt;\n  select(city, date, hpi = value)\n\nhpi_data[1:10, ] |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\n\n\n\n\nDallas\n2006-01-01\n121.9108\n\n\nDallas\n2006-02-01\n121.3285\n\n\nDallas\n2006-03-01\n121.5217\n\n\nDallas\n2006-04-01\n122.3996\n\n\nDallas\n2006-05-01\n123.2863\n\n\nDallas\n2006-06-01\n124.4985\n\n\nDallas\n2006-07-01\n125.3672\n\n\nDallas\n2006-08-01\n125.7007\n\n\nDallas\n2006-09-01\n125.1859\n\n\nDallas\n2006-10-01\n124.5813"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-unemployment-rate-and-gas-price-data-from-fred",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-unemployment-rate-and-gas-price-data-from-fred",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting unemployment rate and gas price data from FRED",
    "text": "Getting unemployment rate and gas price data from FRED\nNext I got series IDs for both average gas prices and unemployment rates for all four cities and got the data for the appropriate time span.\n\n\nCode\n# Get average gas price for each CBSA by month\ngas_ids &lt;- fredr_series_search_text(\n  \"Average Price: Gasoline, Unleaded Regular\"\n) |&gt;\n  filter(\n    str_detect(title, \"New York|Dallas|San Diego|Detroit\"),\n    frequency %in% \"Monthly\",\n    seasonal_adjustment_short %in% \"NSA\"\n  ) |&gt;\n  mutate(city = str_extract(title, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  select(city, id)\n\ngas_data &lt;- gas_ids |&gt;\n  mutate(\n    data = map(\n      id,\n      \\(x) fredr(\n        series_id = x,\n        observation_start = ymd(\"2000-01-01\"),\n        observation_end = ymd(\"2023-12-31\"),\n        frequency = \"m\"\n      )\n    )\n  ) |&gt;\n  select(-id) |&gt;\n  unnest(data) |&gt;\n  select(city, date, avg_gas_price = value)\n\n# Get average unemployment rate for each MSA by month\nunemp_ids &lt;- fredr_series_search_text(\"Unemployment: Rate\") |&gt;\n  filter(\n    str_detect(title, \"New York|Dallas|San Diego|Detroit\"),\n    frequency %in% \"Monthly\",\n    seasonal_adjustment_short %in% \"NSA\"\n  ) |&gt;\n  mutate(city = str_extract(title, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  group_by(city) |&gt;\n  mutate(\n    rnum = n(),\n    msa_str = str_detect(title, \"MSA\")\n  ) |&gt;\n  ungroup() |&gt;\n  filter(rnum == 1 | msa_str) |&gt;\n  select(city, id)\n\nunemp_data &lt;- unemp_ids |&gt;\n  mutate(\n    data = map(\n      id,\n      \\(x) fredr(\n        series_id = x,\n        observation_start = ymd(\"2000-01-01\"),\n        observation_end = ymd(\"2023-12-31\"),\n        frequency = \"m\"\n      )\n    )\n  ) |&gt;\n  select(-id) |&gt;\n  unnest(data) |&gt;\n  select(city, date, unemp_rate = value)"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-cps-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-cps-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting CPS data",
    "text": "Getting CPS data\nGetting CPS data at the MSA level is a little more complicated than it is getting data series from FRED. CPS data can contain well over 100K rows for each month because the microdata are released at the household level and each data set has hundreds of variables. It also requires filtering and grouping the data by MSA code, which means getting the MSA codes that correspond to each MSA in my analysis. However, I knew that MSAs can change over time as population shifts. I felt confident that this wouldn’t be the case for my chosen cities because of their population sizes, but I wanted to check anyway. Finally, I did thought that pulling in all of that data at once then filtering and aggregating would be computationally inefficient and slow, so, instead, I wrote a function to perform all of the necessary operations and ran it in parallel. I chose the method that I did because most of the time was spent connecting to the CPS API. Once data are imported, the computations are fairly simple.\nThe first step was to get the MSA codes that corresponded to my cities of interest and check that I had a unique set across all of my years of interest. The data for these MSA codes came from the U.S. Census’s technical documentation page and MSA definitions only change once every few years.\n\n\nCode\nmsa_code_tbl &lt;- tribble(\n  ~start_yr, ~end_yr,\n  2003, 2006,\n  2007, 2011,\n  2012, 2016,\n  2017, 2021\n) |&gt;\n  mutate(\n    suffix = str_sub(start_yr, 3, 4),\n    msa_url = map_chr(\n      suffix,\n      \\(x) str_c(\n        \"https://www2.census.gov/programs-surveys/cbp/technical-documentation/\",\n        \"reference/metro-area-geography-reference/msa_county_reference\",\n        x,\n        \".txt\"\n      )\n    ),\n    msa_code_data = map(\n      msa_url,\n      \\(x) read_delim(x, delim = \",\", show_col_types = FALSE) |&gt;\n        filter(\n          str_detect(\n            name_msa,\n            \"Dallas.*TX|San Diego.*CA|New York.*NY|Detroit.*MI\"\n          )\n        ) |&gt;\n        mutate(\n          city = str_extract(name_msa, \"Dallas|New York|San Diego|Detroit\"),\n          msa = as.character(msa)\n        ) |&gt;\n        distinct(city, msa)\n    )\n  ) |&gt;\n  distinct(msa_code_data) |&gt;\n  unnest(msa_code_data)\n\nmsa_code_tbl |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\nmsa\n\n\n\n\nDallas\n19100\n\n\nDetroit\n19820\n\n\nNew York\n35620\n\n\nSan Diego\n41740\n\n\n\n\n\n\n\nIn the next step I defined the function for importing, filtering, and aggregating CPS data.\n\n\nCode\nget_cps_summaries &lt;- function(cps_df, geographies) {\n  cps_df |&gt;\n    rename(hispanic = matches(\"hspnon\"), msa = starts_with(\"gt\")) |&gt;\n    filter(msa %in% geographies) |&gt;\n    relocate(msa, pwsswgt, prtage, pemaritl, peeduca, hispanic) |&gt;\n    mutate(\n      msa = as.character(msa),\n      pwsswgt = as.numeric(pwsswgt),\n      across(prtage:hispanic, as.integer),\n      across(prtage:hispanic, \\(x) na_if(x, -1)),\n      age_lt_18 = prtage &lt; 18,\n      age_18_35 = between(prtage, 18, 35),\n      age_36_65 = between(prtage, 36, 65),\n      age_gt_65 = prtage &gt; 65,\n      educ_hs_less = peeduca &lt; 40,\n      educ_college = peeduca %in% 40:43,\n      educ_grad = peeduca &gt; 43,\n      status_married = pemaritl %in% 1:2,\n      status_nev_mar = pemaritl == 6,\n      status_other = !pemaritl %in% c(1:2, 6),\n      hispanic = hispanic == 1,\n      across(\n        c(age_lt_18:status_other, hispanic),\n        \\(x) (x * pwsswgt)\n      )\n    ) |&gt;\n    group_by(msa) |&gt;\n    summarise(\n      across(\n        c(age_lt_18:status_other, hispanic),\n        \\(x) sum(x / sum(pwsswgt), na.rm = TRUE)\n      )\n    )\n}\n\n\nWith that function now defined, I move on to get CPS data using that function.\n\n\nCode\n# Set up a cluster to get all of the CPS data aggregations\ncps_clust &lt;- new_cluster(12)\ncluster_library(cps_clust, c(\"tidyverse\", \"cpsR\"))\ncluster_copy(cps_clust, c(\"get_cps_summaries\", \"msa_code_tbl\"))\n\n# Get CPS data aggregations\ncps_grid &lt;- expand.grid(\n  yr = 2006:2023,\n  mo = 1:12\n) |&gt;\n  partition(cps_clust)\n\ncps_data &lt;- cps_grid |&gt;\n  arrange(yr, mo) |&gt;\n  mutate(\n    cps = map2(\n      yr, mo,\n      possibly(\n        \\(y, m) {\n          get_basic(\n            y, m,\n            c(\"gtcbsa\", \"prtage\", \"pemaritl\", \"peeduca\", \"pehspnon\", \"pwsswgt\")\n          ) |&gt;\n            get_cps_summaries(msa_code_tbl$msa)\n        }\n      )\n    )\n  ) |&gt;\n  collect() |&gt;\n  unnest(cps) |&gt;\n  mutate(msa = as.character(msa)) |&gt;\n  left_join(msa_code_tbl, by = \"msa\") |&gt;\n  select(-msa) |&gt;\n  unite(\"date\", yr, mo, sep = \"-\") |&gt;\n  mutate(date = ym(date)) |&gt;\n  relocate(city, date)\n\n\nThis is what the CPS data looked like.\n\n\nCode\nhead(cps_data) |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nage_lt_18\nage_18_35\nage_36_65\nage_gt_65\neduc_hs_less\neduc_college\neduc_grad\nstatus_married\nstatus_nev_mar\nstatus_other\nhispanic\n\n\n\n\nDallas\n2006-01-01\n0.2891927\n0.2805833\n0.3565649\n0.07365909\n0.3582388\n0.3352050\n0.06503496\n0.4248964\n0.2140741\n0.3610295\n0.26435629\n\n\nDetroit\n2006-01-01\n0.2702241\n0.2240328\n0.3916348\n0.11410831\n0.3702413\n0.3384275\n0.06239968\n0.4012264\n0.2359013\n0.3628723\n0.03441875\n\n\nNew York\n2006-01-01\n0.2499559\n0.2424744\n0.3919559\n0.11561378\n0.3744975\n0.3294315\n0.09275988\n0.4073354\n0.2681326\n0.3245319\n0.21810530\n\n\nSan Diego\n2006-01-01\n0.2391931\n0.2444161\n0.4007565\n0.11563422\n0.2917841\n0.4379721\n0.07820804\n0.4023657\n0.2548609\n0.3427734\n0.29021086\n\n\nDallas\n2006-03-01\n0.2991726\n0.2852235\n0.3324601\n0.08314374\n0.3481381\n0.3412725\n0.06266023\n0.4069059\n0.2147862\n0.3783079\n0.27328065\n\n\nDetroit\n2006-03-01\n0.2660104\n0.2155320\n0.4070590\n0.11139867\n0.3738536\n0.3548189\n0.05050216\n0.4191846\n0.2244699\n0.3563455\n0.03402642"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-population-data",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#getting-population-data",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Getting population data",
    "text": "Getting population data\nOne would think that if any demographic data is simple to get, population data would be simple, right? I learned something different. I was interested in getting population data at a monthly level for each MSA of interest. I knew population was not counted monthly, but I thought I could get it from the CPS. In fact, one might notice that the code actually calls a population variable (pwsswgt ) to perform some calculations. The problem here was that at the MSA level population estimates varied in some cases by as much as 16%. While I I didn’t know if this was true or not I did believe (subjectively) that it was unlikely and I believed that CPS weights are not calibrated to MSAs. My understanding is that they are calibrated to the national and state levels. So, I thought a reasonable alternative would be to get data from the ACS 1-year survey and interpolate data from one year to the next. In fact, the reason that I started my time series in 2006 is that the ACS 1-year and 5-year survey data became available in 2005 and I needed one year before the time series for interpolation. However, there was wrinkle caused by COVID. The 1-year ACS was not released for 2020 because of understandable challenges with data collection. So, for 2020 I used ACS 5-year survey data. I started by getting all of the ACS 1-year data. Here again, I processed the data in parallel.\n\n\nCode\n# Create a cluster for getting ACS data\nacs_clust &lt;- new_cluster(12)\ncluster_library(acs_clust, c(\"tidyverse\", \"tidycensus\"))\n\n# Get ACS data for 2005 to 2019, 2021, and 2022\nacs1_pop_data &lt;- tibble(year = c(2005:2019, 2021:2022)) |&gt;\n  partition(acs_clust) |&gt;\n  mutate(\n    pop_data = map(\n      year,\n      \\(x) get_acs(\n        year = x,\n        variables = \"B01001_001\",\n        geography = \"cbsa\",\n        survey = \"acs1\",\n        show_call = FALSE\n      ) |&gt;\n        filter(str_detect(NAME, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n        mutate(city = str_extract(NAME, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n        select(city, population = estimate)\n    )\n  ) |&gt;\n  collect() |&gt;\n  unnest(pop_data) |&gt;\n  mutate(date = str_c(year, \"12\") |&gt; ym(), .keep = \"unused\")\n\n\nNext I got the data for 2020 and combined it with the ACS 1-year data. I performed interpolation using the ts_impute_vec() function from the timetk (Dancho and Vaughan 2023) package.\n\n\nCode\n# Get data for 2020 separately since it wasn't available in the ACS 1-year\nacs5_pop_data &lt;- get_acs(\n  year = 2020,\n  variables = \"B01001_001\",\n  geography = \"cbsa\",\n  survey = \"acs5\",\n  show_call = FALSE\n) |&gt;\n  filter(str_detect(NAME, \"New York|Dallas|San Diego|Detroit\")) |&gt;\n  mutate(\n    city = str_extract(NAME, \"New York|Dallas|San Diego|Detroit\"),\n    date = rep(ym(\"2020-12\"), 4),\n  ) |&gt;\n  select(city, date, population = estimate)\n\n# Put together all the ACS population data and interpolate between annual\n# estimates\nacs_pop_data &lt;- expand_grid(\n  yr = 2006:2022,\n  mo = 1:12\n) |&gt;\n  add_row(yr = 2005, mo = 12) |&gt;\n  expand_grid(city = c(\"New York\", \"Dallas\", \"San Diego\", \"Detroit\")) |&gt;\n  unite(date, yr, mo, sep = \"-\", remove = TRUE) |&gt;\n  mutate(date = ym(date)) |&gt;\n  arrange(date, city) |&gt;\n  left_join(\n    bind_rows(acs1_pop_data, acs5_pop_data),\n    by = c(\"city\", \"date\")\n  ) |&gt;\n  group_by(city) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    population = timetk::ts_impute_vec(population, period = 1),\n    pop_chg = (population / lag(population)) - 1\n  ) |&gt;\n  drop_na(pop_chg) |&gt;\n  ungroup() |&gt;\n  select(!population)\n\nhead(acs_pop_data) |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ndate\ncity\npop_chg\n\n\n\n\n2006-01-01\nDallas\n0.0040551186\n\n\n2006-01-01\nDetroit\n0.0007530958\n\n\n2006-01-01\nNew York\n0.0021226567\n\n\n2006-01-01\nSan Diego\n0.0034579867\n\n\n2006-02-01\nDallas\n0.0040387411\n\n\n2006-02-01\nDetroit\n0.0007525290"
  },
  {
    "objectID": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#merge-data-and-check",
    "href": "posts/20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.html#merge-data-and-check",
    "title": "Forecasting with {modeltime} - Part I",
    "section": "Merge data and check",
    "text": "Merge data and check\nWith all of the data gathered and each data set containing variables for city and date (monthly) along with at least one demographic or economic variable, they were ready to be merged.\n\n\nCode\necon_data &lt;- list(hpi_data, unemp_data, gas_data, cps_data, acs_pop_data) |&gt;\n  reduce(inner_join, by = c(\"city\", \"date\"))\n\n\nIt would be a mistake to ever think that there is no more cleaning that can be done, so I skimmed the data using the skimr() function from the skimr (Waring et al. 2022) package.\n\n\nCode\nskimr::skim(econ_data)\n\n\n\nData summary\n\n\nName\necon_data\n\n\nNumber of rows\n816\n\n\nNumber of columns\n17\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n1\n\n\nnumeric\n15\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncity\n0\n1\n6\n9\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate\n0\n1\n2006-01-01\n2022-12-01\n2014-06-16\n204\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nhpi\n0\n1.00\n172.19\n64.38\n64.47\n122.40\n168.06\n203.87\n427.80\n▆▇▃▁▁\n\n\nunemp_rate\n0\n1.00\n6.50\n2.93\n2.80\n4.20\n5.60\n8.30\n23.90\n▇▃▁▁▁\n\n\navg_gas_price\n144\n0.82\n2.95\n0.76\n1.36\n2.38\n2.84\n3.48\n6.29\n▅▇▅▁▁\n\n\nage_lt_18\n0\n1.00\n0.24\n0.02\n0.19\n0.22\n0.24\n0.25\n0.30\n▂▇▇▅▁\n\n\nage_18_35\n0\n1.00\n0.25\n0.02\n0.19\n0.24\n0.25\n0.26\n0.31\n▁▃▇▅▁\n\n\nage_36_65\n0\n1.00\n0.39\n0.02\n0.32\n0.38\n0.39\n0.40\n0.44\n▁▂▇▃▁\n\n\nage_gt_65\n0\n1.00\n0.12\n0.03\n0.07\n0.11\n0.12\n0.14\n0.24\n▃▇▅▁▁\n\n\neduc_hs_less\n0\n1.00\n0.33\n0.03\n0.24\n0.31\n0.33\n0.35\n0.39\n▁▃▇▇▃\n\n\neduc_college\n0\n1.00\n0.38\n0.03\n0.31\n0.36\n0.38\n0.40\n0.48\n▂▇▆▃▁\n\n\neduc_grad\n0\n1.00\n0.09\n0.02\n0.05\n0.08\n0.09\n0.11\n0.16\n▃▇▇▅▁\n\n\nstatus_married\n0\n1.00\n0.40\n0.02\n0.34\n0.39\n0.40\n0.41\n0.45\n▁▃▇▆▁\n\n\nstatus_nev_mar\n0\n1.00\n0.27\n0.03\n0.19\n0.25\n0.27\n0.29\n0.34\n▂▅▇▇▂\n\n\nstatus_other\n0\n1.00\n0.33\n0.02\n0.28\n0.32\n0.33\n0.34\n0.45\n▃▇▃▁▁\n\n\nhispanic\n0\n1.00\n0.22\n0.11\n0.02\n0.17\n0.25\n0.30\n0.42\n▅▁▆▇▂\n\n\npop_chg\n0\n1.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n▁▂▇▃▁\n\n\n\n\n\nOverall the data look good except for the avg_gas_price variable, which is the only variable that has any missing values. I’ll turn to that shortly, but I’ll briefly describe the variables in the data set below:\n\ncity : self-explanatory\ndate: month expressed as the first day of the month\nhpi: Case-Shiller Home Price Index value\nunemp_rate: Unemployment rate\navg_gas_price: Average gas price\nage_lt_18: Proportion of the population younger than 18 years old\nage_18_35: Proportion of the population between the ages of 18 and 35\nage_36_65: Proportion of the population between the ages of 36 and 65\nage_gt_65: Proportion of the population older than 65 years old\neduc_hs_less: Proportion of the population with up to a high school diploma\neduc_college: Proportion of the population with more than a high school diploma and up to a bachelor’s degree\neduc_grad: Proportion of the population with education higher than a bachelor’s degree\nstatus_married: Proportion of the population that is married; including separated\nstatus_nev_mar: Proportion of the population that has never been married\nstatus_other: Proportion of the population not categorized as either status_married or status_nev_mar\nhispanic: Proportion of the population that reported Hispanic ethnicity\npop_chg: Population change from one month to the next\n\nOnce I skimmed the data, I took a closer look at the percent of avg_gas_price values that were missing for each city.\n\n\nCode\necon_data |&gt;\n  group_by(city) |&gt;\n  summarise(pct_miss_gas = sum(is.na(avg_gas_price)) / n()) |&gt;\n  ggplot(aes(y = pct_miss_gas, x = city)) +\n  geom_col(fill = \"#2c3e50\") +\n  scale_y_continuous(limits = c(-0.1, 1.1), labels = scales::percent) +\n  labs(\n    y = \"Percent Missing\",\n    x = NULL,\n    title = \"Missingness in avg_gas_price variable by city\"\n  ) + \n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nEvidently data were missing only for San Diego. Below are the start and end dates of average gas price data for San Diego in this data set.\n\n\nCode\necon_data |&gt;\n  filter(city %in% \"San Diego\", !is.na(avg_gas_price)) |&gt;\n  summarise(start_date = min(date), end_date = max(date)) |&gt;\n  gt() |&gt;\n  gt_bold_head()\n\n\n\n\n\n\n\n\nstart_date\nend_date\n\n\n\n\n2018-01-01\n2022-12-01\n\n\n\n\n\n\n\nAs it turns out, reports of average gas price data for San Diego start in January of 2018, which means that 12 years of data would be missing. With that many values missing for one of the cities it makes sense to drop the average gas price variable.\n\n\nCode\necon_data &lt;- select(econ_data, !avg_gas_price)\n\necon_data |&gt; slice(1:6) |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\ncity\ndate\nhpi\nunemp_rate\nage_lt_18\nage_18_35\nage_36_65\nage_gt_65\neduc_hs_less\neduc_college\neduc_grad\nstatus_married\nstatus_nev_mar\nstatus_other\nhispanic\npop_chg\n\n\n\n\nDallas\n2006-01-01\n121.9108\n5.1\n0.2891927\n0.2805833\n0.3565649\n0.07365909\n0.3582388\n0.3352050\n0.06503496\n0.4248964\n0.2140741\n0.3610295\n0.2643563\n0.004055119\n\n\nDallas\n2006-02-01\n121.3285\n5.2\n0.2894647\n0.2715200\n0.3474445\n0.09157079\n0.3543670\n0.3517287\n0.06345765\n0.4104042\n0.2209950\n0.3686007\n0.2612721\n0.004038741\n\n\nDallas\n2006-03-01\n121.5217\n5.0\n0.2991726\n0.2852235\n0.3324601\n0.08314374\n0.3481381\n0.3412725\n0.06266023\n0.4069059\n0.2147862\n0.3783079\n0.2732806\n0.004022495\n\n\nDallas\n2006-04-01\n122.3996\n4.8\n0.2938252\n0.2784571\n0.3445776\n0.08314010\n0.3324701\n0.3589532\n0.06155738\n0.3932068\n0.2133219\n0.3934713\n0.2540514\n0.004006380\n\n\nDallas\n2006-05-01\n123.2863\n4.9\n0.2744636\n0.2824573\n0.3631222\n0.07995692\n0.3610289\n0.3519889\n0.05978182\n0.4005990\n0.2167855\n0.3826155\n0.2722199\n0.003990393\n\n\nDallas\n2006-06-01\n124.4985\n5.3\n0.2739175\n0.2861816\n0.3643186\n0.07558240\n0.3512415\n0.3512756\n0.06253083\n0.4065603\n0.2122996\n0.3811401\n0.2734042\n0.003974533\n\n\n\n\n\n\n\nFinally, I’d like to visualize the densities of all of the variables by city just to make sure nothing jumps out that might need any correction.\n\n\nCode\necon_data |&gt;\n  select(-date) |&gt;\n  pivot_longer(-city, names_to = \"variable\") |&gt;\n  ggplot(aes(x = value, fill = city)) +\n  geom_density(alpha = 0.5) +\n  facet_wrap(\"variable\", scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThere are some interesting patterns, particularly in the differences across the age groups, but there’s nothing alarming. With all this data in hand, the next step is to perform some time series analysis on the Case-Shiller HPI which I’ll show in Part 2 of this set of posts."
  },
  {
    "objectID": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html",
    "href": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html",
    "title": "Forecasting with {modeltime} - Part III",
    "section": "",
    "text": "In my previous post on this topic – Forecasting with {modeltime} - Part II – I performed a basic analysis of the Case-Shiller HPI time series for four U.S. cities (Dallas, Detroit, NYC, and San Diego). In this post I’m going to use some of the conclusions from that analysis to build ARIMA models using both a “global” modeling process and an “iterative” modeling process using the modeltime (Dancho 2023) and tidymodels (Kuhn and Wickham 2020) frameworks.\nOne of the distinguishing features of modeltime among other time series modeling frameworks is that it works very well with tidymodels. In fact, like tidymodels , modeltime serves as a platform for building consistent workflows using algorithms from other packages. modeltime builds on top of tidymodels to create a framework for dealing with the specific challenges of time series modelling and putting it all in a convenient, consistent API.\nI’ll also be using tidyverse (Wickham et al. 2019) and gt (Iannone et al. 2024), per usual, and timetk (Dancho and Vaughan 2023). modeltime and timetk are developed and maintained by the good people at Business Science and tidyverse, tidymodels, and gt are developed and maintained by the generous folks at Posit."
  },
  {
    "objectID": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#time-series-analysis-review",
    "href": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#time-series-analysis-review",
    "title": "Forecasting with {modeltime} - Part III",
    "section": "Time Series Analysis Review",
    "text": "Time Series Analysis Review\nIn the aforementioned time series analysis I found that the time series data for all four cities had a strong trend and a strong seasonal component. The trend was mostly muted by using a lag of 1 and the seasonal component of each series was generally muted by differencing twelve times, which makes sense given that the data are monthly (annual seasonal cycle). I was able to use these methods to greatly reduce the influence of non-stationary components according to the Augmented Dickey-Fuller test. The ACF and PACF plots led me to conclude to use an AR(3) and MA(5) ARIMA model. To deal with the seasonality I differenced the data twelve times. This will translate to a seasonal differencing component of (1) in the ARIMA model. In the previous post I wrote that I would be performing ARIMA rather than SARIMA… things change.\n\n\nCode\nlibrary(tidyverse)\nlibrary(timetk)\nlibrary(modeltime)\nlibrary(tidymodels)\nlibrary(gt)\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")\ntidymodels_prefer()"
  },
  {
    "objectID": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#differences-between-global-and-iterative-modeling",
    "href": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#differences-between-global-and-iterative-modeling",
    "title": "Forecasting with {modeltime} - Part III",
    "section": "Differences between global and iterative modeling",
    "text": "Differences between global and iterative modeling\nConventionally, time series models work by regressing the outcome – the most recent or future observation(s) – on previous observations of itself. We can refer to this as a “local” model because the variance in the data corresponds to only the single time series. A global model, on the other hand, pools the data to get a global variance and uses that information to make forecasts.\nThe primary advantages of a global model are speed and scalability. A second advantage is that each, individual forecasting model can “learn” from the history of the other time series that it’s pooled with. The disadvantage is that each individual model will likely be less accurate than if it was fit on an individual basis.\nThe advantage of modeling time series locally and iteratively is that this process most likely yields the highest quality fit and accuracy for each, individual model. The disadvantage is that as the number of models grows, so do the computational burden and processing time.\nIn two sections below I’ll demonstrate both processes with an ARIMA model using the parameters mentioned above and in the last section I’ll compare the results from both."
  },
  {
    "objectID": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#building-a-global-arima-model",
    "href": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#building-a-global-arima-model",
    "title": "Forecasting with {modeltime} - Part III",
    "section": "Building a global ARIMA model",
    "text": "Building a global ARIMA model\nAs mentioned above, modeltime follows the Tidymodels convention. As such, it starts with partitioning data into training and testing data sets, continues with writing a recipe and model specification (and combining them in a workflow object), follows with fitting the model, and continues with model evaluation and selection.\n\n\n\n\n\n\nTip\n\n\n\nFor more information on the Tidymodels framework, please check out https://www.tmwr.org/.\n\n\n\nSplitting data\nFor a global modeling process one can use the timetk::time_series_split() function which will create a list containing the data set, one vector of training data IDs and one of testing data IDs, and a 1-column data set containing all IDs.\nThe time_series_split() function uses 5 initial cross-validation periods by default with each one being progressively longer using a specified period of time as the assessment period. This assessment period is specified by the asses argument.\n\n\nCode\necon_splits_global &lt;- econ_data |&gt;\n  time_series_split(\n    assess = \"2 year\",\n    cumulative = TRUE,\n    date_var = date   \n  )\n\nstr(econ_splits_global)\n\n\nList of 4\n $ data  : tibble [816 × 3] (S3: tbl_df/tbl/data.frame)\n  ..$ city: chr [1:816] \"Dallas\" \"San Diego\" \"New York\" \"Detroit\" ...\n  ..$ date: Date[1:816], format: \"2006-01-01\" \"2006-01-01\" ...\n  ..$ hpi : num [1:816] 122 247 213 127 121 ...\n $ in_id : int [1:720] 1 2 3 4 5 6 7 8 9 10 ...\n $ out_id: int [1:96] 721 722 723 724 725 726 727 728 729 730 ...\n $ id    : tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ id: chr \"Slice1\"\n - attr(*, \"class\")= chr [1:2] \"ts_cv_split\" \"rsplit\"\n\n\n\n\nSpecifying a modeling workflow and fit the model\nTo specify the workflow I’ll first write the recipe…\n\n\n\n\n\n\nLagging/Differencing as a recipe step\n\n\n\n\n\nOne might be tempted to add lags and differences in the recipe, but this leads to a mess that’s really difficult to get out of. It all starts with the fact that a lagged and/or differenced variable will have a different name and will create missing values. I found that it’s best to handle these operations in the model specification.\n\n\n\n\n\nCode\narima_rec_global &lt;- recipe(hpi ~ date, data = training(econ_splits_global))\n\narima_rec_global |&gt; summary() |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\nvariable\ntype\nrole\nsource\n\n\n\n\ndate\ndate\npredictor\noriginal\n\n\nhpi\ndouble, numeric\noutcome\noriginal\n\n\n\n\n\n\n\n… then the model object.\nThe model specification below will be used for both processes.\n\n\nCode\n1arima_spec &lt;- arima_reg(\n2  non_seasonal_ar = 3,\n3  non_seasonal_ma = 5,\n4  non_seasonal_differences = 1,\n5  seasonal_differences = 1,\n6  seasonal_period = 12\n) |&gt;\n7  set_engine(\"arima\")\n\narima_spec\n\n\n\n1\n\nCall the API for building ARIMA models\n\n2\n\nSet the AR order (p in conventional notation)\n\n3\n\nSet the MA order (q in conventional notation)\n\n4\n\nSet the degree of non-seasonal differencing (d in conventional notation)\n\n5\n\nSet the degree of seasonal differencing (D in conventional notation)\n\n6\n\nSet the length of the seasonal period\n\n7\n\nSet the modeling engine to stats::arima()\n\n\n\n\nARIMA Regression Model Specification (regression)\n\nMain Arguments:\n  seasonal_period = 12\n  non_seasonal_ar = 3\n  non_seasonal_differences = 1\n  non_seasonal_ma = 5\n  seasonal_differences = 1\n\nComputational engine: arima \n\n\nNow that those are complete I can put them together in a workflow object.\n\n\nCode\n1arima_wflow_global &lt;- workflow() |&gt;\n2  add_model(arima_spec) |&gt;\n3  add_recipe(arima_rec_global)\n\narima_wflow_global\n\n\n\n1\n\nCreate a container for a workflow object\n\n2\n\nAdd the above model specification\n\n3\n\nAdd the recipe\n\n\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nMain Arguments:\n  seasonal_period = 12\n  non_seasonal_ar = 3\n  non_seasonal_differences = 1\n  non_seasonal_ma = 5\n  seasonal_differences = 1\n\nComputational engine: arima \n\n\nThe final step here is to fit the model\n\n\nCode\nglobal_fit_start &lt;- proc.time()\n\narima_fit_global &lt;- arima_wflow_global |&gt;\n  fit(training(econ_splits_global))\n\nglobal_fit_end &lt;- proc.time()\n\narima_fit_global\n\n\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nSeries: outcome \nARIMA(3,1,5)(0,1,0)[12] \n\nCoefficients:\n          ar1      ar2      ar3     ma1     ma2      ma3     ma4     ma5\n      -1.0582  -0.9518  -0.8510  0.3737  0.1482  -0.2853  0.3877  0.5537\ns.e.   0.0215   0.0290   0.0211  0.0424  0.0587   0.0406  0.0251  0.0324\n\nsigma^2 = 1.659:  log likelihood = -1186.55\nAIC=2391.11   AICc=2391.37   BIC=2432.16\n\n\n\n\nModel evaluation\nTo deal with some of the characteristics specific to time series modeling, the modeltime framework uses conventions like the mdl_time_tbl (modeltime table) class to store a variety of elements of a time series model including accuracy metrics. This class is also used in model calibration and refitting for future forecasting. Here I’ll write the modeltime table for the global process.\nI’m showing the model time table using a print() method rather than as a gt table because the list in the second column (list) would render in a cumbersome way.\n\n\nCode\narima_mtt_global &lt;- modeltime_table(arima_fit_global)\n\narima_mtt_global\n\n\n# Modeltime Table\n# A tibble: 1 × 3\n  .model_id .model     .model_desc            \n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                  \n1         1 &lt;workflow&gt; ARIMA(3,1,5)(0,1,0)[12]\n\n\nThis table contains the model and some additional information about it. I’ll now use this table to calibrate the global model to the individual time series with\n\n\nCode\narima_calib_global &lt;- arima_mtt_global |&gt;\n  modeltime_calibrate(new_data = testing(econ_splits_global), id = \"city\")\n\narima_calib_global\n\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; ARIMA(3,1,5)(0,1,0)[12] Test  &lt;tibble [96 × 5]&gt;\n\n\nWith the calibration done I can now look at the accuracy of the model both at a global level and at an individual model level.\nGetting the accuracy is done by the same function: modeltime_accuracy(). Whether it outputs global or local model accuracy is determined by the acc_by_id argument.\n\nGlobal model accuracy\n\n\nCode\narima_calib_global |&gt;\n  modeltime_accuracy(acc_by_id = FALSE) |&gt;  \n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nARIMA(3,1,5)(0,1,0)[12]\nTest\n19.61\n6.3\n0.18\n6.6\n29.38\n0.94\n\n\n\n\n\n\n\n\n\nLocal model accuracy\n\n\nCode\narima_calib_global |&gt;\n  modeltime_accuracy(acc_by_id = TRUE) |&gt;\n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\nTable 1: global model accuracy table\n\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\n.model_id\n.model_desc\n.type\ncity\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\n1\nARIMA(3,1,5)(0,1,0)[12]\nTest\nDallas\n23.44\n8.36\n4.48\n8.85\n27.63\n0.86\n\n\n1\nARIMA(3,1,5)(0,1,0)[12]\nTest\nDetroit\n6.47\n3.89\n3.68\n3.74\n9.62\n0.82\n\n\n1\nARIMA(3,1,5)(0,1,0)[12]\nTest\nNew York\n3.80\n1.45\n1.60\n1.43\n5.15\n0.95\n\n\n1\nARIMA(3,1,5)(0,1,0)[12]\nTest\nSan Diego\n44.73\n11.49\n5.93\n12.36\n50.70\n0.68\n\n\n\n\n\n\n\n\n\n\nFor comparison you can jump ahead to see the iterative model accuracy table 2.\n\n\nAccuracy plot\nBelow is a visual representation of how the models performed against the test data (the last 2 years).\n\n\nCode\n1arima_calib_global |&gt;\n2  modeltime_forecast(\n3    new_data = testing(econ_splits_global),\n4    actual_data = econ_data,\n5    conf_by_id = TRUE,\n6    keep_data = TRUE\n  ) |&gt;\n7  group_by(city) |&gt;\n8  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"Forecast of Test Data - Global\",\n    .facet_ncol = 2\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n1\n\nCall up the object containing the calibrated models\n\n2\n\nCall the modeltime_forecast() function\n\n3\n\nSpecify the new data to test forecasts against; in this case it’s the testing split from the object created by time_series_split()\n\n4\n\nSpecify what to use as the actual data; in this case it’s the original econ_data object\n\n5\n\nSpecify whether to generate confidence intervals by individual data series\n\n6\n\nSpecify whether to keep the columns from the actual data; I specified TRUE to keep the “city” column\n\n7\n\nGroup by city to generate the forecasts by city\n\n8\n\nPlot the forecasts as a stationary plot\n\n\n\n\n\n\n\n\n\n\nFigure 1: global model accuracy plot\n\n\n\n\n\nFor comparison you can jump ahead to see the iterative model accuracy plot 3.\n\n\n\nForecasting the future\nThe first step in getting forecasts for the future (with respect to the available data) is to refit the model using all of the training data after calibrating the model. That is done with the modeltime_refit() function for a global process.\n\n\nCode\nglobal_refit_start &lt;- proc.time()\n\narima_refit_global &lt;- modeltime_refit(arima_calib_global, data = econ_data) \n\nglobal_refit_end &lt;- proc.time()\n\narima_refit_global\n\n\n# Modeltime Table\n# A tibble: 1 × 5\n  .model_id .model     .model_desc             .type .calibration_data\n      &lt;int&gt; &lt;list&gt;     &lt;chr&gt;                   &lt;chr&gt; &lt;list&gt;           \n1         1 &lt;workflow&gt; ARIMA(3,1,5)(0,1,0)[12] Test  &lt;tibble [96 × 5]&gt;\n\n\nThe next step is to build a container for the data that has all the correct dates and ID columns. This is done with timetk::future_frame() .\n\n\n\n\n\n\nfuture_frame() with multiple series\n\n\n\n\n\nfuture_frame() only generates one series per data set fed to it, so it’s really important to group the data before executing the function if you have multiple series or you’ll only get one series of future dates.\n\n\n\n\n\nCode\narima_future_global &lt;- econ_data |&gt;\n  group_by(city) |&gt;\n  future_frame(.length_out = 12, .bind_data = FALSE, .date_var = date)\n\n\nAnd finally one can generate a plot of the forecasts with modeltime_forecast() and plot the time series with plot_modeltime_forecast() . Better not forget to group by series!\n\n\nCode\nforecast_future_global &lt;- arima_refit_global |&gt;\n  modeltime_forecast(\n    new_data = arima_future_global,\n    actual_data = econ_data,\n    conf_by_id = TRUE\n  )\n\nforecast_future_global |&gt;\n  group_by(city) |&gt;\n  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"1 Year Forecast into the Future - Global\",\n    .facet_ncol = 2\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#building-an-iterative-arima-model",
    "href": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#building-an-iterative-arima-model",
    "title": "Forecasting with {modeltime} - Part III",
    "section": "Building an iterative ARIMA model",
    "text": "Building an iterative ARIMA model\nNow I’ll go through an iterative modeling workflow with the same model parameters for comparison. A key difference in the workflow is that the iterative model requires the construction of a nested table.\nIn modeltime iterative forecasting is called “Nested Forecasting”.\n\nSplitting data\nOne of the big differences in building a nested model from building a global model in modeltime happens right at the beginning with splitting the data. With this method the future dates are added right up front with extend_timeseries() , the length of actual data is distinguished from that of future periods in nest_timeseries() and the train/test split is created with split_nested_timeseries() .\n\n\nCode\necon_splits_iterative &lt;- econ_data |&gt;\n  extend_timeseries(\n    .id_var = city,\n    .date_var = date,\n    .length_future = 12\n  ) |&gt;\n  nest_timeseries(\n    .id_var = city,\n    .length_future = 12,\n    .length_actual = 17 * 12\n  ) |&gt;\n  split_nested_timeseries(.length_test = 24)\n\necon_splits_iterative\n\n\n# A tibble: 4 × 4\n  city      .actual_data       .future_data      .splits         \n  &lt;chr&gt;     &lt;list&gt;             &lt;list&gt;            &lt;list&gt;          \n1 Dallas    &lt;tibble [204 × 2]&gt; &lt;tibble [12 × 2]&gt; &lt;split [180|24]&gt;\n2 Detroit   &lt;tibble [204 × 2]&gt; &lt;tibble [12 × 2]&gt; &lt;split [180|24]&gt;\n3 New York  &lt;tibble [204 × 2]&gt; &lt;tibble [12 × 2]&gt; &lt;split [180|24]&gt;\n4 San Diego &lt;tibble [204 × 2]&gt; &lt;tibble [12 × 2]&gt; &lt;split [180|24]&gt;\n\n\n\n\nSpecifying a modeling workflow and fit the model\nI’ll use the same model specification as above, but the recipe has to be specified to use the new splits object.\n\n\nCode\narima_rec_iterative &lt;- recipe(\n  hpi ~ date,\n  extract_nested_train_split(econ_splits_iterative)\n)\n\narima_rec_iterative |&gt; summary() |&gt; gt() |&gt; gt_bold_head()\n\n\n\n\n\n\n\n\nvariable\ntype\nrole\nsource\n\n\n\n\ndate\ndate\npredictor\noriginal\n\n\nhpi\ndouble, numeric\noutcome\noriginal\n\n\n\n\n\n\n\nI’ll also create a new workflow object that just updates the previous one with the new recipe.\n\n\nCode\narima_wflow_iterative &lt;- arima_wflow_global |&gt;\n  update_recipe(arima_rec_iterative)\n\narima_wflow_iterative\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: arima_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nARIMA Regression Model Specification (regression)\n\nMain Arguments:\n  seasonal_period = 12\n  non_seasonal_ar = 3\n  non_seasonal_differences = 1\n  non_seasonal_ma = 5\n  seasonal_differences = 1\n\nComputational engine: arima \n\n\nAnd now I’ll fit the model to all four cities iteratively using modeltime_nested_fit() .\n\n\nCode\niterative_fit_start &lt;- proc.time()\n\narima_fit_iterative &lt;- econ_splits_iterative |&gt;\n  modeltime_nested_fit(arima_wflow_iterative)\n\niterative_fit_end &lt;- proc.time()\n\narima_fit_iterative\n\n\n# Nested Modeltime Table\n  # A tibble: 4 × 5\n  city      .actual_data       .future_data .splits          .modeltime_tables \n  &lt;chr&gt;     &lt;list&gt;             &lt;list&gt;       &lt;list&gt;           &lt;list&gt;            \n1 Dallas    &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n2 Detroit   &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n3 New York  &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n4 San Diego &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n\n\n\n\nModel evaluation\n\nLocal model accuracy\n\n\nCode\narima_fit_iterative |&gt;\n  extract_nested_test_accuracy() |&gt;\n  table_modeltime_accuracy(.interactive = FALSE)\n\n\n\n\nTable 2: iterative model accuracy table\n\n\n\n\n\n\n\n\n\nAccuracy Table\n\n\ncity\n.model_id\n.model_desc\n.type\nmae\nmape\nmase\nsmape\nrmse\nrsq\n\n\n\n\nDallas\n1\nARIMA\nTest\n40.94\n14.63\n7.82\n16.09\n46.48\n0.81\n\n\nDetroit\n1\nARIMA\nTest\n4.64\n2.84\n2.64\n2.88\n5.38\n0.84\n\n\nNew York\n1\nARIMA\nTest\n5.35\n2.04\n2.25\n2.07\n6.91\n0.89\n\n\nSan Diego\n1\nARIMA\nTest\n30.24\n7.81\n4.01\n8.23\n35.85\n0.65\n\n\n\n\n\n\n\n\n\n\nYou can jump back to compare this with the global model accuracy table 1.\n\n\nAccuracy plot\nBelow is a visual representation of how the models from the iterative modeling process performed against the test data (the last 2 years).\n\n\nCode\narima_fit_iterative |&gt;\n  extract_nested_test_forecast() |&gt;\n  group_by(city) |&gt;\n  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"Forecast of Test Data - Iterative\",\n    .facet_ncol = 2\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 3: iterative model accuracy plot\n\n\n\n\n\nYou can jump back to compare this with the global model accuracy plot 1.\n\n\n\nForecasting the future\nAs with the global process above, the first step here will be to refit the model to the full training data set. Note that the iterative model process requires the use of modeltime_nested_refit() rather than modeltime_refit() .\n\n\n\n\n\n\ncontrol_* functions\n\n\n\n\n\nmodeltime has separate control_* functions for different purposes such as this case in which modeltime_refit() has control_refit() and modeltime_nested_refit() has control_nested_refit(). Make sure to use the correct one.\n\n\n\n\n\nCode\niterative_refit_start &lt;- proc.time()\n\narima_refit_iterative &lt;- arima_fit_iterative |&gt;\n  modeltime_nested_refit(control = control_nested_refit(verbose = FALSE))\n\niterative_refit_end &lt;- proc.time()\n\narima_refit_iterative\n\n\n# Nested Modeltime Table\n  # A tibble: 4 × 5\n  city      .actual_data       .future_data .splits          .modeltime_tables \n  &lt;chr&gt;     &lt;list&gt;             &lt;list&gt;       &lt;list&gt;           &lt;list&gt;            \n1 Dallas    &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n2 Detroit   &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n3 New York  &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n4 San Diego &lt;tibble [204 × 2]&gt; &lt;tibble&gt;     &lt;split [180|24]&gt; &lt;mdl_tm_t [1 × 5]&gt;\n\n\nSince the future data container was already created in the splitting object, I can go right to forecasting the future here.\n\n\nCode\nforecast_future_iterative &lt;- arima_refit_iterative |&gt;\n  extract_nested_future_forecast()\n\nforecast_future_iterative |&gt;\n  group_by(city) |&gt;\n  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"1 Year Forecast into the Future - Global\",\n    .facet_ncol = 2\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#comparison",
    "href": "posts/20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.html#comparison",
    "title": "Forecasting with {modeltime} - Part III",
    "section": "Comparison",
    "text": "Comparison\nThe two processes follow the same workflow, but have some key operational differences:\n\nDifferent structures for the data split objects\nThe iterative workflow does not require calibration to the individual time series\nProcessing time\n\nBelow are two tables showing the processing times for fitting and refitting the global and iterative models.\n\n\nCode\nglobal_fit_time &lt;- (global_fit_end - global_fit_start) |&gt;\n  enframe(name = \"metric\") |&gt;\n  mutate(value = as.numeric(value), model_process = \"Global Fit\") |&gt;\n  drop_na()\n\niterative_fit_time &lt;- (iterative_fit_end - iterative_fit_start) |&gt;\n  enframe(name = \"metric\") |&gt;\n  mutate(value = as.numeric(value), model_process = \"Iterative Fit\") |&gt;\n  drop_na()\n\nbind_rows(global_fit_time, iterative_fit_time) |&gt;\n  ggplot(\n    aes(x = metric, y = value, fill = model_process, group = model_process)\n  ) +\n  geom_col(position = \"dodge\") +\n  labs(\n    fill = NULL,\n    x = NULL,\n    y = \"Time in seconds\",\n    title = \"Comparison of Model Fit Processing Times\"\n  ) +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_timetk +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\nCode\nglobal_refit_time &lt;- (global_refit_end - global_refit_start) |&gt;\n  enframe(name = \"metric\") |&gt;\n  mutate(value = as.numeric(value), model_process = \"Global Refit\") |&gt;\n  drop_na()\n\niterative_refit_time &lt;- (iterative_refit_end - iterative_refit_start) |&gt;\n  enframe(name = \"metric\") |&gt;\n  mutate(value = as.numeric(value), model_process = \"Iterative Refit\") |&gt;\n  drop_na()\n\nbind_rows(global_refit_time, iterative_refit_time) |&gt;\n  ggplot(\n    aes(x = metric, y = value, fill = model_process, group = model_process)\n  ) +\n  geom_col(position = \"dodge\") +\n  labs(\n    fill = NULL,\n    x = NULL,\n    y = \"Time in seconds\",\n    title = \"Model Refit Processing Times\"\n  ) +\n  scale_fill_manual(values = c(\"red\", \"blue\")) +\n  theme_timetk +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 6\n\n\n\n\n\nBesides these operational differences, the processes yield different levels of accuracy due to differences in the data on which they are trained (or to which they are initially fit). It’s also clear that the differences in models can yield some very big differences in forecasts. Global models have performed well in competitions that involved thousands of time series, so it’s possible that the variance across the four time series in this analysis is too high to make global modeling worthwhile.\n\n\nCode\npatchwork::wrap_plots(\n  forecast_future_global |&gt;\n  group_by(city) |&gt;\n  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"1 Year Forecast into the Future - Global\",\n    .facet_ncol = 1\n  ) +\n  theme(plot.title = element_text(hjust = 0.5)),\n  forecast_future_iterative |&gt;\n  group_by(city) |&gt;\n  plot_modeltime_forecast(\n    .interactive = FALSE,\n    .title = \"1 Year Forecast into the Future - Iterative\",\n    .facet_ncol = 1\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n)\n\n\n\n\n\n\n\n\n\nIn the next post I’ll run a global modeling process with a few different algorithms and specifications to get a few hundred models and find which ones run best."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Projects in Data Science and Statistics",
    "section": "",
    "text": "Forecasting with {modeltime} - Part IV\n\n\nTuning many global models\n\n\n\nTime Series\n\n\nForecasting\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting with {modeltime} - Part III\n\n\nGlobal vs. iterative modeling\n\n\n\nTime Series\n\n\nForecasting\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nFeb 16, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting with {modeltime} - Part II\n\n\nTime series analysis\n\n\n\nTime Series\n\n\nInferential Statistics\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting with {modeltime} - Part I\n\n\nGathering data\n\n\n\nTime Series\n\n\nData Acquisition\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Modeling with R\n\n\n\n\n\n\nText Analysis\n\n\nTopic Modeling\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to cepumd\n\n\n\n\n\n\nPackage Development\n\n\nData Wrangling\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Threshold vs. Upsampling\n\n\nDealing with Unbalanced Data\n\n\n\nMachine Learning\n\n\nClassification\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\nArcenis Rojas\n\n\n\n\n\n\n\n\n\n\n\n\nOpening the ML ‘Black Box’ with Shapley Values\n\n\nA Tidy Approach\n\n\n\nMachine Learning\n\n\nPrediction\n\n\n\n\n\n\n\n\n\nAug 29, 2021\n\n\nArcenis Rojas\n\n\n\n\n\n\nNo matching items"
  }
]