{
  "hash": "109075cd47876a0fac9d10f9b81be231",
  "result": {
    "markdown": "---\ntitle: \"Topic Modeling with R\"\nauthor: \"Arcenis Rojas\"\nformat: \n  html:\n    link-external-newwindow: true\neditor: visual\n---\n\n\n## Introduction\n\nFor this project I'll be following an example of performing topic modeling with R using the Tidytext format introduced by Julia Silge and David Robinson in their [\"Text Mining with R! A Tidy Approach\"](https://www.tidytextmining.com/index.html). And I'll be using abstracts submitted by [Johns Hopkins University](https://www.jhu.edu/) for funding to the [NIH HEAL Initiative](https://heal.nih.gov/). The list of funded projects can be found at <https://heal.nih.gov/funding/awarded>. A benefit of using this data is that it doesn't require much cleaning as Twitter text might. For the purposes of this demonstration I won't do any data cleaning. The objective will be to find the optimal number of topics to which to attribute the abstracts. I'll be using the `tidyverse` and `tidytext` packages for wrangling, the `stm` package for modeling, the `janitor` package for cleaning column names (but I won't load it because I'm only going to use it once), the `knitr` package for table formatting, and the `pacman` package to load my packages (also only used once).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, tidytext, stm, knitr)\n```\n:::\n\n\n## Downloading the abstract data\n\nI'll start by downloading the data and just looking at the dimensionality.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_data <- read_csv(\n  \"https://heal.nih.gov/funding/awarded/export?combine=johns%20hopkins&_format=csv\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 30 Columns: 10\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (9): Project #, Project Title, Research Focus Area, Research Program, Ad...\ndbl (1): Year Awarded\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncolnames(jh_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Project #\"           \"Project Title\"       \"Research Focus Area\"\n [4] \"Research Program\"    \"Administering IC(s)\" \"Institution(s)\"     \n [7] \"Investigator(s)\"     \"Location(s)\"         \"Year Awarded\"       \n[10] \"Summary\"            \n```\n:::\n:::\n\n\nIt looks like there are 30 columns in the data. I'm going to clean up the column names and select only the Project \\# (to use as an identifier), Project Title, and Summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_clean <- jh_data %>%\n  janitor::clean_names() %>%\n  select(project_number, project_title, summary)\n\nhead(jh_clean, 3) %>% kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n|project_number  |project_title                                                                                               |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n|:---------------|:-----------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|1RF1NS134549-01 |Validation of a New Large-Pore Channel as a Novel Target for Neuropathic Pain                               |Activation of immune cells (microglia) in the central nervous system and neuroinflammation have emerged as key drivers of neuropathic pain. These processes can be triggered by release of ATP, the compound that provides energy to many biochemical reactions. The source and mechanism of ATP release are poorly understood but could be targets of novel treatment approaches for neuropathic pain. This project will use genetic, pharmacological, and electrophysiological approaches to determine whether a large pore channel called Swell 1 that spans the cell membrane is the source of ATP release and resulting neuropathic pain and thus could be a treatment target.                                                                 |\n|1R01DA059473-01 |Sleep and Circadian Rhythm Phenotypes and Mechanisms Associated With Opioid Use Disorder Treatment Outcomes |Chronic opioid use has well known effects on sleep quality, including disordered breathing during sleep and other abnormalities related to circadian rhythms. However, little is known about the relationship between sleep-related symptoms and non-medical opioid use among individuals being treated for opioid use disorder. This longitudinal study aims to identify biological pathways that may account for these associations. The research will first determine associations of sleep and proxy measures of circadian rhythms with non-medical opioid use. Second, they will investigate emotional processes associated with sleep/circadian symptoms and opioid treatment outcomes.                                                       |\n|5U54DA049110-04 |Data Center for Acute to Chronic Pain Biosignatures                                                         |Understanding the mechanisms underlying the transition to chronic pain is key to mitigating the dual epidemics of chronic pain and opioid use in the United States. As part of the National Institutes of Health-funded Acute to Chronic Pain Signatures Program, the Data Integration and Resource Center aims to This project will support a post-doctoral trainee to develop the skills and knowledge needed to pursue a successful career in clinical pain research. The research will involve integrating imaging, physiology, -omics, behavioral, and clinical data to develop biosignatures for the transition from acute to chronic pain, toward understanding how the nervous and immune systems affect post-surgical pain and opioid use. |\n:::\n:::\n\n\n## Tidy the data\n\nIn this next step I'm going to tidy the data and remove stop words. Then I'll take a look at word frequencies and TF-IDF metrics to get an idea of what I might expect from my model.\n\nI'm first going to use the `tidytext::unnest_tokens()` function to tokenize the data by single word then remove stop words from the text. I'll also get word frequencies in this one step. Also, I'm going to drop the project title for the time being, but I'll bring it back later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tidy <- jh_clean %>%\n  mutate(summary = str_c(project_title, summary, sep = \" \")) %>%\n  select(-project_title) %>%\n  unnest_tokens(word, summary, token = \"words\") %>%\n  anti_join(stop_words, by = \"word\") %>%\n  count(project_number, word)\n\njh_tidy %>% arrange(-n) %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 3\n   project_number    word         n\n   <chr>             <chr>    <int>\n 1 1RF1NS113883-01   pain         9\n 2 3R01MD009063-05S1 pain         9\n 3 1UG3NS115718-01   mrgprx1      8\n 4 1U01HL150568-01   sleep        7\n 5 1U01HL150835-01   sleep        7\n 6 5U54DA049110-04   pain         7\n 7 1R01DA059473-01   opioid       6\n 8 1R01DA059473-01   sleep        6\n 9 1R61AT012279-01   shoulder     6\n10 1U01HL150835-01   stress       6\n```\n:::\n:::\n\n\nThe words \"pain\" and \"spontaneous\" are 2 of the more frequently occurring words in document 1RF1NS113883-01 and the words \"pain\" and \"sleep\" seem to be 2 of the most frequent across documents. Let's see how these words do in terms of how important they are in distinguishing a document from other documents.\n\nI'm going to add TF-IDF metrics using the `tidytext::bind_tf_idf()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tfidf <- jh_tidy %>% bind_tf_idf(word, project_number, n)\n\njh_tfidf %>%\n  arrange(-tf_idf) %>% head(15)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 Ã— 6\n   project_number    word            n     tf   idf tf_idf\n   <chr>             <chr>       <int>  <dbl> <dbl>  <dbl>\n 1 3U24TR001609-04S1 tin             1 0.333   3.40  1.13 \n 2 3U24TR001609-04S1 summary         1 0.333   2.71  0.903\n 3 3U24TR001609-04S1 supplement      1 0.333   2.30  0.768\n 4 1R61HL156248-01   intranasal      1 0.125   3.40  0.425\n 5 1R61HL156248-01   leptin          1 0.125   3.40  0.425\n 6 1R61HL156248-01   induced         1 0.125   2.71  0.339\n 7 1R61HL156248-01   respiratory     1 0.125   2.71  0.339\n 8 1R61HL156248-01   summary         1 0.125   2.71  0.339\n 9 1R61HL156248-01   depression      1 0.125   2.30  0.288\n10 1R61AT012279-01   shoulder        6 0.0833  3.40  0.283\n11 1RF1AG068997-01   bone            3 0.0698  3.40  0.237\n12 1R01DA057655-01   harm            4 0.0667  3.40  0.227\n13 1R01DA057655-01   reduction       4 0.0667  3.40  0.227\n14 1UG3NS115718-01   mrgprx1         8 0.0656  3.40  0.223\n15 1R01DA059473-01   sleep           6 0.102   2.01  0.205\n```\n:::\n:::\n\n\nIt's very strange that 2 documents have the term \"summary\" as the only term in the summary. This is indicated by a term frequency (tf) of 1. Let's take a look at the original text for these documents.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_clean %>%\n  filter(\n    project_number %in% c(\"1R61HL156248-01\", \"3U24TR001609-04S1\")\n  ) %>%\n  select(project_number, summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 2\n  project_number    summary   \n  <chr>             <chr>     \n1 1R61HL156248-01   no summary\n2 3U24TR001609-04S1 no summary\n```\n:::\n:::\n\n\nThere was no summary for either of these documents! The word \"summary\" had a really high TF-IDF because it was the only word (\"no\" is a stopword and was dropped) in these documents and it likely isn't used very frequently in other documents. This is one of the benefits of doing this kind of check before going into modeling. Interestingly, though, neither \"pain\" nor \"stress\" are in the terms with the top 15 TF-IDF values. I imagine it's because these words occur in a lot of the documents and, therefore, would not be very useful in distinguishing one project from another. So I'll drop the two project with no summaries and go into the modeling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tidy <- jh_tidy %>%\n  filter(\n    !project_number %in% c(\"1R61HL156248-01\", \"3U24TR001609-04S1\")\n  )\n```\n:::\n\n\n## Modeling\n\nTo be able to perform text mining of any kind, including structural topic modeling, I first need to put my data into a sparse matrix. For this I'll use the `tidytext::cast_sparse()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_sparse <- jh_tidy %>% cast_sparse(project_number, word, n)\n\ndim(jh_sparse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]   28 1023\n```\n:::\n:::\n\n\nI now have a matrix of 28 and 1023 columns. There is now one column for each distinct word in my corpus of data. I'm going to create a structural model with 3 topics (number chosen arbitrarily) just to get an idea of how I might evaluate a model. I'm going to use the `stm::stm()` function and use the LDA algorithm to generate my model.\n\n\n::: {.cell hash='topic-modeling_cache/html/run-lda-3_b447db75d38a687085e552c63b5475b7'}\n\n```{.r .cell-code}\njh_lda_3 <- jh_sparse %>% stm(K = 3, init.type = \"LDA\", seed = 212)\n```\n:::\n\n\nNow I want to see what some of the most frequently occurring words are within each topic. For this I'm going to simply use the `tidy` method from `stm` on the model. By default, the tidy method extracts values of \"beta\" which tells us the probability that a word comes from a given topic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_lda_3 %>%\n  tidy() %>% \n  group_by(topic) %>%\n  slice_max(beta, n = 10, with_ties = FALSE) %>%\n  ungroup() %>%\n  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  labs(y = \"Beta\", x = NULL) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](topic-modeling_files/figure-html/lda-3-top-terms-1.png){width=672}\n:::\n:::\n\n\nThe word \"pain\" is the most frequently occurring word in each topic and the word \"opioid\" occurs in the top 10 words of 2 of the topics. It looks like the model's not doing a great job of separating topics based on this.\n\nAnother interesting metric to look at is \"gamma\" which is the probability that a given document is related to a given topic and this is also provided by our model output, which we can access by specifying the `matrix = \"gamma\"` argument in our `tidy()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_lda_3 %>%\n  tidy(matrix = \"gamma\") %>%\n  mutate(topic = str_c(\"Topic\", topic, sep = \" \") %>% factor()) %>%\n  ggplot(aes(document, gamma, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(breaks = 1:nrow(distinct(jh_tidy, project_number))) +\n  facet_wrap(~ topic, scales = \"free\", ncol = 1) +\n  labs(x = \"Document\", y = \"Gamma\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](topic-modeling_files/figure-html/lda-3-topic-probs-1.png){width=672}\n:::\n:::\n\n\nIt looks like each document has a high probability of coming from only 1 topic. This is pretty good considering what the beta values showed.\n\nNext I'll see if there's a better number of topics to separate these documents into.\n\n## Choosing a number of topics\n\nFirst I'm going to build a set of LDA models with values of K (number of topics) ranging from 2 to 24. I'm also going to create a holdout dataset to calculate the heldout-likelihood metric later. This is somewhat akin to using a cross-validation holdout set in other machine learning methodologies.\n\n\n::: {.cell hash='topic-modeling_cache/html/tune-models_ae2f2385410a727074611470885b0711'}\n\n```{.r .cell-code}\njh_tune_models <- tibble(k = 2:24) %>%\n  mutate(\n    lda_mod = map(\n      k,\n      ~ stm(jh_sparse, K = .x, init.type = \"LDA\", seed = 212)\n    )\n  )\n\nheldout <- make.heldout(jh_sparse)\n```\n:::\n\n\nNow I'm going to extract the metrics that I want to use to evaluate my model. I'm going to focus on the approach of find the value of K with highest held-out likelihood and lowest residual, but I'm also interested in looking at semantic coherence and exclusivity metrics, so I'll keep those as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tune_results <- jh_tune_models %>%\n  mutate(\n    exclusivity = map(lda_mod, exclusivity),\n    semantic_coherence = map(\n      lda_mod,\n      semanticCoherence,\n      documents = jh_sparse,\n      M = 10\n    ),\n    eval_heldout = map(lda_mod, eval.heldout, heldout$missing),\n    residual = map(lda_mod, checkResiduals, jh_sparse)\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tune_results %>%\n  transmute(\n    k,\n    Residuals = map_dbl(residual, \"dispersion\"),\n    `Semantic coherence` = map_dbl(semantic_coherence, mean),\n    `Held-out likelihood` = map_dbl(eval_heldout, \"expected.heldout\"),\n    Exclusivity = map_dbl(exclusivity, mean)\n  ) %>%\n  pivot_longer(-k, names_to = \"metrics\", values_to = \"value\") %>%\n  ggplot(aes(k, value, color = metrics)) +\n  geom_line(show.legend = FALSE) +\n  labs(x = \"Number of Topics\", y = NULL) +\n  facet_wrap(~ metrics, scales = \"free_y\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](topic-modeling_files/figure-html/plot-tune-results-1.png){width=672}\n:::\n:::\n\n\nThe highest held-out likelihood score comes from the model with `K = 17` and the lowest residual comes from the model with `K = 6`. However, the model with `K = 17` also has a relatively low residual whereas the model with `K = 6` has the lowest held-out likelihood. Based on this, it looks like the 17-topic model is best. This model also has relatively high semantic coherence and exclusivity. Notably, exclusivity and the residual seem to flatten out and semantic coherence increases dramatically at `K > 10`.\n\nNow I'm going to look at both the `K = 6` and `K = 17` models to see whether one is clearly better. I'm going to start by looking at how easily each model would predict each document to belong to a specific topic. For that, I'm going to look at the maximum values of gamma for each document. For comparison I'm going to show the same plot for the approach of maximizing semantic coherence (`K = 21`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tune_models %>% \n  filter(k %in% c(6, 17, 21)) %>%\n  mutate(\n    max_gammas = map(\n      lda_mod,\n      ~ tidy(.x, matrix = \"gamma\") %>%\n        group_by(document) %>%\n        summarise(gamma = max(gamma), .groups = \"drop\")\n    ),\n    k_label = str_c(\"Number of Topics:\", k, sep = \" \") %>% fct_reorder(k)\n  ) %>%\n  select(-lda_mod) %>%\n  unnest(max_gammas) %>%\n  ggplot(aes(document, gamma, fill = factor(k))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_continuous(breaks = 1:24) +\n  labs(x = \"Document\", y = \"Maximum Gamma\") +\n  facet_wrap(~ k_label, ncol = 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](topic-modeling_files/figure-html/compare-gamma-1.png){width=672}\n:::\n:::\n\n\nIt looks like separating the documents into 17 topics yields relatively high probabilities of corresponding to specific topics across the board, whereas separating the documents into 6 topics gives a max probability of less than 75% of document 1 belonging to a specific topic. If I'm interested in parsimony, `K = 6` isn't so bad, but if I'm more interested in accuracy I'd go with `K = 17`. Looking at the 21-topic model -- the model with highest semantic coherence -- the maximum probabilities of belonging to a singular, specific topic are relatively low for documents 20 and 21.\n\nFinally, using the 6-topic model, I'd like to see how the titles of the projects grouped by topic.\n\nFor the purposes of visualization I'll look at the top words defining the topics for the 6-topic model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tune_models %>% \n  filter(k == 6) %>%\n  pluck(\"lda_mod\", 1) %>%\n  tidy() %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>%\n  ungroup() %>%\n  mutate(topic = str_c(\"Topic\", topic, sep = \" \")) %>%\n  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  scale_x_reordered() +\n  labs(y = \"Beta\", x = NULL) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](topic-modeling_files/figure-html/lda-6-top-words-1.png){width=672}\n:::\n:::\n\n\nThe word \"pain\" appears in the top 10 words of all but topic 3 (in further analysis I might consider making this a stop word), but topic 4 seems to be about joint issues given the prevalence of the words \"osteoarthritis\" and \"knee\" and topic 2 seems to be about brain activity during sleep given the prevalence of the words \"sleep\", \"neuropathic\", \"sensory\", and \"firing\" (neurons \"firing\"?).\n\nFinally, I'll look at the titles grouped by each project's maximum gamma value. You be the judge ðŸ˜Š\n\n\n::: {.cell}\n\n```{.r .cell-code}\njh_tune_models %>% \n  filter(k == 6) %>%\n  pluck(\"lda_mod\", 1) %>%\n  tidy(matrix = \"gamma\") %>%\n  nest(data = -document) %>%\n  mutate(\n    data = map(data, ~ slice_max(.x, gamma, n = 1, with_ties = FALSE)),\n    project_number = rownames(jh_sparse)\n  ) %>%\n  unnest(data) %>%\n  left_join(\n    jh_clean %>% select(project_number, project_title),\n    by = \"project_number\"\n  ) %>%\n  select(topic, project_title, gamma) %>%\n  mutate(gamma = round(gamma, 3)) %>%\n  arrange(topic, gamma) %>%\n  kable(booktabs = TRUE)\n```\n\n::: {.cell-output-display}\n| topic|project_title                                                                                                                                                                | gamma|\n|-----:|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----:|\n|     1|Home-based transcutaneous electrical acustimulation for abdominal pain                                                                                                       | 0.967|\n|     1|Data Center for Acute to Chronic Pain Biosignatures                                                                                                                          | 0.993|\n|     1|Quantifying and Treating Myofascial Dysfunction in Post Stroke Shoulder Pain                                                                                                 | 0.994|\n|     1|Validation of peripheral CGRP signaling as a target for the treatment of pain in chronic pancreatitis                                                                        | 0.996|\n|     1|Sympathetic-mediated sensory neuron cluster firing as a novel therapeutic target for neuropathic pain                                                                        | 0.996|\n|     1|Development of MRGPRX1 positive allosteric modulators as non-addictive therapies for neuropathic pain                                                                        | 0.997|\n|     2|A sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain                                                                                        | 0.988|\n|     2|A sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain                                                                                        | 0.990|\n|     2|Increasing Participant Diversity in a 'Sequenced-Strategy to Improve Outcomes in People with Knee Osteoarthritis Pain (SKOAP)                                                | 0.990|\n|     2|Mentorship of Junior Investigators on HEAL-SKOAP                                                                                                                             | 0.992|\n|     2|A sequenced-strategy for improving outcomes in patients with knee osteoarthritis pain                                                                                        | 0.993|\n|     2|Improving Function and Reducing Opioid Use for Patients with Chronic Low Back Pain in Rural Communities Through Improved Access to Physical Therapy Using Telerehabilitation | 0.993|\n|     3|Sleep and Circadian Rhythm Phenotypes and Mechanisms Associated With Opioid Use Disorder Treatment Outcomes                                                                  | 0.992|\n|     3|Subchondral Bone Cavities in Osteoarthritis Pain                                                                                                                             | 0.992|\n|     3|Validation of a New Large-Pore Channel as a Novel Target for Neuropathic Pain                                                                                                | 0.992|\n|     3|Implementing and Evaluating the Impact of Novel Mobile Harm Reduction Services on Overdose Among Women who use Drugs: The SHOUT Study                                        | 0.994|\n|     3|The Short and Long-Term Dynamics of Opioid/Stimulant Use: Mixed Methods to Inform Overdose Prevention and Treatment Related to Polysubstance Use                             | 0.994|\n|     3|Evaluating Suvorexant for Sleep Disturbance in Opioid Use Disorder                                                                                                           | 0.994|\n|     3|UNDERSTANDING THE INTERSECTION BETWEEN OPIOIDS AND SUICIDE THROUGH THE SOUTHWEST HUB                                                                                         | 0.996|\n|     4|DC Research Infrastructure Building &amp; Initiative to Reach, Engage, and Retain in MOUD Patients with OUD                                                                  | 0.992|\n|     4|DC Research Infrastructure Building &amp; Initiative to Reach, Engage, and Retain in MOUD Patients with OUD                                                                  | 0.992|\n|     5|Social Networks among Native American caregivers participating in an evidence-based and culturally informed intergenerational intervention                                   | 0.994|\n|     5|Evaluating the Role of the Orexin System in Circadian Rhythms of Sleep and Stress in Persons on Medication-Assisted Treatments for Opioid Use Disorder                       | 0.995|\n|     5|ETHNIC DIFFERENCES IN ENDOGENOUS PAIN REGULATION: PET IMAGING OF OPIOID RECEPTORS                                                                                            | 0.996|\n|     6|Effects of experimental sleep disruption and fragmentation on cerebral Mu-opioid receptor function, Mu-opioid receptor agonist analgesia, and abuse liability.               | 0.678|\n|     6|7/24 Healthy Brain and Child Development National Consortium                                                                                                                 | 0.992|\n|     6|HEALthy ORCHARD: Developing plans for a Baltimore site of the HEALthy BCD study                                                                                              | 0.994|\n|     6|DEVELOPMENT &amp; MALLEABILITY FROM CHILDHOOD TO ADULTHOOD                                                                                                                   | 0.994|\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}