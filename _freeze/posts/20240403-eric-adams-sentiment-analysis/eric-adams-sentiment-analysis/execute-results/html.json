{
  "hash": "3b3ca21106ab2ac8111fe2b5ed3c7c5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Sentiment Analysis of NYC's Mayor Eric Adams' Speeches\"\nsubtitle: \"Problem-Solving in Action\"\nauthor: \"Arcenis Rojas\"\ndate: \"04/03/2024\"\neditor: visual\nexecute:\n  warning: false\n  error: false\nbibliography: eric-adams-speeches.bib\ncategories:\n  - Web Scraping\n  - Text Analysis\n  - Animation\n---\n\n\n## Project description\n\nThis project came from a question that I had about how the sentiment in NYC Mayor Eric Adams' speeches might have changed over the last few years given some of the different challenges he's had in office ranging from COVID-19-related policies to more recent issues with immigration in NYC. As often happens, this simple question introduced unanticipated challenges and again made me really grateful to the open-source community.\n\nFor this analysis I relied heavily on [tidyverse](https://www.tidyverse.org/) [@tidyverse] packages for data wrangling; the [rvest](https://rvest.tidyverse.org/) [@rvest] and [polite](https://dmi3kno.github.io/polite/) [@polite] packages for webscraping; the [tidytext](https://juliasilge.github.io/tidytext/) [@tidytext], [textdata](https://github.com/EmilHvitfeldt/textdata) [@textdata], and [textclean](https://github.com/trinker/textclean) [@textclean] packages for text analysis functions; and the [ggplot2](https://ggplot2.tidyverse.org/) [@ggplot2], [ggwordcloud](https://lepennec.github.io/ggwordcloud/index.html) [@ggwordcloud], and [gganimate](https://gganimate.com/) [@gganimate] packages for visualization.\n\n## Getting speech data\n\nTo perform this analysis I would have find a repository of his speeches online and figure out how to scrape them. Because I thought there would be a lot of speeches, I knew that I would run into issues dealing with having to add delays to my scraping function so as not to run into conflict with the rules of the site. I also thought about the problem of having to scrape pages with Javascript on them and remembered that [Hadley Wickham](https://hadley.nz/) [posted on LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:7159172919758065665?updateEntityUrn=urn%3Ali%3Afs_feedUpdate%3A%28V2%2Curn%3Ali%3Aactivity%3A7159172919758065665%29) about an experimental update to [rvest](https://rvest.tidyverse.org/) that included tools for scraping live sites using [chromote](https://rstudio.github.io/chromote/). Without remember this I probably would not have pursued the project because the alternative for scraping live web pages with R is [RSelenium](https://docs.ropensci.org/RSelenium/articles/basics.html) (at least as far as I know), which requires a lot more set-up work than this project would have been worth.\n\nAs far as dealing with the robots on the NYC news site, I also wanted to scrape in an ethical way. Thinking about this led me to two things: 1) [Rules for ethical webscraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) and 2) the [polite](https://dmi3kno.github.io/polite/) package. First, there is the practical consideration that some sites place limits on the frequency with which they will serve results to clients and if the scraping process doesn't respect that one may end up getting one result with content and a bunch of empty results thereafter if not just outright errors. Aside from this practical consideration, my position is that if I'm getting all this great data for free (that someone is paying to host) the least I can do is identify myself to the host so the host knows I mean no harm.\n\n### Finding speech transcripts\n\nOnce I decided to go ahead, the first challenge was to get transcripts of Mayor Adams' speeches. A simple web search led me to the [NYC.gov](https://www.nyc.gov/) website which has a section for [news coming out of the office of the Mayor](https://www.nyc.gov/office-of-the-mayor/news.page). I noticed that this site had more than his speeches, but there were some titles of articles that started with the word \"Transcript\". After looking through a few pages I saw that this was the case for many articles, so I thought I could use this to isolate the articles that contained speeches. The next step was figuring out how to scrape the data.\n\n### Challenges in webscraping\n\nI first figured out that I would have to scrape in two stages. In the first stage, I would have to scrape all of the pages containing news article titles and their corresponding links, which would require knowing how many pages of links there are. I would then have to filter these links for the ones that contained speech transcripts. In the second stage I would have to go to each of the links from the first stage and scrape the speech text.\n\nGoing into the first stage I wanted to use the `polite::nod()` to jump from one page to the next. However, I learned that all of the pages with search results were live pages (Javascript) and `polite::scrape()` is designed to work with static elements, so there were elements of the page that I wasn't getting. I thought I would either be stuck or have to make a trade-off between getting output and behaving ethically until I found [yusuzech's Cheat Sheet for Web Scraping Using R](https://github.com/yusuzech/r-web-scraping-cheat-sheet) in which he talks about different ways to deal with this particular challenge. There I learned about an [entry in Hartley Brody's blog](https://blog.hartleybrody.com/web-scraping-cheat-sheet/) containing his Web Scraping Cheat Sheet. In the \"More Advanced Topics\" section he mentioned that a page with live elements (Javascript) requires that the content be in static form *somewhere* on the server. I was able to find this page for the NYC site, which then required identifying the links that I needed using [regex](https://en.wikipedia.org/wiki/Regular_expression), which was a much smaller problem than choosing between getting data from a live site and scraping ethically.\n\n### Webscraping code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(xml2)\nlibrary(rvest)\nlibrary(polite)\nlibrary(tidytext)\nlibrary(textclean)\nlibrary(textdata)\nlibrary(ggwordcloud)\nlibrary(gganimate)\n\n# Resolve common namespace conflicts about function names\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"lag\", \"dplyr\")\nif (\"tidymodels\" %in% .packages()) tidymodels_prefer()\n```\n:::\n\n\nBelow is the code that I used for scraping the data. I tried to parallize this code, but I learned that code that uses external pointers, will error out with the different packages I tried for parallelization.\n\nHere I declare a function for the first stage of the web scraping process which takes as arguments the `polite::bow()` object and the number of the search page to scrape.\n\n::: callout-note\n`polite::nod()` shows that this function jumps to the static site listing the search results. The query in the path string shows that all of the results are from the dates between 12/31/2021 and 12/31/2024.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nget_article_links <- function(bow_obj, srch_pg_num) {\n  message(str_c(\"Scraping links from page \", srch_pg_num))\n  \n  search_sess <- nod(                                                     # <1>\n    bow_obj,\n    path = str_c(\n      \"/home/lscs/NewsFilterService.page\",\n      \"?category=all&contentType=all&startDate=12/31/2021&endDate=12/31/2024&\",\n      \"language=english&pageNumber=\",\n      srch_pg_num\n    )\n  )\n  \n  scrape(search_sess) |>                                                  # <2>\n    as_list() |>\n    pluck(\"root\", 2) |>                                                   # <3>\n    str_extract(\"\\\\[\\\\{(.*)\") |>                                          # <4>\n    str_extract_all('(?<=URL\":\")[\\\\/\\\\-a-z0-9]+') |>                      # <5>\n    flatten_chr() |>                                                            \n    str_subset(\"transcript\") |>                                           # <6>\n    enframe(value = \"speech_link\") |>                                     # <7>\n    select(-name)\n}\n```\n:::\n\n\n1.  Jump from the root node of the website to the search page of interest\n2.  Scrape the HTML as XML text\n3.  Keep only the second element of the root node (exclude head elements)\n4.  Keep everything after some additional metadata elements\n5.  Extract everything that proceeded the text indicating a URL\n6.  Keep only the URL's that contained the term \"transcript\"\n7.  Put the vector of links into a dataframe\n\nThe next function scrapes a page containing speech data taking as arguments the `polite::bow()` object, one of the links from the first stage, and a number of the link being read to track progress.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\nget_speech_data <- function(bow_obj, article_path, speech_num = 1) {\n  message(str_c(\"Scraping speech number \", speech_num))\n  \n  speech_html <- nod(bow_obj, path = article_path) |>               # <1>\n    scrape(accept = \"html\") |>\n    html_element(\".ls-col .ls-col-body .ls-row.main\") |>\n    html_element(\".iw_component .container .col-content\")\n  \n  speech_date <- speech_html |>                                     # <2>\n    html_elements(\".richtext p .date\") |>\n    html_text2()\n  \n  speech_content <- speech_html |>                                  # <3>\n    html_element(\".richtext\") |>\n    html_elements(\"p\")\n  \n  data.frame(                                                       # <4>\n    speech_title = speech_html |>                                   # <5>\n      html_element(\".article-title\") |>\n      html_text2(),\n    speech_date = speech_date,\n    speaker = speech_content |>                                     # <6>\n      map(\n        \\(x) {\n          bold_txt <- x |> html_elements(\"strong\") |> html_text()\n          \n          ifelse(length(bold_txt) == 0, NA_character_, bold_txt)\n        }\n      ) |>\n      flatten_chr(),\n    speech_text = html_text2(speech_content)                        # <7>\n  ) |>\n    filter(\n      !speech_text %in% speech_date,                                # <8>\n      str_detect(speech_text, \"^\\\\#+$\", negate = TRUE)              # <9>\n    ) |>\n    mutate(                                                         # <10>\n      speaker = str_extract(speaker, \".*(?<=:)\"),\n      speech_text = if_else(\n        !is.na(speaker),\n        str_remove(speech_text, speaker),\n        speech_text\n      ) |>\n        str_trim() |>\n        str_squish(),\n      speaker = str_remove(speaker, \":$\")\n    ) |>\n    fill(speaker, .direction = \"down\") |>                           # <11>\n    group_by(speech_title, speech_date, speaker) |>\n    summarise(                                                      # <12>\n      speech_text = str_flatten(speech_text, collapse = \" \"),\n      .groups = \"drop\"\n    ) |>\n    filter(                                                         # <13>\n      speaker %in% c(\"Eric Adams\") || str_detect(speaker, \"Mayor\")\n    )\n}\n```\n:::\n\n\n1.  Jump from the root node of the website to the page containing the speech text and scrape the text\n2.  Store the date from the text\n3.  Store all of the content that is in a paragraph element\n4.  Create a dataframe to organize all the data\n5.  Create a column for the speech title\n6.  Extract the bold text which indicates who the speaker is (some transcripts had multiple speakers)\n7.  Create a column to store the speech text\n8.  Filter out any rows in which the speech text is the date (the date is stored in a separate column)\n9.  Remove rows that only contain hash tags in the speech text\n10. Clean up the speech text and speaker data columns\n11. Fill the speaker column down to ensure that every paragraph has a speaker associated with it\n12. Collapse all of the text together by speaker\n13. Keep only text spoken by Mayor Eric Adams\n\nNext is the code to actually scrape the data using the above functions. You might notice that I used `possibly()` from the `purrr` package. I used this for error handling because I wanted to be able to filter each of the data sets for any empty results. Of course, empty results were a lot less likely because the polite::bow() object stored all the necessary permissions. I thought the redundancy was more helpful than harmful here given how long this code would have to run. This process resulted in 280 search pages and about 1,100 speeches with a couple of seconds of wait time between requests, so I had a strong incentive to minimize errors and to be able to easily identify operations that either returned errors or empty results.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"show\"}\n# Store the domain of the NYC.gov website\nnyc_root <- \"https://www.nyc.gov\"\n\n# Instantiate a session with permissions to the NYC website\nnyc_sess <- bow(nyc_root, user_agent = \"arcenis-r\", force = TRUE)\n\n# Get the number of pages of news article links\nnum_search_pages <- read_html_live(\n  str_c(nyc_root, \"/office-of-the-mayor/news.page\")\n) |>\n  html_element(\".main-search-results\") |>\n  html_text2() |>\n  str_extract(\"\\\\d+(?= pages)\") |>\n  as.numeric()\n\n# Get the links to all news articles from each page\nspeech_links <- tibble(search_page_num = seq(1, num_search_pages)) |>\n  mutate(\n    page_links = map(\n      search_page_num,\n      possibly(\\(x) get_article_links(nyc_sess, x), NULL)\n    )\n  )\n\n# Scrape speeches\nnyc_mayor_speeches <- speech_links |>\n  unnest(page_links) |>\n  mutate(speech_number = row_number()) |>\n  mutate(\n    speech_data = map2(\n      speech_link, speech_number,\n      possibly(\\(x, y) get_speech_data(nyc_sess, x, y), NULL)\n    )\n  )\n```\n:::\n\n\n## Sentiment analysis over time\n\nWith the data secured, the next step would be to look at changes in the sentiment of his speeches over time. To do that I tokenized the speech data and used both the AFINN-111 valence lexicon [@nielsen11] and the NRC word-emotion association lexicon [@mohammad13].\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nspeech_df <- nyc_mayor_speeches |>\n  select(-c(search_page_num, speech_number)) |>\n  mutate(\n    speech_text = str_to_lower(speech_text) |>\n      replace_url() |>\n      replace_html() |>\n      replace_contraction() |>\n      replace_word_elongation() |>\n      replace_ordinal() |>\n      replace_date() |>\n      replace_money() |>\n      replace_number() |>\n      replace_kern() |>\n      replace_curly_quote() |>\n      replace_non_ascii() |>\n      str_squish() |>\n      str_trim(),\n    speech_date = mdy(speech_date)\n  ) |>\n  group_by(speech_date) |>\n  mutate(\n    speech_id = str_c(\n      speech_date |> as.character() |> str_remove_all(\"-\"),\n      row_number()\n    )\n  ) |>\n  ungroup()\n\nspeech_word_tokens <- speech_df |>\n  unnest_tokens(word, speech_text) |>\n  anti_join(stop_words, by = \"word\") |>\n  filter(is.na(as.numeric(word)))\n\nspeech_tf_idf <- speech_word_tokens |>\n  count(speech_id, word, sort = TRUE) |>\n  group_by(speech_id) |>\n  mutate(speech_word_count = sum(n)) |>\n  ungroup() |>\n  bind_tf_idf(word, speech_id, n)\n```\n:::\n\n\nThis first plot shows sentiment over time based on the AFINN-111 lexicon with a linear modeling smoother. I This lexicon applies a value between -5 (most negative) and 5 (most positive) to each word in the lexicon. I calculated sentiment by date by taking the mean sentiment of all of the words used on each day. The plot includes a linear smoother. It appears that the sentiment in Eric Adams' speeches has become more positive over time.\n\n::: callout-note\nI calculated average sentiment by day rather than by speech because Eric Adams made multiple speeches on some days.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspeech_word_tokens |>\n  inner_join(get_sentiments(\"afinn\"), by = \"word\") |>\n  group_by(speech_date) |>\n  summarise(mean_valence = mean(value, na.rm = TRUE), .groups = \"drop\") |>\n  ggplot(aes(speech_date, mean_valence)) +\n  geom_line() +\n  stat_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Sentiment in Eric Adams' Speeches Over Time\",\n    subtitle = \"Sentiment based on the AFINN lexicon\",\n    y = \"Average Speech Sentiment\",\n    x = NULL\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n```\n\n::: {.cell-output-display}\n![](eric-adams-sentiment-analysis_files/figure-html/plot-sentiment-afinn-1.png){width=672}\n:::\n:::\n\n\nThe below plot shows sentiment over time based on the NRC lexicon. This lexicon assigns each word a label indicating a given emotion such as anger, joy, or disgust. It also assigns \"positive\" and \"negative\" labels, which I filtered out. For this plot I calculated the percentage of words that were related to each emotion out of all non-stop words in the speech and applied a gamma smoother. This plot shows that there were spikes in disgust, fear, and anger in the first half of 2023. The reduction of these negative emotions in Eric Adams' speeches could account for the apparent positive trend in the last plot; the emotional content got less negative.\n\n::: callout-note\n**Stop words** are words that are inconsequential to determining the meaning of a set of words or distinguish that set of words from other words. They also tend to be very common.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspeech_word_tokens |>\n  group_by(speech_date) |>\n  count(word) |>\n  mutate(total_words = sum(n)) |>\n  inner_join(\n    get_sentiments(\"nrc\") |>\n      filter(!sentiment %in% c(\"positive\", \"negative\")),\n    by = \"word\", relationship = \"many-to-many\"\n  ) |>\n  group_by(speech_date, sentiment) |>\n  reframe(sentiment_pct = sum(n) / total_words) |>\n  ggplot(aes(x = speech_date, y = sentiment_pct)) +\n  geom_line() +\n  stat_smooth(method = \"gam\", formula = y ~ s(x, bs = \"cs\")) +\n  labs(\n    title = \"Sentiment in Eric Adams' Speeches Over Time\",\n    subtitle = \"Sentiment based on the NRC lexicon\",\n    y = \"Percentage Speech Sentiment\",\n    x = NULL\n  ) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n```\n\n::: {.cell-output-display}\n![](eric-adams-sentiment-analysis_files/figure-html/plot-sentiment-nrc-1.png){width=672}\n:::\n:::\n\n\n## Word cloud animation\n\nBelow is a short animation of word clouds for each month in the data. Each word cloud includes 50 words with the highest TF-IDF (term frequency, inverse document frequency) values by month.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(98457)\nwrdcld_data <- speech_tf_idf |>\n  left_join(select(speech_df, speech_id, speech_date), by = \"speech_id\") |>\n  mutate(\n    speech_yrmo = floor_date(speech_date, \"month\")\n  ) |>\n  group_by(speech_yrmo) |>\n  slice_max(tf_idf, n = 50, with_ties = FALSE) |>\n  ungroup() |>\n  mutate(\n    yrmo_label = format(speech_date, format = \"%B, %Y\") |>\n      fct_reorder(speech_yrmo),\n    word_size = abs(1 / log(tf_idf)),\n    word_color = sample(1:5, n(), replace = TRUE) |> factor(),\n    word_angle = 45 * sample(-2:2, n(), replace = TRUE, prob = c(1, 1, 4, 1, 1))\n  )\n\nnum_frames <- wrdcld_data |>\n  distinct(speech_yrmo) |>\n  count() |>\n  pull(n)\n\nadams_cloud <- wrdcld_data |>\n  ggplot(\n    aes(label = word, size = tf_idf, color = word_color, angle = word_angle)\n  ) +\n  geom_text_wordcloud_area() +\n  scale_size_area(max_size = 50) +\n  labs(title = \"{closest_state}\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  transition_states(speech_yrmo) +\n  enter_fade() +\n  exit_fade() +\n  ease_aes(\"sine-in-out\")\n\nanimate(adams_cloud, fps = 0.5, nframes = num_frames)\n```\n\n::: {.cell-output-display}\n![](eric-adams-sentiment-analysis_files/figure-html/word-cloud-animation-1.gif)\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}