{
  "hash": "0fd75abf003a21f90c3c0486e4977473",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Dealing with Unbalanced Data\"\nsubtitle: \"Optimal Threshold vs. Upsampling\"\nauthor: \"Arcenis Rojas\"\nexecute:\n  warning: false\n  error: false\nbibliography: class-thresh-vs-upsampling-references.bib\ndate: \"2/15/2023\"\ncategories:\n  - machine learning\n  - data science\n  - classification\n---\n\n\n## Introduction\n\nIn any binary classification analysis it is common to have to deal with heavily unbalanced data, i.e., the frequency of the majority class in the dependent variable is overwhelmingly greater than that of the minority class. One of the main reasons this can be problematic is that many classification algorithms are biased toward the majority class. If 95% of the observations belong to the majority class in the training data and the algorithm always predicts the majority class, then it will have achieved 95% accuracy.\n\nThere are many ways of dealing with unbalanced data including changing from a classification algorithm to an anomaly detection algorithm like an isoforest or a one-class SVM, using SMOTE to re-balance the data sets in pre-processing, up- or down-sampling, and changing the classification threshold. Here I will only deal with up-sampling and changing the classification threshold to see how it affects classification metrics by way of going through an exercise. I'll first create and tune a base model then do the same for an up-sampled model using Tidymodels [@tidymodels]. Hyperparameter tuning will be done with the `finetune` package [@finetune]. I'll then find the optimal threshold for each using the `probably` package [@probably]. Finally I'll show a comparison of their respective classification metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(colino)\nlibrary(themis)\nlibrary(finetune)\nlibrary(probably)\nlibrary(skimr)\nlibrary(knitr)\nlibrary(kableExtra)\n\nconflicted::conflict_prefer(\"filter\", \"dplyr\")\nconflicted::conflict_prefer(\"select\", \"dplyr\")\ntidymodels_prefer()\n```\n:::\n\n\n## Lending Club Data\n\nFor this project I'll be using the `lending_club` data set from the `modeldata` [@modeldata] package. The data contain the `Class` variable which indicates whether a loan was \"good\" or bad\". For the purposes of this analysis I also will consider the \"bad\" outcome as the event to predict.\n\n## Data Exploration\n\nThe first step is to load the data using the `readr` package from the Tidyverse [@tidyverse] and summarize it using the `skimr` package [@skimr].\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(\"lending_club\", package = \"modeldata\")\n\nskim(lending_club) |> select(-complete_rate)\n```\n\n::: {.cell-output-display}\n\nTable: Data summary\n\n|                         |             |\n|:------------------------|:------------|\n|Name                     |lending_club |\n|Number of rows           |9857         |\n|Number of columns        |23           |\n|_______________________  |             |\n|Column type frequency:   |             |\n|factor                   |6            |\n|numeric                  |17           |\n|________________________ |             |\n|Group variables          |None         |\n\n\n**Variable type: factor**\n\n|skim_variable       | n_missing|ordered | n_unique|top_counts                              |\n|:-------------------|---------:|:-------|--------:|:---------------------------------------|\n|term                |         0|FALSE   |        2|ter: 7047, ter: 2810                    |\n|sub_grade           |         0|FALSE   |       35|C1: 672, B5: 624, A1: 612, B3: 607      |\n|addr_state          |         0|FALSE   |       50|CA: 1324, TX: 900, NY: 767, FL: 731     |\n|verification_status |         0|FALSE   |        3|Sou: 3742, Not: 3434, Ver: 2681         |\n|emp_length          |         0|FALSE   |       12|emp: 3452, emp: 893, emp: 769, emp: 733 |\n|Class               |         0|FALSE   |        2|goo: 9340, bad: 517                     |\n\n\n**Variable type: numeric**\n\n|skim_variable              | n_missing|     mean|       sd|      p0|      p25|      p50|      p75|      p100|hist  |\n|:--------------------------|---------:|--------:|--------:|-------:|--------:|--------:|--------:|---------:|:-----|\n|funded_amnt                |         0| 15683.56|  8879.11| 1000.00|  8500.00| 15000.00| 21000.00|  40000.00|▆▇▅▃▂ |\n|int_rate                   |         0|    12.53|     4.89|    5.32|     8.49|    11.99|    15.31|     28.99|▇▇▃▂▁ |\n|annual_inc                 |         0| 80320.36| 53450.17|    0.00| 50000.00| 68900.00| 96000.00| 960000.00|▇▁▁▁▁ |\n|delinq_2yrs                |         0|     0.33|     0.89|    0.00|     0.00|     0.00|     0.00|     22.00|▇▁▁▁▁ |\n|inq_last_6mths             |         0|     0.58|     0.88|    0.00|     0.00|     0.00|     1.00|      5.00|▇▁▁▁▁ |\n|revol_util                 |         0|    51.73|    24.38|    0.00|    33.20|    51.80|    70.40|    144.30|▅▇▇▂▁ |\n|acc_now_delinq             |         0|     0.01|     0.08|    0.00|     0.00|     0.00|     0.00|      2.00|▇▁▁▁▁ |\n|open_il_6m                 |         0|     2.75|     2.93|    0.00|     1.00|     2.00|     3.00|     32.00|▇▁▁▁▁ |\n|open_il_12m                |         0|     0.74|     1.01|    0.00|     0.00|     0.00|     1.00|     20.00|▇▁▁▁▁ |\n|open_il_24m                |         0|     1.62|     1.70|    0.00|     0.00|     1.00|     2.00|     30.00|▇▁▁▁▁ |\n|total_bal_il               |         0| 35286.92| 41923.62|    0.00|  9450.00| 23650.00| 46297.00| 585583.00|▇▁▁▁▁ |\n|all_util                   |         0|    60.31|    20.28|    0.00|    47.00|    62.00|    75.00|    198.00|▂▇▂▁▁ |\n|inq_fi                     |         0|     0.93|     1.47|    0.00|     0.00|     0.00|     1.00|     15.00|▇▁▁▁▁ |\n|inq_last_12m               |         0|     2.19|     2.44|    0.00|     0.00|     2.00|     3.00|     32.00|▇▁▁▁▁ |\n|delinq_amnt                |         0|    12.17|   565.47|    0.00|     0.00|     0.00|     0.00|  42428.00|▇▁▁▁▁ |\n|num_il_tl                  |         0|     8.64|     7.52|    0.00|     4.00|     7.00|    11.00|     82.00|▇▁▁▁▁ |\n|total_il_high_credit_limit |         0| 45400.75| 45103.21|    0.00| 16300.00| 34375.00| 60786.00| 554119.00|▇▁▁▁▁ |\n\n\n:::\n:::\n\n\nThe above data summary shows some important things. First, there are 9,857 observations and 23 variables with no missing values. Also, the data has 6 factor variables and 17 numeric variables. It shows that most of the numeric variables are left skewed. The \"sub_grade\" and \"addr_state\" factor variables have 35 and 50 levels respectively. With so many unique values it's very likely that some of the classes in each of those variables become overwhelmed by majority classes. While there are feature engineering steps that I can take such as consolidating infrequent classes or splitting up `sub_grade` into constituent parts, I'll just allow the feature selection step to handle it. Finally, the \"Class\" factor variable (dependent variable) has two levels with the majority class – \"good\" – having 9,340 observations and the minority class – \"bad\" – having 517 observations.\n\nHere I'll just look at the proportions of the `Class` variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlending_club |>\n  ggplot(aes(y = Class)) +\n  geom_bar(aes(x = (..count..) / sum(..count..))) +\n  scale_x_continuous(labels = scales::percent) +\n  labs(\n    x = \"Proportion\",\n    y = NULL,\n    title = \"Frequency of loan outcome categories (Class)\"\n  ) +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5))\n```\n\n::: {.cell-output-display}\n![](class-thresh-vs-upsampling_files/figure-html/outcome-frequencies-1.png){width=672}\n:::\n:::\n\n\nHere we see a sever class unbalance with only about 5% of loans having a \"bad\" outcome.\n\n## Create Data Splits and Other Common Elements\n\nFor the analysis I'll split the data into training (75%) and test (25%) sets stratified by the `Class` variable. I'll then create 100 bootstraps of the training data for model tuning, also stratified by the `Class` variable. Stratification will ensure that both cuts of the data and the bootstraps retain the class unbalance as close to the proportions of the original data.\n\nHere I also create a list of features to control the model tuning process.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create training/test split\nset.seed(95)\ntt_split <- initial_split(lending_club, prop = 0.75, strata = Class)\n\n# Create the bootstrap object\nset.seed(386)\nbstraps <- bootstraps(training(tt_split), times = 100, strata = Class)\n\n# Create the model object for the workflows\nlr_mod <- logistic_reg(mode = \"classification\", engine = \"glm\")\n\n# Create a list of control options\nrace_control <- control_race(\n  verbose = FALSE,\n  allow_par = FALSE,\n  burn_in = 3,\n  save_workflow = TRUE,\n  save_pred = TRUE\n)\n```\n:::\n\n\n## Train Base Model\n\nThe base recipe will perform the following steps:\n\n1.  Remove variables with near-zero variance\n\n2.  Scale and center all numeric variables\n\n3.  Convert categorical variables to dummy variables\n\n4.  Select variables based on mRMR using the `colino` package [@colino]; the percentile threshold used to decide which variables to keep will be chosen through model tuning\n\nThe base recipe will be combined with a logistic regression model specification to put into a workflow. This workflow then goes through a Bayesian hyperparameter tuning process to find the best mRMR threshold as mentioned above. \"Best\" is defined as the full model specification that yields the highest area under the receiver operator curve (ROC-AUC) because I'll be choosing the optimal probability threshold on the basis of this curve, even though it would be preferable to choose on the basis of the area under the precision-recall curve (PR-AUC) with unbalanced data. [@imbal_auprc]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a base recipe\nbase_rec <- recipe(Class ~ ., data = training(tt_split)) |>\n  step_nzv(all_predictors()) |>\n  step_normalize(all_numeric_predictors()) |>\n  step_dummy(all_nominal_predictors(), one_hot = FALSE) |>\n  step_select_mrmr(\n    all_predictors(),\n    threshold = tune(),\n    outcome = \"Class\"\n  )\n\n# First tune the base model to get the best set of variables with MRMR\nbase_wflow <- workflow() |> add_recipe(base_rec) |> add_model(lr_mod)\n\nset.seed(40)\nbase_tuned <- tune_race_win_loss(\n  base_wflow,\n  resamples = bstraps,\n  control = race_control,\n  grid = 10,\n  metrics = metric_set(pr_auc, roc_auc, accuracy)\n)\n\n# Get the workflow with the best MRMR threshold\nbest_base <- select_best(base_tuned, \"roc_auc\")\n\n# Get the MRMR threshold from the best base model\nmrmr_threshold <- best_base |> pull(threshold)\n```\n:::\n\n\nThe mRMR threshold associated with the best model is 0.9478018 .\n\n## Train Upsampled Model\n\nThe upsampled model will only add the pre-processing step of upsampling the minority class to the same proportion as the majority class (or close to it) by resampling observations with a `Class` value from the minority class using the `themis` package [@themis]. To ensure that this is the only difference, I'll take the mRMR threshold that was tuned for the base model directly from the best base model and specify it as the threshold for the upsampling model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Build a recipe for upsampling by first updating the MRMR step from the base recipe with the threshold from the tuning process then adding upsampling\nupsample_rec <- base_rec\nselect_mrmr_step <- tidy(upsample_rec) |>\n  filter(type %in% \"select_mrmr\") |>\n  pull(number)\n\nupsample_rec$steps[[select_mrmr_step]] <- update(\n  upsample_rec$steps[[select_mrmr_step]],\n  threshold = mrmr_threshold\n)\n\nupsample_rec <- upsample_rec |>\n  step_upsample(Class, over_ratio = tune())\n\n# Create the upsample workflow object\nupsample_wflow <- workflow() |> add_recipe(upsample_rec) |> add_model(lr_mod)\n\n# Tune the workflow containing the upsample recipe\nset.seed(40)\nupsample_tuned <- tune_race_win_loss(\n  upsample_wflow,\n  resamples = bstraps,\n  control = race_control,\n  grid = 10,\n  metrics = metric_set(pr_auc, roc_auc, accuracy)\n)\n\n# Get the workflow with the best MRMR threshold\nbest_upsample <- select_best(upsample_tuned, \"roc_auc\")\n```\n:::\n\n\nThe best over-sampling ratio is 1.1065577 .\n\n## Compare Model Classification Metrics\n\nNow that I've run both models and gotten the best hyperparameters for each, I'll finalize both models and ensure that I have the same sets of variables in the models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get evaluation datasets for both workflows\nset.seed(75)\neval_base <- base_wflow |>\n  finalize_workflow(best_base) |>\n  last_fit(tt_split)\n\n# Build the evaluation tibble for the upsample case\nset.seed(212)\neval_upsample <- upsample_wflow |>\n  finalize_workflow(select_best(upsample_tuned, \"roc_auc\")) |>\n  last_fit(tt_split)\n\neval_base |>\n  extract_fit_engine() |>\n  tidy() |>\n  select(base = term) |>\n  bind_cols(\n    eval_upsample |>\n      extract_fit_engine() |>\n      tidy() |>\n      select(upsample = term)\n  ) |>\n  kable(booktabs = TRUE) |> \n  kable_styling(\"striped\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> base </th>\n   <th style=\"text-align:left;\"> upsample </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> int_rate </td>\n   <td style=\"text-align:left;\"> int_rate </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> inq_last_6mths </td>\n   <td style=\"text-align:left;\"> inq_last_6mths </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> open_il_12m </td>\n   <td style=\"text-align:left;\"> open_il_12m </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sub_grade_G4 </td>\n   <td style=\"text-align:left;\"> sub_grade_G4 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> addr_state_SD </td>\n   <td style=\"text-align:left;\"> addr_state_SD </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> verification_status_Verified </td>\n   <td style=\"text-align:left;\"> verification_status_Verified </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe above table shows that both models ended up with the same sets of variables. Next we'll find the optimal threshold cut-offs for each set of predictions on the test data using Youden's J-Index.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_preds <- tibble(\n  truth = collect_predictions(eval_base) |> pull(Class),\n  estimate = collect_predictions(eval_base) |> pull(.pred_bad),\n  pred_class = collect_predictions(eval_base) |> pull(.pred_class)\n)\n\n# Find the optimal classification threshold for the base model\nopt_thresh_base <- base_preds |>\n  threshold_perf(truth, estimate, seq(0, 1, length.out = 1000)) |>\n  filter(.metric %in% \"j_index\") |>\n  slice_max(.estimate, with_ties = FALSE) |>\n  pull(.threshold)\n\nupsample_preds <- tibble(\n  truth = collect_predictions(eval_upsample) |> pull(Class),\n  estimate = collect_predictions(eval_upsample) |> pull(.pred_bad),\n  pred_class = collect_predictions(eval_upsample) |> pull(.pred_class)\n)\n\n# Find the optimal classification threshold for the upsample model\nopt_thresh_upsample <- upsample_preds |>\n  threshold_perf(truth, estimate, seq(0, 1, length.out = 1000))|>\n  filter(.metric %in% \"j_index\") |>\n  slice_max(.estimate, with_ties = FALSE) |>\n  pull(.threshold)\n```\n:::\n\n\nThe optimal probability threshold for classifying a loan as bad with the base model is 0.0550551 and that for the upsampling model is 0.5525526\n\nEach table of predictions currently has 3 columns showing the actual class for a given observation (truth), the prediction probability associated with that observation being a bad loan (estimate), and a predicted class based on a probability threshold of 0.5 (pred_class). The table below shows the first few predictions from the base model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_preds |>\n  slice_head(n = 10) |>\n  kable(booktabs = TRUE) |>\n  kable_styling(\"striped\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> truth </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:left;\"> pred_class </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0350714 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0107027 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0297951 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0432709 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0335889 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0310299 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0452632 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0226798 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0170578 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.0286983 </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nNow I'd like to add to each of these tables another column containing the predicted class based on the optimal probability threshold for each respective table. The below table shows a few cases for which the predicted class using a 0.5 probability threshold does not match the predicted class using the optimal probability threshold of 0.5525526 .\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_preds <- base_preds |>\n  mutate(\n    opt_pred_class = if_else(estimate >= opt_thresh_base, \"bad\", \"good\") |>\n      factor(levels = c(\"bad\", \"good\"))\n  )\n\nupsample_preds <- upsample_preds |>\n  mutate(\n    opt_pred_class = if_else(\n      estimate >= opt_thresh_upsample,\n      \"bad\",\n      \"good\"\n    ) |>\n      factor(levels = c(\"bad\", \"good\"))\n  )\n\nupsample_preds |> \n  filter(opt_pred_class != pred_class) |>\n  slice_head(n = 10) |>\n  kable(booktabs = TRUE) |>\n  kable_styling(\"striped\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> truth </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:left;\"> pred_class </th>\n   <th style=\"text-align:left;\"> opt_pred_class </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5030688 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:right;\"> 0.5284232 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5373165 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5007787 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5173375 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5463947 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:right;\"> 0.5297278 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5227147 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5284232 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> good </td>\n   <td style=\"text-align:right;\"> 0.5284232 </td>\n   <td style=\"text-align:left;\"> bad </td>\n   <td style=\"text-align:left;\"> good </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nNext I'll plot confusion matrices for each set of predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mats <- list(base = base_preds, upsample = upsample_preds) |>\n  map(\n    \\(x) list(\n      conf_mat(x, truth = truth, estimate = pred_class),\n      conf_mat(x, truth = truth, estimate = opt_pred_class)\n    ) |>\n      set_names(\"standard\", \"optimal\")\n  ) |>\n  list_flatten()\n\nconf_mat_plots <- conf_mats |>\n  imap(\n    \\(x, y) {\n      plot_title <- str_replace(y, \"_\", \" \") |>\n        str_to_title()\n      \n      autoplot(x, type = \"heatmap\") +\n        ggtitle(plot_title) +\n        theme(plot.title = element_text(hjust = 0.5))\n    }\n  )\n\ncowplot::plot_grid(plotlist = conf_mat_plots)\n```\n\n::: {.cell-output-display}\n![](class-thresh-vs-upsampling_files/figure-html/plot-conf-mats-1.png){width=672}\n:::\n:::\n\n\nThe above confusion matrices show that both changing the probability threshold for classification as a bad loan and upsampling have a significant impact on the number of positive predictions (\"bad\" loan), even though most of those are predicted incorrectly with this model. It's also interesting to note that doing both – changing the probability threshold and upsampling – shows very little improvement over doing just one or the other. Below are some classification metrics for each model specification.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mats |>\n  imap(\n    \\(x, y) summary(x) |>\n      select(-.estimator) |>\n      set_names(\"metric\", str_replace(y, \"_\", \" \") |> str_to_title())\n  ) |>\n  reduce(left_join, by = \"metric\") |>\n  kable(booktabs = TRUE) |>\n  kable_styling(\"striped\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> metric </th>\n   <th style=\"text-align:right;\"> Base Standard </th>\n   <th style=\"text-align:right;\"> Base Optimal </th>\n   <th style=\"text-align:right;\"> Upsample Standard </th>\n   <th style=\"text-align:right;\"> Upsample Optimal </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> accuracy </td>\n   <td style=\"text-align:right;\"> 0.9436105 </td>\n   <td style=\"text-align:right;\"> 0.7083164 </td>\n   <td style=\"text-align:right;\"> 0.6908722 </td>\n   <td style=\"text-align:right;\"> 0.7452333 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> kap </td>\n   <td style=\"text-align:right;\"> 0.0103118 </td>\n   <td style=\"text-align:right;\"> 0.1137115 </td>\n   <td style=\"text-align:right;\"> 0.1086066 </td>\n   <td style=\"text-align:right;\"> 0.1316040 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> sens </td>\n   <td style=\"text-align:right;\"> 0.0073529 </td>\n   <td style=\"text-align:right;\"> 0.6470588 </td>\n   <td style=\"text-align:right;\"> 0.6691176 </td>\n   <td style=\"text-align:right;\"> 0.6176471 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> spec </td>\n   <td style=\"text-align:right;\"> 0.9982825 </td>\n   <td style=\"text-align:right;\"> 0.7118935 </td>\n   <td style=\"text-align:right;\"> 0.6921426 </td>\n   <td style=\"text-align:right;\"> 0.7526836 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ppv </td>\n   <td style=\"text-align:right;\"> 0.2000000 </td>\n   <td style=\"text-align:right;\"> 0.1159420 </td>\n   <td style=\"text-align:right;\"> 0.1126238 </td>\n   <td style=\"text-align:right;\"> 0.1272727 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> npv </td>\n   <td style=\"text-align:right;\"> 0.9451220 </td>\n   <td style=\"text-align:right;\"> 0.9718640 </td>\n   <td style=\"text-align:right;\"> 0.9728425 </td>\n   <td style=\"text-align:right;\"> 0.9711911 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> mcc </td>\n   <td style=\"text-align:right;\"> 0.0285977 </td>\n   <td style=\"text-align:right;\"> 0.1775336 </td>\n   <td style=\"text-align:right;\"> 0.1757144 </td>\n   <td style=\"text-align:right;\"> 0.1909560 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> j_index </td>\n   <td style=\"text-align:right;\"> 0.0056355 </td>\n   <td style=\"text-align:right;\"> 0.3589523 </td>\n   <td style=\"text-align:right;\"> 0.3612602 </td>\n   <td style=\"text-align:right;\"> 0.3703306 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> bal_accuracy </td>\n   <td style=\"text-align:right;\"> 0.5028177 </td>\n   <td style=\"text-align:right;\"> 0.6794762 </td>\n   <td style=\"text-align:right;\"> 0.6806301 </td>\n   <td style=\"text-align:right;\"> 0.6851653 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> detection_prevalence </td>\n   <td style=\"text-align:right;\"> 0.0020284 </td>\n   <td style=\"text-align:right;\"> 0.3079108 </td>\n   <td style=\"text-align:right;\"> 0.3277890 </td>\n   <td style=\"text-align:right;\"> 0.2677485 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> precision </td>\n   <td style=\"text-align:right;\"> 0.2000000 </td>\n   <td style=\"text-align:right;\"> 0.1159420 </td>\n   <td style=\"text-align:right;\"> 0.1126238 </td>\n   <td style=\"text-align:right;\"> 0.1272727 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> recall </td>\n   <td style=\"text-align:right;\"> 0.0073529 </td>\n   <td style=\"text-align:right;\"> 0.6470588 </td>\n   <td style=\"text-align:right;\"> 0.6691176 </td>\n   <td style=\"text-align:right;\"> 0.6176471 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> f_meas </td>\n   <td style=\"text-align:right;\"> 0.0141844 </td>\n   <td style=\"text-align:right;\"> 0.1966480 </td>\n   <td style=\"text-align:right;\"> 0.1927966 </td>\n   <td style=\"text-align:right;\"> 0.2110553 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe first thing that one might notices from the above is how high the accuracy is for the base model. It's important to remember, though, that when a data set is so unbalanced all a model has to do is predict the majority class all the time and it will have high accuracy. That model, though, has a very low sensitivity and high specificity. The high specificity, again, is due to the fact that the actual data has an overwhelming proportion of the majority class. Looking across at the metrics for the other models one sees drops in both accuracy and specificity, but much more significant increases in sensitivity from the base model. It's also useful to note that other metrics like balanced accuracy, kappa, mcc, and f_measure also improve. This is further evidence of the effect that the imbalance in classes has on the base model.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}