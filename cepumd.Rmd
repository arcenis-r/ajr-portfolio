

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "cepumd_files/figure-html/cepumd-",
  out.width = "100%",
  message = FALSE,
  warning = FALSE
)
```


## Introduction
The purpose of \code[cepumd]  is to make working with Consumer Expenditure Surveys (CE) Public-Use Microdata (PUMD) easier toward calculating mean, weighted, annual expenditures (henceforth "mean expenditures"). The challenges \code[cepumd] seeks to address deal primarily with pulling together the necessary data toward this end. Some of the overarching ideas underlying the package are as follows:

* Use a [Tidyverse](https://www.tidyverse.org/) framework for most operations and be (hopefully) generally [Tidyverse](https://www.tidyverse.org/) friendly
* The best practice is to put all required data in one place, which `cepumd` does by default
* Balance the effort to make the end user's experience with CE PUMD easier while being flexible enough to allow that user to perform any analysis with the data they wish
* Only designed to help users calculate mean **expenditures** on and of the consumer unit (CU), i.e., not income, not assets, not liabilities, not gifts, at least for now

## History of CE
First a little history...
The first Consumer Expenditure Survey happened in 1888 (https://www.bls.gov/opub/hom/cex/history.htm), it was first used to revise CPI weights in 1972-1973, and it has been collected on a monthly basis since 1979. For a little bit more information on the detail on the history of the CE, check out the slide deck of a presentation delivered by Steve Henderson (former Chief of the Branch of Information and Analysis) and Adam Safir (current Division Chief of CE) called [130 Years of theConsumer Expenditure Surveys (CE): 1888 - 2018](https://www.bls.gov/cex/ce-130-presentation-safir-henderson.pdf)


## Overview of CE and CE PUMD

From the CE home page:

> "The Consumer Expenditure Surveys (CE) program provides data on expenditures, income, and demographic characteristics of consumers in the United States. The CE program provides these data in tables, LABSTAT database, news releases, reports, and public use microdata files.

> CE data are collected by the Census Bureau for BLS in two surveys, the Interview Survey for major and/or recurring items and the Diary Survey for more minor or frequently purchased items. CE data are primarily used to revise the relative importance of goods and services in the market basket of the Consumer Price Index. The CE is the only Federal household survey to provide information on the complete range of consumers' expenditures and incomes. Here is an overview of the CE program and its methods."

Some important things to note are that expenditure data are collected through two different survey instruments (Diary and Interview), expenditure categories are organized hierarchichally, and data are stored across thousands of files to which the CE provides access through their website. Also, given the length of the program, it would be difficult to harmonize data across all those years and files, so there are some inconsistencies in the way data are stored, which \code[cepumd] seeks to address (more on this further down).

Please visit the following pages to learn more about the CE program overall and CE PUMD more specifically.

* CE homepage: (https://www.bls.gov/cex/)
* CE PUMD page: (https://www.bls.gov/cex/pumd.htm)
* CE PUMD Getting Started Guide: https://www.bls.gov/cex/pumd-getting-started-guide.htm
* CE Dictionary for Interview and Diary Surveys (XLSX download) (https://www.bls.gov/cex/pumd/ce_pumd_interview_diary_dictionary.xlsx)
* CE PUMD published tables: (https://www.bls.gov/cex/tables.htm)
* CE PUMD Handbook of Methods: https://www.bls.gov/opub/hom/cex/
* CE Frequently Asked Questions: https://www.bls.gov/cex/csxfaqs.htm

## Challenges addressed by {cepumd}
\code[cepumd] seeks to address challenges in three categories: data gathering/organization; managing data inconsistencies; and calculating weighted, annual metrics.

* **Data gathering/organization**
    + Facilitate data and metadata downloads with `ce_download()`, `store_ce_hg()`, and `store_ce_dict()`
    + Convert hierarchical grouping (HG) files to data tables using `ce_hg()`
    + Help the user identify the Universal Classification Codes (UCCs) related to their analysis using a combination of `ce_hg()` and `ce_uccs()`
    + Combine all required files and variables using `ce_prepdata()`

* **Managing data inconsistencies**
    + Provide the ability to recode variable categories using the CE Dictionary for Interview and Diary Surveys
    + Resolve some inconsistencies such as differences code definitions between the Interview and Diary (check the definitions of the "FAM_TYPE" variable categories in 2015 for an example)
    + Provide useful errors or warnings when there are multiple categories of something the user is trying to access, e.g., some titles in the hierarchical grouping files ("stub" or "HG" files) repeat and requires more careful selection of UCCs

* **Calculating weighted, annual metrics**
    + Calculate a mean expenditure with `ce_mean()` or expenditure quantile with `ce_quantile()`
    + Account for the factor (annual vs. quarterly expenditure)
    + Account for the "months in scope" of a given consumer unit (CU)
    + Annualize expenditures for either Diary or Interview expenditures
    + Integrate Interview and Diary data as necessary
    
Source code and other package information is available at https://github.com/arcenis-r/cepumd

## Cautions and Recommendations
* Estimates produced using PUMD, which is topcoded by the CE and has some records suppressed to protect respondent confidentiality, will not match the published estimates released by the CE in most cases. The CE's published estimates are based on confidential data that are not topcoded nor have records suppressed. You can learn more at [CE Protection of Respondent Confidentiality](https://www.bls.gov/cex/pumd_disclosure.htm).

* When calculating estimates for sub-samples or crosss-sections of data it is best to stick to the combinations of variables that the CE uses in it's publication tables, e.g., income, geography, composition of CU, size of CU. This is because CE data are collected using a [stratified, random sample](https://www.bls.gov/opub/hom/cex/design.htm) (a.k.a., "representative sample") and only analyses conducted using the stratification variables are statistically valid. Using other variables can be helpful to understand spending across different groups, but unweighted estimates are likely more useful for this. `cepumd` currently does not support unweighted estimates, but data for such an analysis can be prepared using `ce_prepdata()`.

* Quantiles should only be generated using data from 1 survey instrument as the samples for the Interview and Diary are different.

* The variables EXPNMO and EXPNYR (Diary reference month and year variables respectively) are only available from 2012 forward and will have missing values in them for any years before 2012.

* Check the expenditure category in the appropriate HG file to ensure that it is the category for which you intend to generate an estimate.

* Store an HG object in the environment and call that directly in `ce_prepdata()`.

## Installation

You can install the development version of `cepumd` from [GitHub](https://github.com), but you'll first need the `devtools` package:

```{r, installation, eval=FALSE}
if (!"devtools" %in% installed.packages()[, "Package"]) {
  install.packages("devtools", dependencies = TRUE)
}

devtools::install_github("arcenis-r/cepumd")
```

## Key cepumd functions
* The workhorse of `cepumd` is `ce_prepdata()`. It merges the household characteristics file (FMLI/-D) with the corresponding expenditure tabulation file (MTBI/EXPD) for a specified year, adjusts weights for months-in-scope and the number of collection quarters, adjusts some cost values by their periodicity factor (some cost categories are represented as annual figures and others as quarterly). With the recent update it only requires the first 3 arguments to function: the year, the survey type, and one or more valid UCCs. `ce_prepdata()` now creates all of the other necessary objects within the function if not provided.

* There are three functions for downloading the data and necessary documentation:
    + `ce_download()` downloads zip files for a given year and survey instrument directly from the CE website
    + `store_ce_hg()` downloads the zip file containing all HG files maintained by the CE to the specified location.
    + `store_ce_dict()` downloades the CE PUMD dictionary from CE's website to the specified location.
    
* There are two functions for wrangling hierarchical grouping data into more useable formats:
    + `ce_hg()` pulls the requested type of HG file (Interview, Diary, or Integrated) for a specified year.
    + `ce_uccs()` filters the HG file for the specified expenditure category and returns either a data frame with only that section of the HG file or the Universal Classification Codes (UCCs) that make up that expenditure category.

* There are two functions that the user can use to calculate CE summary statistics:
    + `ce_mean()` calculates a mean expenditure, standard error of the mean, coefficient of variation, and an aggregate expenditure.
    + `ce_quantiles()` calculates weighted expenditure quantiles. It is important to note that calculating medians for integrated expenditures is not recommended because the calculation involves using weights from both the Diary and Survey instruments.

* There are two utility functions to make the workflow a bit easier:
    + `ce_pumd_years()` scrapes the main PUMD website to get a vector of years for which PUMD are available. The vector is limited to the years for which there are also HG files available.
    + `ce_cleanup()` deletes a file containing CE data that may only be necessary temporarily.

## Example workflows

The following are a few sample workflows that show how \code[cepumd] can be used. Before jumping into those I'll first install and load the necessary pacakges.

```{r, load_pkgs}

# Store a vector of names of additional packages to be used
pkgs <- c("tidyverse", "devtools", "readxl", "knitr")

# Install packages from CRAN
invisible(
  sapply(
    pkgs, function(x) if (!x %in% installed.packages()) install.packages(x)
  )
)

library(knitr)
library(readxl)
library(tidyverse)
library(cepumd)
```

### Simple workflow
The following is an example of how someone might go about using \code[cepumd] to calculate a 2021 annual, weighted estimate of mean expenditures on pets for all of the U.S. using CE integrated data without creating a separate directory for the data. This is just a quick and easy calculation.

```{r simple_wflow, cache=TRUE}
ce_prepdata(
  2021,
  integrated,
  uccs = ce_hg(2021, integrated) %>% ce_uccs("Pets")
) %>%
  ce_mean() %>%
  kable(booktabs = TRUE)
```

Yup... that's all it takes. I simply ran `ce_prepdata()` with the year, the survey type, and the uccs I needed and piped that directly into `ce_mean()`.

But where are all the files? Zip files, etc.? They're in my R session's temporary directory under a sub-directory named "ce-data".

```{r show-tempdir}
list.files(file.path(tempdir(), "ce-data"))
```

I'll go ahead and clean those files up really quickly.

```{r show-tempdir-2}
ce_cleanup()
list.files(file.path(tempdir(), "ce-data"))
```

### Slightly more advanced workflow
* Used Cars & Trucks 2021, Interview
* by Urbanicity
* Show stub file to highlight that expenditures only come from Interview
* Calculate median with the same `ce_prepdata()` object

In this example I'll calculate estimated annual expenditures on used cars and trucks by urbanicity also for 2021. Once the data are prepped with `ce_data()` I'll just nest the data by urbanicity and run `ce_means()` and `ce_quantiles()` on the nested datasets. 

First, I'll get the portion of the stub file pertaining to used cars and trucks to ensure that I get the correct UCCs.

```{r filter_stub}
stub_2021 <- ce_hg(2021, integrated)

stub_2021 %>%
  filter(str_detect(title, "[C|c]ars")) %>%
  kable(booktabs = TRUE)
```

It looks like the expenditure category I want is "Cars and trucks, used".

```{r stub_used_cars_trucks}
ce_uccs(stub_2021, "Cars and trucks, used", uccs_only = FALSE)
```

I notice from the "survey" column that all of the UCCs associated with used cars and trucks come from the Interview survey (as indicated by the "I"). So when I prepare my data I'll only need data from the Interview.

```{r cars_trucks_means, cache = TRUE}
cars_trucks <- ce_prepdata(
  2021,
  interview,
  uccs = ce_uccs(stub_2021, "Cars and trucks, used", uccs_only = TRUE),
  bls_urbn,
  recode_variables = TRUE,
  hg = stub_2021
)

cars_trucks %>%
  nest(data = -bls_urbn) %>%
  mutate(ce_mn_df = map(data, ce_mean)) %>% 
  select(-data) %>% 
  unnest(ce_mn_df) %>%
  kable(booktabs = TRUE)
```

Getting the annual, weighted estimate of the median would be just as easy. Since I'm using interview data only here, this would be a good example. I'll calculate the median and 99th percentiles for the overall sample rather than breaking it down by urbanicity.

```{r cars_trucks_median, cache = TRUE}
ce_quantiles(cars_trucks, probs = c(0.5, 0.99)) %>% kable(booktabs = TRUE)
```

```{r cars_trucks_cleanup}
ce_cleanup()
```


### Very advanced workflow

In this last example I'm going to assume very little knowledge about the CE. I'd like to compare mean annual expenditures on food away from home between 2010 and 2020 by household size and I want to convert expenditures to 2022 dollars using the CPI. Also, I'm going to set up a directory on my local machine to put all the data and metadata files into.

First, I'll set up that directory. I'll put the directory path in a variable called "food_away_dir" for simplicity.

```{r setup_dir}
food_away_dir <- file.path("..", "food-away")
dir.create(food_away_dir)
list.files(food_away_dir)
```

Next, I want to make sure that there are data for my years of interest.
```{r see_pumd_years}
ce_pumd_years()
```

Now I want to store the CE HG files and data dictionary.

```{r store_metadata}
store_ce_hg(food_away_dir)
store_ce_dict(food_away_dir)
```

Let's take a look at what the files are called.

```{r show_md_files}
list.files(food_away_dir)
```

I'm going to start by looking at the dictionary to ensure that the variables I want to subset by are there.

First I'll take a look at the sheets in the dictionary.

```{r show_dict_sheets}
ce_dict_file_path <- file.path(food_away_dir, "ce-dict.xlsx")
excel_sheets(ce_dict_file_path)
```

Now I'll see what variables contain anything about the number of household members. To do that I'll have to load the sheet from the dictionary containing the variable definitions. I also want to filter the variable data to only the FMLI where the "Last year" column is missing, i.e., the variable definition is still in use.
```{r cu_size_vars}
ce_variables <- read_excel(ce_dict_file_path, sheet = "Variables")

ce_variables %>%
  filter(
    str_detect(File, "FMLI"),
    str_detect(
      tolower(`Variable description`), "number of members"
    )
  ) %>%
  kable(booktabs = TRUE)
```

It looks like FAM_SIZE is the variable I want. I can see that this variable was used from 1980 through 1981 then was dropped and re-introduced in 1984 and has been in use since. So it looks like it's available for my 2 years of interest. Next I'll check whether the FAM_SIZE variable has any value codes associated with it. I'll have to pull in the "Codes" sheet. (Check your spelling here.)

```{r fam_size_codes}
ce_codes <- read_excel(ce_dict_file_path, sheet = "Codes ")

ce_codes %>%
  filter(File %in% "FMLI", Variable %in% "FAM_SIZE") %>%
  kable(booktabs = TRUE)
```

It looks like FAM_SIZE is not a coded variable (no observations in the "Codes" sheet), so it must be numeric.

Since I'm going to need 

Next I want to see what the 2010 HG file looks like for 2010 for expenditures on "food away from home". First I'll download both HG files (2010 and 2020), then I'll find the correct title in the 2010 HG file for my category.

```{r convert_hg_files}
hg_10 <- ce_hg(2010, integrated, food_away_dir)
hg_20 <- ce_hg(2020, integrated, food_away_dir)

hg_10 %>%
  filter(str_detect(title, "[F|f]ood [A|a]way")) %>%
  kable(booktabs = TRUE)

hg_20 %>%
  filter(str_detect(title, "[F|f]ood [A|a]way")) %>%
  kable(booktabs = TRUE)
```

It looks like the title is the same in both years, so I'll store the title in a variable to use downstream.

```{r store_title}
food_away_title <- hg_10 %>%
  filter(str_detect(title, "[F|f]ood [A|a]way")) %>%
  pull(title)
```

Now I'll use that title to get the UCCs and see the entire table with "food away from home" expenditures (only for 2010 as a sample).

```{r show_food_away_hg}
food_away_hg_10 <- ce_uccs(hg_10, food_away_title, uccs_only = FALSE)
food_away_hg_20 <- ce_uccs(hg_20, food_away_title, uccs_only = FALSE)

food_away_hg_10 %>% kable(booktabs = TRUE)
```

Next I'll use the dictionary to find the variable for household size. 

 With all that, I'm ready to prepare my data.
The first thing I'll need are the UCCs for each of the two years in my analysis.

```{r show_uccs_10}
food_away_uccs_10 <- ce_uccs(food_away_hg_10, food_away_title, uccs_only = TRUE)
food_away_uccs_20 <- ce_uccs(food_away_hg_20, food_away_title, uccs_only = TRUE)

food_away_uccs_10
```

```{r show_uccs_20}
food_away_uccs_20
```

The lists of UCCs look identical, but I'll keep both just to be cautious.

Next I'll prepare the 2010 data and get a summary of the FAM_SIZE variable since it is a continuous variable.

```{r prep_data_10}
food_away_data_10 <- ce_prepdata(
  2010,
  integrated,
  food_away_uccs_10,
  recode_variables = FALSE,
  ce_dir = food_away_dir,
  dict_path = "ce-dict.xslx",
  hg = food_away_hg_10,
  fam_size
)

summary(food_away_data_10$fam_size)
```

Since some households have as many as 14 people, I'll create a FAM_SIZE label with any number greater than 4 taking on the value "5+". Next, I'll prepare the 2020 data and rowbind it with the 2010 data as well as create the "fam_size_label" variable. I'll also take a look at just a snippet of the data.

```{r ex-3-get-wt-estimates}
food_away_data_20 <- ce_prepdata(
  2020,
  integrated,
  food_away_uccs_20,
  recode_variables = FALSE,
  ce_dir = food_away_dir,
  dict_path = "ce-dict.xslx",
  hg = food_away_hg_20,
  fam_size
)

food_away_comp_data <- food_away_data_10 %>%
  mutate(year = 2010) %>%
  bind_rows(food_away_data_20 %>% mutate(year = 2020)) %>%
  mutate(
    fam_size_label = if_else(fam_size > 4, "5+", as.character(fam_size)),
    year = factor(year)
  )

food_away_comp_data %>%
  select(year, newid, finlwt21, cost, ref_yr, ref_mo) %>%
  group_by(year) %>%
  slice_sample(n = 5) %>%
  ungroup() %>%
  kable(booktabs = TRUE)
```

Since I'm going to adjust for inflation, I also want to see the years in which expenditures were made just as an extra check.
























<!----------------------------------------------------------------------------->



















# cepumd

[cepumd](https://github.com/arcenis-r/cepumd) facilitates the calculation of Consumer Expenditure Survey (CE) annual, weighted, estimated mean expenditures using CE Public-Use Microdata (PUMD) by addressing some unique challenges that exist in working with CE PUMD. Some examples are:

* Downloading CE PUMD from within R
* Converting hierarchical grouping (HG) files to data tables
* Accounting for the factor (annual vs. quarterly expenditure)
* Accounting for the "months in scope" of a given consumer unit (CU)
* Annualizing expenditures for either Diary or Interview expenditures
* Integrating Interview and Diary data as necessary
* Calculating weighted CE quantiles

For more information on the CE, please visit https://www.bls.gov/cex/.

The workhorse of [cepumd](https://github.com/arcenis-r/cepumd) is `ce_prepdata()`. It merges the household characteristics file (FMLI/-D) with the corresponding expenditure tabulation file (MTBI/EXPD) for a specified year, adjusts weights for months-in-scope and the number of collection quarters, adjusts some cost values by their periodicity factor (some cost categories are represented as annual figures and others as quarterly). With the recent update it only requires the first 3 arguments to function: the year, the survey type, and one or more valid UCCs. `ce_prepdata()` now creates all of the other necessary objects within the function if not provided.

There are three other functions that help the user download and wrangle the data
and necessary documentation, such as the HG files:

* `ce_download()` downloads zip files for a given year and survey instrument directly from the CE website
* `ce_hg()` pulls the requested type of HG file (Interview, Diary, or Integrated) for a specified year.
* `ce_uccs()` filters the HG file for the specified expenditure category and returns either a data frame with only that section of the HG file or the Universal Classification Codes (UCCs) that make up that expenditure category.

There are two functions that the user can use to calculate CE summary statistics:

* `ce_mean()` calculates a mean expenditure, standard error of the mean, coefficient of variation, and an aggregate expenditure.
* `ce_quantiles()` calculates weighted expenditure quantiles. It is important to note that calculating medians for integrated expenditures is not recommended because the calculation involves using weights from both the Diary and Survey instruments.

There are two functions that allow the user to download metadata that can be useful in preparing data:

* `store_ce_hg()` downloads the zip file containing all HG files maintained by the CE to the specified location.
* `store_ce_dict()` downloades the CE PUMD dictionary from CE's website to the specified location.

Finally, there are two utility functions to make the workflow a bit easier:

* `ce_pumd_years()` scrapes the main PUMD website to get a vector of years for which PUMD are available. The vector is limited to the years for which there are also HG files available.
* `ce_cleanup()` deletes a file containing CE data that may only be necessary temporarily.

## Updates

Due to changes in the way CE PUMD stores their data starting with the 2020 PUMD
I revisited the way that the main functions of [cepumd](https://github.com/arcenis-r/cepumd) stores and accesses
files locally. The most important change is that all files are expected to be
stored in one directory. To that end, if a directory is not provided to the
functions, one will be created in the local temporary directory. Also, most of the functions used for downloading or preparing data now take a `ce_dir` argument to help with this. The major benefit of this change is that all files are now in one place and can be quickly cleaned up. An ancillary benefit is that it can make the workflow much simpler for quickly getting an expenditure estimate.

## Word of caution

The CE PUMD is a wonderfully rich data set that can provide all manner of insight into how Americans spend money, but there are some strict limitations with respect to the types of analyses that can be done with this data. One of the more common violations of sound statistical methodology that I'm aware of is trying to calculate annual expenditure means for subgroups that are defined by variables that were not used in the sample design, i.e., the weights. The best advice that I can give is that users of this package (or anyone using CE PUMD to estimate annual expenditures) is to stick to the same classifications used by the CE in their [published tables](https://www.bls.gov/cex/tables.htm). I'll also point users of this package to the [CE PUMD Getting Started Guide](https://www.bls.gov/cex/tables.htm).

## Installation

You can install the development version of [cepumd](https://github.com/arcenis-r/cepumd) from [GitHub](https://github.com) with:

```{r, get_cepumd, eval=FALSE}
devtools::install_github("arcenis-r/cepumd")
```

## Prep work

The first step is to load the necessary packages into the environment.

```{r, load_pkgs}

# Store a vector of names of additional packages to be used
pkgs <- c("tidyverse", "devtools", "rlang", "readxl", "knitr")

# Install packages from CRAN
invisible(
  sapply(
    pkgs, function(x) if (!x %in% installed.packages()) install.packages(x)
  )
)

library(knitr)
library(readxl)
library(tidyverse)
library(cepumd)
```

## Example Workflow 1
The following is an example of how someone might go about using [cepumd](https://github.com/arcenis-r/cepumd) to calculate a 2021 annual, weighted estimate of mean expenditures on pets for all of the U.S. using CE integrated data without creating a separate directory for the data. This is just a quick and easy calculation.

```{r ex-1-mean}
ex1 <- ce_prepdata(
  2021,
  integrated,
  uccs = ce_hg(2021, integrated) %>% ce_uccs("Pets")
) %>%
  ce_mean() %>%
  kable(booktabs = TRUE)
```

Yup... that's all it takes. I simply ran `ce_prepdata()` with the year, the survey type, and the uccs I needed and piped that directly into `ce_mean()`.

But where are all the files? Zip files, etc.? They're in my R session's temporary directory under a sub-directory named "ce-data".

```{r show-tempdir}
list.files(file.path(tempdir(), "ce-data"))
```

I'll go ahead and clean those files up really quickly.

```{r show-tempdir-2}
ce_cleanup()
list.files(file.path(tempdir(), "ce-data"))
```

This works because if `ce_cleanup()` isn't given a directory to clean up it looks R's temporary directory for a sub-directory called "ce-data". If it doesn't find that AND it gets no other directory to look in, it returns a message saying there's nothing to clean up.

## Example Workflow 2

In this example I'll calculate estimated annual expenditures on used cars and trucks by urbanicity also for 2021. I already know that the UCC for used cars is 460110 for and the UCC for used trucks is 460901. I also know that all of these data are collected through the interview survey, so I'll use the interview data only. Now here's a wrinkle: because I'm going to add a grouping variable ("BLS_URBN"), I do need to specify all of the other named arguments. This is due to the developer's inexperience with using missing arguments (if anyone can help me out with this, I would greatly appreciate it). Once the data are prepped with `ce_data()` I'll just nest the data by urbanicity and run `ce_means()` on the nested datasets.

```{r ex-2}
ce_prepdata(
  2021,
  interview,
  uccs = c("460110", "460901"),
  recode_variables = TRUE,
  ce_dir = NULL,
  dict_path = NULL,
  int_zp = NULL,
  dia_zp = NULL,
  hg = NULL,
  bls_urbn
) %>%
  nest(data = -bls_urbn) %>%
  mutate(ce_mn_df = map(data, ce_mean)) %>% 
  select(-data) %>% 
  unnest(ce_mn_df) %>%
  kable(booktabs = TRUE)

ce_cleanup()
```

There have been 2 examples of calculated a mean estimate in the last 2 examples, but getting the annual, weighted estimate of the median would be just as easy. Since I'm using interview data only here, this would be a good example. I'll calculate the overall median rather than breaking it down by urbanicity.

```{r ex-2-median}
ce_prepdata(
  2021,
  interview,
  uccs = c("460110", "460901")
) %>%
  ce_quantiles() %>%
  kable(booktabs = TRUE)

ce_cleanup()
```

## Example Workflow 3

In this last example I'm going to assume very little knowledge about the CE. I'd like to compare mean annual expenditures on food away from home between 2010 and 2020 by household size. Also, I'm going to set up a directory on my local machine to put all the data and metadata files into.

First, I'll set up that directory. I'll put the directory path in a variable called "food_away_dir" for simplicity.
```{r ex-3-setup-dir}
food_away_dir <- file.path("..", "food-away")
dir.create(food_away_dir)
list.files(food_away_dir)
```

Next, I want to make sure that there are data for my years of interest.
```{r ex-3-pumd-years}
ce_pumd_years()
```

Now I want to store the CE HG files and data dictionary.

```{r ex-3-store-metadata}
store_ce_hg(food_away_dir)
store_ce_dict(food_away_dir)
```

Let's take a look at what the files are called.

```{r ex-3-show-md-files}
list.files(food_away_dir)
```

Next I want to see what the 2010 HG file looks like for 2010 for expenditures on "food away from home". First I'll download both HG files (2010 and 2020), then I'll find the correct title in the 2010 HG file for my category. I'm going to cheat a little by magically knowing that the title is the same in 2020.

```{r ex3-store-hg}
hg_10 <- ce_hg(2010, integrated, food_away_dir)
hg_20 <- ce_hg(2020, integrated, food_away_dir)
```

```{r ex-3-get-hg-title}
food_away_title <- hg_10 %>%
  filter(str_detect(title, "[F|f]ood [A|a]way")) %>%
  pull(title)

food_away_title
```

Now I'll use that title to get the UCCs and see the entire table with "food away from home" expenditures for 2010

```{r ex-3-show-hg-10}
food_away_hg_10 <- ce_uccs(hg_10, food_away_title, uccs_only = FALSE)
food_away_hg_20 <- ce_uccs(hg_20, food_away_title, uccs_only = FALSE)

food_away_hg_10 %>% kable(booktabs = TRUE)
```

Next I'll use the dictionary to find the variable for household size. First I'll take a look at the sheets in the dictionary.

```{r ex-3-ce-dict-sheets}
ce_dict_file_path <- file.path(food_away_dir, "ce-dict.xlsx")
excel_sheets(ce_dict_file_path)
```

Now I'll see what variables contain anything about the number of household members. To do that I'll have to load the sheet from the dictionary containing the variable definitions
```{r ex-3-cu-size-vars}
ce_variables <- read_excel(ce_dict_file_path, sheet = "Variables")

ce_variables %>%
  filter(
    str_detect(File, "FMLI"), is.na(`Last year`),
    str_detect(
      tolower(`Variable description`), "number of members"
    )
  ) %>%
  kable(booktabs = TRUE)
```

It looks like FAM_SIZE is the variable I want. Next I'll check whether the FAM_SIZE variable has any value codes associated with it. I'll have to pull in the "Codes" sheet. (Check your spelling here.)

```{r ex-3-fam-size-codes}
ce_codes <- read_excel(ce_dict_file_path, sheet = "Codes ")

ce_codes %>%
  filter(File %in% "FMLI", Variable %in% "FAM_SIZE") %>%
  kable(booktabs = TRUE)
```

It looks like FAM_SIZE is not a coded variable (no observations in the "Codes" sheet), so it must be numeric. With all that, I'm ready to prepare my data.
The first thing I'll need are the UCCs for each of the two years in my analysis.

```{r ex-3-get-uccs}
food_away_uccs_10 <- ce_uccs(food_away_hg_10, food_away_title, uccs_only = TRUE)
food_away_uccs_20 <- ce_uccs(food_away_hg_20, food_away_title, uccs_only = TRUE)

food_away_uccs_10
```

Next I'll prepare the 2010 data and get a summary of the FAM_SIZE variable since it is a continuous variable.

```{r ex-3-prep-data-10}
food_away_data_10 <- ce_prepdata(
  2010,
  integrated,
  food_away_uccs_10,
  recode_variables = FALSE,
  ce_dir = food_away_dir,
  dict_path = "ce-dict.xslx",
  int_zp = NULL,
  dia_zp = NULL,
  hg = food_away_hg_10,
  fam_size
)

summary(food_away_data_10$fam_size)
```

Since some households have as many as 14 people, I'll create a FAM_SIZE label with any number greater than 4 taking on the value "5+" later on. Next, I'll prepare the 2020 data and rowbind it with the 2010 data as well as create the "fam_size_label" variable. I'll also go ahead and get weighted, annual estimated means in one go.

```{r ex-3-get-wt-estimates}
food_away_data_20 <- ce_prepdata(
  2020,
  integrated,
  food_away_uccs_20,
  recode_variables = FALSE,
  ce_dir = food_away_dir,
  dict_path = "ce-dict.xslx",
  int_zp = NULL,
  dia_zp = NULL,
  hg = food_away_hg_20,
  fam_size
)

food_away_comp_data <- food_away_data_10 %>%
  mutate(year = 2010) %>%
  bind_rows(food_away_data_20 %>% mutate(year = 2020)) %>%
  mutate(
    fam_size_label = if_else(fam_size > 4, "5+", as.character(fam_size)),
    year = factor(year)
  ) %>%
  nest(data = -c(year, fam_size_label)) %>%
  mutate(ce_mn_df = map(data, ce_mean)) %>% 
  ungroup() %>%
  select(-data) %>% 
  unnest(ce_mn_df) %>%
  mutate(
    lower = mean_exp - (qnorm(0.975) * se),
    upper = mean_exp + (qnorm(0.975) * se)
  )

food_away_comp_data %>% kable(booktabs = TRUE)
```

Plotting these data would be pretty straightforward, as well.

```{r ex-3-group-means-plot}
food_away_comp_data %>%
  ggplot(aes(x = fam_size_label, y = mean_exp, fill = year, group = year)) +
  geom_bar(stat = "identity", position = "dodge", width = 0.8) +
  geom_errorbar(
    aes(ymin = lower, ymax = upper),
    width = 0.4,
    position = position_dodge(0.75)
  ) +
  scale_fill_manual(values = c("red", "blue")) +
  labs(
    title =
      "Estimated annual mean food away from home expenditures by CU size",
    x = "CU size",
    y = "Estimated, weighted, annual mean expenditure ($)",
    fill = "Year"
  ) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom")
```

And that's it. This analysis would give me the weighted, annual estimated mean expenditures on food away from home by family size in 2010 and 2020. 

Now for a very quick analysis, I'll generate a plot of the expenditures at each weighted, annual, estimated quantile (from 0.01 through 0.99, by 0.01) for the same years, but only using Diary data, since most of the UCCs (16 out of 21) in the "food away from home" category come from the Diary. This analysis involves re-running everything in one call to `map2_df()` giving it the 2 years of interest and the Diary zip files as arguments then performing all of the steps for aggregation by year. 

```{r ex-3-group-qtile-plot}
food_away_comp_quantiles <- map2_df(
  c(2010, 2020),
  c("diary10.zip", "diary20.zip"),
  ~ ce_prepdata(
    .x,
    diary,
    ce_hg(.x, diary) %>% ce_uccs(food_away_title),
    recode_variables = FALSE,
    ce_dir = food_away_dir,
    dict_path = "ce-dict.xslx",
    int_zp = NULL,
    dia_zp = .y,
    hg = ce_hg(.x, diary)
  ) %>%
    mutate(year = .x)
) %>%
  mutate(year = factor(year)) %>%
  nest(data = -year) %>%
  mutate(
    fa_qtile = map(data, ce_quantiles, probs = c(seq(0, 0.95, by = 0.05), 0.99))
  ) %>%
  select(-data) %>%
  unnest(fa_qtile) %>%
  mutate(probs = parse_number(probs) / 100)

food_away_comp_quantiles %>%
  ggplot(aes(x = probs, y = quantile, group = year, color = year)) +
  geom_line() +
  scale_color_manual(values = c("red", "blue")) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title =
      "Estimated, annual food away from home expenditure quantiles",
    x = "Quantile",
    y = "Estimated, weighted, annual expenditure ($)",
    color = "Year"
  ) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom")

ce_cleanup(food_away_dir)
```

Interestingly the expenditures don't appear to have changed much between 2010 and 2020 across the different quantiles. There are a lot of 0-value reported expenditures, though, in the CE on food away from home. Unfortunately, I can't perform an analysis using only respondents that did have expenditures in this category, i.e., dropping the 0's, because whether someone had an expenditure on food away from home is not one of the variables used for generating the survey weights. In other words, the analysis can be done, but it would not be statistically valid and I definitely wouldn't be able to make any inferences from it. This is my cautionary note to anyone using this package who might use it in a way that is not statistically sound. Please visit the [CE's website](https://www.bls.gov/cex/) and read the [CE PUMD Getting Started Guide](https://www.bls.gov/cex/tables.htm) for more information.
