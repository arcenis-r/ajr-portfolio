---
title: "Forecasting with {modeltime} - Part IV"
subtitle: "Tuning many global models"
author: "Arcenis Rojas"
date: "2/20/2024"
bibliography: references.bib
fig-height: 8
fig-width: 8
categories:
  - Time Series
  - Forecasting
  - Machine Learning
---

## Review

Up to this point in the project I acquired data in [Forecasting with {modeltime} - Part I](../20240212-forecasting-with-modeltime-part-1/forecasting-with-modeltime-part-1.qmd), performed some standard time series analysis in [Forecasting with {modeltime} - Part II](../20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.qmd), and explored the two types of modeling processes – global and iterative – for multiple time series in [Forecasting with {modeltime} - Part III](../20240216-forecasting-with-modeltime-part-3/forecasting-with-modeltime-part-3.qmd). In this post I'm going to tune multiple models using a global process to select a small number of them that I will run iteratively in the final post. I'll incorporate some of the other variables in the data set that I gathered in the the first post to forecast the Case-Shiller Home Price Index (HPI). Since I'll be creating variations of model specifications, I'm also going to use functions from the Tidymodels [workflowsets](https://workflowsets.tidymodels.org/) [@workflowsets] package to run the models. As before I'll be using [modeltime](https://business-science.github.io/modeltime/index.html) [@modeltime], [timetk](https://business-science.github.io/timetk/) [@timetk], [tidymodels](https://www.tidymodels.org/) [@tidymodels], [tidyverse](https://www.tidyverse.org/) [@tidyverse], and [gt](#0) [@gt]. Before jumping in, below I show a glimpse of the data that I'll be using.

```{r}
#| label: load-packages

library(tidyverse)
library(timetk)
library(modeltime)
library(tidymodels)
library(gt)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("lag", "dplyr")
tidymodels_prefer()

```

```{r}
#| label: import-data
#| include: false

econ_data <- read_rds("../_common-resources/econ-data.Rds")

source("../_common-resources/gt-bold-head.R")

```

```{r}
#| label: glimpse-data

skimr::skim(econ_data)

```

## Splitting the data

The data split object for a global modeling process is different than that for an iterative modeling process. For the global modeling process I'll use `timetk::time_series_split()` . Below is a look at how the data are split.

```{r}
#| label: split-data

econ_splits <- econ_data |>
  time_series_split(
    assess = "2 year",
    cumulative = TRUE,
    date_var = date
  )

econ_splits

```

## Choosing algorithms

As with selecting the cities in the analysis I'm interested in getting a little variation in the algorithms to see how they perform and I want to vary the parameters a bit. To get something of a traditional flavor I'm going to run ARIMA models with three different model formulas:

1\) hpi \~ date

2\) hpi \~ date + unemployment

3\) hpi \~ date + unemployment + demographic variables

I'll also have four specifications for the algorithms. One will be the default auto-ARIMA and the other three will be auto-ARIMA specifications with different season lengths. This will give me twelve different ARIMA models (3 recipes x 4 algorithm specifications).

In much the same way I will have different sets of recipes and model specifications for the following:

-   an exponential smoothing algorithm (including a Holt-Winters specification) to allow for weighting the periods exponentially

-   an STLM algorithm using an ARIMA engine (STLM: **S**easonal **T**rend Decomposition using **L**oess with **M**ultiple seasonal periods)

-   A feed-forward, auto-regressive neural-net algorithm because these tend to have high predictive ability

Some of these algorithms allow for covariates and others (like exponential smoothing) do not. In the end I will have 90 model specifications to test. In order to do speed up the processing I'll employ [modeltime](https://business-science.github.io/modeltime/index.html)'s parallel processing feature.

## Building workflow objects

Here I'll write the three recipes for the ARIMA models and one for the auto-regressive neural-net model. The exponential smoothing and STLM models will both use the first of the three ARIMA recipes as neither of these algorithms can handle covariates.

You might notice in the code below that I exclude some of the demographic variables like "educ_hs_less" for the third ARIMA recipe. I exclude these variables because I found that including all of the variables for a demographic variable group causes the functions to error out much in the same way that including all of the categories of a factor variable would. Initially I did not expect that I would need contrasts considering that the variable values are ratios, but it seems that the fact that the sum of the ratios for a given variable group, e.g., education, is 1 for each row and for each variable group creates a need for contrasts in the regression (this is my guess).

### Writing recipes

::: {.callout-note title="Code Explanations"}
Some of the code chunks below contain explanations. Lines in explanations will be marked with encircled numbers. Just hover over those numbers to see the corresponding explanation.
:::

```{r}
#| label: write-recs

arima_rec1 <- recipe(hpi ~ date, data = training(econ_splits)) # <1>

arima_rec2 <- recipe(                                          # <2>
  hpi ~ date + unemp_rate,
  data = training(econ_splits)
) |>
  step_center(all_numeric_predictors()) |>                     # <3>
  step_scale(all_numeric_predictors()) |>                      # <4>
  step_lag(unemp_rate, lag = c(1, 3, 6))                       # <5>

rec3_dep_vars <- econ_data |>                                  # <6>
  select(date, unemp_rate:population) |>
  select(!c(educ_hs_less, status_married, age_36_65)) |>
  names() |>
  str_flatten(collapse = " + ")

arima_rec3 <- recipe(
  formula(str_c("hpi", rec3_dep_vars, sep = " ~ ")),
  data = training(econ_splits)
) |>
  step_center(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors()) |>
  step_lag(unemp_rate, population, lag = c(1, 3, 6))

nnet_rec <- recipe(
  hpi ~ .,
  data = training(econ_splits)
) |>
  update_role(city, new_role = "ID") |>                         # <7>
  step_center(all_numeric_predictors()) |>
  step_scale(all_numeric_predictors()) |>
  step_timeseries_signature(date) |>                            # <8>
  step_nzv(all_predictors())                                    # <9>

```

1.  Write a base recipe to regress HPI on the date variable
2.  Write a recipe formula that adds unemployment as an explanatory variable
3.  Mean-center all numeric predictors (only "unemployment rate" here)
4.  Scale all numeric predictors to have a mean of "0" and a standard error of 1
5.  Create lag variables for "unemployment rate" and "population"
6.  Get the names of all the economic and demographic variables from the data set and concatenate them into a formula string to make the right side of a regression formula, i.e., "var1 + var2 + var3..."
7.  Because the "outcome \~ ." notation is being used, the "city" variable is included in the ".", meaning it will be treated as an explanatory variable. This step changes the role of that variable from "predictor" to an ID variable, which modeling workflows will exclude from sets of predictors.
8.  Add a set of variables related to the date such as day of the week of the date, day of the month, number of the week of the year, etc.
9.  Remove any variables with zero or near-zero variance. An example of this would be a variable indicating the day of the month. Since this is monthly data, all values would be "1", i.e., the first day of the month.

Here is a reminder of what a recipe looks like. This is the neural-net recipe.

```{r}
#| label: show-rec

nnet_rec

```

And this is the data set that gets created by that recipe.

```{r}
#| label: show-rec-juiced

nnet_rec |> prep() |> juice() |> slice(1:5) |> gt() |> gt_bold_head()

```

### Specifying models

In this next step I'll specify the model objects for each algorithm along with the search grids for their various hyperparameters. I chose to keep only some of the combinations of the exponential smoothing specifications to limit the number of models being run that are likely to fail.

```{r}
#| label: specify-models

arima_default <- arima_reg() |> set_engine("auto_arima")                        # <1>

arima_spec_grid <- tibble(seasonal_period = seq(9, 18, by = 3)) |>              # <2>
  create_model_grid(                                                            # <3>
    f_model_spec = arima_reg,
    engine_name = "auto_arima",
    mode = "regression"
  )

ets_default <- exp_smoothing() |> set_engine("ets")

set.seed(40)                                                                    # <4>
alpha_vals <- runif(5, 0.01, 0.5) *
  sample(c(1, 0.1, 0.01, .001), 5, replace = TRUE)
beta_vals <- runif(5, 0.01, 0.2) * sample(c(1, 0.1, 0.01), 5, replace = TRUE)
gamma_vals <- map_dbl(
  alpha_vals,
  \(x) runif(1, 0, 1 - x)
)

ets_spec_grid <- expand_grid(                                                   # <5>
  smooth_level = alpha_vals[-2],
  smooth_seasonal = gamma_vals[2:3],
  seasonal_period = seq(9, 18, by = 3)
) |>
  create_model_grid(
    f_model_spec = exp_smoothing,
    engine_name = "ets",
    mode = "regression",
    error = "auto",
    trend = "auto",
    season = "auto",
    smooth_trend = beta_vals[3]
  )

stlm_spec_grid <- expand_grid(
  seasonal_period_2 = c(NULL, seq(6, 18, by = 3))
) |>
  create_model_grid(
    f_model_spec = seasonal_reg,
    engine_name = "stlm_arima",
    mode = "regression",
    seasonal_period_1 = 12
  )

nnet_default <- nnetar_reg() |> set_engine("nnetar")

nnet_spec_grid <- expand_grid(
  non_seasonal_ar = 1:3,
  hidden_units = 3:5,
  num_networks = c(20, 25),
  penalty = c(0.2, 0.25)
) |>
  create_model_grid(
    f_model_spec = nnetar_reg,
    engine_name = "nnetar",
    mode = "regression",
    seasonal_period = 12,
    epochs = 100,
    seasonal_ar = 1
  )

```

1.  Specify an ARIMA regression model using the "auto_arima" engine with all default arguments
2.  Create a 1-column `tibble` (data frame) with the `seasonal_period` argument set to vary as 9, 12, 15, and 18 (months).
3.  `create_model_grid()` allows one to set static arguments to combine them with the arguments that vary. This is meant to serve a similar purpose as `tune::tune_grid()` from Tidymodels.
4.  Set a random seed for reproducibility and generate vectors for the `smooth_level`, `smooth_trend`, and `smooth_seasonal` arguments. In `forecast::ets()` these are `alpha`, `beta`, and `gamma` respectively.
5.  Use `tidyr::expand_grid()` to get all combinations of the included vectors.

Below is what a model grid looks like.

```{r}
#| label: show-mod-grid

ets_spec_grid |> slice(1:5)
```

This is a glimpse of a model specification.

```{r}
#| label: show-mod-spec

ets_spec_grid |> slice(1) |> pluck(".models", 1)
```

### Building workflowsets

A `workflowset` is, you guessed it, a set of workflows. Mechanically it is a very convenient wrapper that can put together different combinations of recipes (pre-processors) and models. I don't want to mix all of my recipes with all of my models, however. So, in this step I'll make sets of individuals that combine the appropriate sets of recipes and models then I'll bind all of them together at the end.

```{r}
#| label: make-workflowsets

arima_wfset <- workflow_set(                # <1>
  preproc = list(                           # <2>
    base_rec = arima_rec1,
    econ_rec = arima_rec2,
    demog_rec = arima_rec3
  ),
  models = c(                               # <3>
    arima_default = list(arima_default),
    arima_spec = arima_spec_grid$.models
  ),
  cross = TRUE                              # <4>
)

ets_wfset <- workflow_set(
  preproc = list(base_rec = arima_rec1),
  models = c(ets_default = list(ets_default), ets_spec = ets_spec_grid$.models),
  cross = TRUE
)

stlm_wfset <- workflow_set(
  preproc = list(base_rec = arima_rec1),
  models = c(stlm_spec = stlm_spec_grid$.models),
  cross = TRUE
)

nnet_wfset <- workflow_set(
  preproc = list(nnet_rec = nnet_rec),
  models = c(
    nnet_default = list(nnet_default),
    nnet_spec = nnet_spec_grid$.models
  ),
  cross = TRUE
)

```

1.  Call `workflowsets::workflowset()`
2.  Provide a named list of recipes to use in the `workflowset`; naming the elements at this stage makes it easier to match up the elements of the final table containing the trained models with the original recipes and model specifications
3.  Provide a named list of model specifications by pulling the ".models" column from each grid
4.  Indicate that all recipes should be combined with all model specifications

The individual specifications in the final model table will be named using a combination of the name of the recipe and the name of the model specification given in the respective lists with a count added whenever there are duplicate names. For example, arima_spec_grid has four model specifications in it. The ones that are combined with the first ARIMA recipe (named "base_rec") will be named BASE_REC_ARIMA_SPEC1, BASE_REC_ARIMA_SPEC2, BASE_REC_ARIMA_SPEC3, and BASE_REC_ARIMA_SPEC4.

With all of the model-specific workflow sets created, now it's time to put them all together.

```{r}
#| label: bind-workflowsets

hpi_wfset <- bind_rows(arima_wfset, ets_wfset, stlm_wfset, nnet_wfset)

hpi_wfset |> slice(1:5)

```

That's all the heavy lifting. Now it's time to let the computer processors do their part and run the models.

## Fitting the models

```{r}
#| label: run-models
#| output: false
#| cache: true

avail_cores <- unname(parallelly::availableCores() - 1)                     # <1>
modulo_vec <- sapply(avail_cores:2, \(x) nrow(hpi_wfset) %% x)              
num_cores <- max(                                                           # <2>
  max(which(modulo_vec == max(modulo_vec))),
    max(which(modulo_vec == 0))
) + 1

cl <- parallel::makePSOCKcluster(num_cores)                                 # <3>
doParallel::registerDoParallel(cl)                                          # <3>
invisible(                                                                  # <3>
  parallel::clusterCall(cl, function(x) .libPaths(x), .libPaths())          # <3>
)                                                                           # <3>
parallel::clusterEvalQ(cl, set.seed(500))                                   # <4>

hpi_mods_start <- proc.time()

hpi_mods <- hpi_wfset |>                                                    # <5>
  modeltime_fit_workflowset(                                                # <6>
    data = training(econ_splits),                                           # <7>
    control = control_fit_workflowset(verbose = FALSE, allow_par = TRUE)    # <8>
  )

hpi_mods_end <- proc.time()

parallel::stopCluster(cl)                                                   # <9>
rm(cl)
gc()

```

1.  Get the number of cores available and leave one out for common tasks like running the operating system
2.  Calculate the number of cores to use in order to maximize efficiency
3.  Set up the cluster for parallel processing; I copied this code from `modeltime::parallel_start()`
4.  Set the same random seed for each node
5.  Call the `workflowset` object containing all the workflows to run
6.  Use `modeltime::modeltime_fit_workflowset()` to fit all of the models
7.  Use the training portion of the split object to train the models
8.  Set some controls for the model fitting function
9.  Stop the parallel cluster from running

```{r}
#| label: show-fitting-time

fit_time <- (hpi_mods_end - hpi_mods_start) |>
  enframe("time_metric", "seconds") |>
  drop_na(seconds)

fit_time |> gt() |> gt_bold_head()

```

About `r fit_time |> filter(time_metric == "elapsed") |> pull(seconds)` seconds elapsed during model-fitting with parallel processing. Not too bad. Unfortunately, though, some of the models failed (I suppressed error printing). So, before calibrating the models I'm going to drop those from the model output data set. But first I'll investigate the model failures a little.

## Investigating failed models

Here's a look at some of the models that failed.

```{r}
#| label: make-mod-fail-df

mod_fail_df <- hpi_mods |>
  mutate(mod_fail = map_lgl(.model, \(x) is.null(x))) |>
  select(!.model)

mod_fail_df |>
  group_by(mod_fail) |>
  slice_min(.model_id, n = 3) |>
  gt(groupname_col = NULL) |>
  gt_bold_head()

```

Now I want to see the distinct names of the recipes and algorithms (model specifications) that failed.

```{r}
#| label: get-failed-mod-names

failure_spec_df <- mod_fail_df |>
  filter(mod_fail) |>
  separate_wider_delim(
    .model_desc,
    delim = "_",
    names = c("rec_name", "rec", "algo", "spec")
  ) |>
  separate_wider_regex(spec, patterns = c(spec = "SPEC", spec_num = "\\d+"))

failure_spec_df |> distinct(rec_name, algo, spec) |> gt() |> gt_bold_head()

```

Since the failures occurred with only one algorithm they'll be easy to isolate. Now I want to get the numbers of the specifications that failed so that I can pull up the details on those specifications.

```{r}
#| label: get-failed-spec-nums

failures <- failure_spec_df |>
  pull(spec_num) |>
  as.numeric()

ets_failed <- ets_spec_grid |>
  slice(failures) |>
  select(!.models)

ets_failed |>
  count(smooth_level) |>
  gt() |>
  gt_bold_head()

```

Overall there were `r nrow(failure_spec_df)` out of `r nrow(ets_spec_grid)` failed exponential smoothing models. That's not terrible. I'm going to remove these specifications from the trained model data set and move on to model evaluation.

## Model evaluation

To evaluate the models first I need to calibrate them. I'll remove the failed models, calibrate the successful ones, and store accuracy tables here. Below are the 10 best-performing global models, i.e., the ones that performed best on the batched series with all four cities. I defined "best" as the models with the lowest SMAPE (Scaled Mean Average Percent Error) values.

::: {.callout-note collapse="true" title="Breaking up calibration step"}
I broke up the code for calibrating the models below into two separate chunks to keep the file sizes of cached files a bit smaller. I cached the output to make the page easier to render. This step of splitting up the `modeltime_table` for calibration is not typically necessary.
:::

```{r}
#| label: filter-mods

hpi_mods <- hpi_mods |> slice(which(!mod_fail_df$mod_fail))

num_mods <- nrow(hpi_mods)
hpi_mod_idx <- split(1:num_mods, ceiling(seq_along(1:num_mods) / (num_mods / 3)))

```

```{r}
#| label: calibrate-mods1
#| cache: true

hpi_mod_calib1 <- hpi_mods |>
  slice(hpi_mod_idx[[1]]) |>
  modeltime_calibrate(testing(econ_splits), id = "city", quiet = TRUE)

```

```{r}
#| label: calibrate-mods2
#| cache: true

hpi_mod_calib2 <- hpi_mods |>
  slice(hpi_mod_idx[[2]]) |>
  modeltime_calibrate(testing(econ_splits), id = "city", quiet = TRUE)
```

```{r}
#| label: calibrate-mods3
#| cache: true

hpi_mod_calib3 <- hpi_mods |>
  slice(hpi_mod_idx[[3]]) |>
  modeltime_calibrate(testing(econ_splits), id = "city", quiet = TRUE)

```

```{r}
#| label: combine-calib-tbls

hpi_mod_calib <- bind_rows(hpi_mod_calib1, hpi_mod_calib2, hpi_mod_calib3)

```

```{r}
#| label: get-mod-accuracy

hpi_mod_accuracy <- hpi_mod_calib |>
  modeltime_accuracy() |>
  drop_na(.type)

hpi_local_mod_accuracy <- hpi_mod_calib |>
  modeltime_accuracy(acc_by_id = TRUE) |>
  drop_na(.type)

best_global_table <- hpi_mod_accuracy |>
  slice_min(smape, n = 10)

best_local_table <- hpi_local_mod_accuracy |>
  group_by(city) |>
  slice_min(smape, n = 3)

best_global_table |>
  table_modeltime_accuracy(
    .interactive = FALSE,
    .title = "Global Accuracy"
  ) |>
  gt_bold_head() |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_title()
  )

```

Next is a table with the best 3 models calibrated to each city.

```{r}
#| label: show-best-local-mods

best_local_table|>
  table_modeltime_accuracy(
    .interactive = FALSE,
    .title = "Accuracy by City"
  ) |>
  gt_bold_head() |>
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_title()
  ) |>
  tab_style(
    style = list(cell_text(weight = "bold"), cell_fill(color = "lightgray")),
    locations = cells_group()
  ) |>
  tab_style(
    style = cell_borders(
      sides = c("top", "bottom"),
      weight = px(2),
      style = ("solid")
    ),
    locations = cells_group()
  )

```

There's a pretty good mix of models across the two tables including ARIMA, ETS (lots of this one), STLM, and neural-net. Among the ARIMA models there were some with the basic recipe and some with the recipe that included both economic and demographic variables.

Below are plots of forecasts of the test data period for each city using the best 3 global models.

```{r}
#| label: fig-acc

top_3_global <- best_global_table |> slice_min(smape, n = 3) |> pull(.model_id)

hpi_mod_calib |>
  filter(.model_id %in% top_3_global) |>                            
  modeltime_forecast(                            
    new_data = testing(econ_splits),      
    actual_data = econ_data,                     
    conf_by_id = TRUE,                           
    keep_data = TRUE                             
  ) |>
  group_by(city) |>                             
  plot_modeltime_forecast(                      
    .interactive = FALSE,
    .title = "Forecast of Test Data",
    .facet_ncol = 1
  ) +
  theme(plot.title = element_text(hjust = 0.5))

```

Just for fun, here are the 5 worst-performing models (global).

```{r}
#| label: show-worst-mods

hpi_mod_accuracy |>
  slice_max(smape, n = 5, with_ties = FALSE) |>
  gt() |>
  gt_bold_head()

```

## Forecasting with global models

Below are forecasts for each city using the top 3 global models.

```{r}
#| label: forecast-global

hpi_refit <- hpi_mod_calib |>
  filter(.model_id %in% top_3_global) |>
  modeltime_refit(data = econ_data) 

hpi_future <- econ_data |>
  group_by(city) |>
  future_frame(.length_out = 12, .bind_data = FALSE, .date_var = date)

hpi_future_forecast <- hpi_refit |>
  modeltime_forecast(
    new_data = hpi_future,
    actual_data = econ_data,
    conf_by_id = TRUE
  )

hpi_future_forecast |>
  group_by(city) |>
  plot_modeltime_forecast(
    .interactive = FALSE,
    .title = "1 Year Forecast into the Future - Global",
    .facet_ncol = 1
  ) +
  theme(plot.title = element_text(hjust = 0.5))

```

## Choosing models for iterative process

For iterative modeling I'm going to use the 10 best global models and the 3 best local models for each city (there's some overlap in these lists). The specifications that I'm going to use for iterative modeling in the next post are:

```{r}
#| label: store-best-specs

best_global_table |>
  slice_min(smape, n = 5) |>
  bind_rows(best_local_table) |>
  distinct(.model_desc) |>
  gt() |>
  gt_bold_head()

```
