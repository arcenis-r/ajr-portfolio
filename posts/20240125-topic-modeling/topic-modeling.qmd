---
title: "Topic Modeling with R"
author: "Arcenis Rojas"
editor: visual
execute:
  warning: false
  error: false
date: "1/25/2024"
categories:
  - text analysis
  - topic modeling
---

## Introduction

For this project I'll be following an example of performing topic modeling with R using the Tidytext format introduced by Julia Silge and David Robinson in their ["Text Mining with R! A Tidy Approach"](https://www.tidytextmining.com/index.html). And I'll be using abstracts submitted by [Johns Hopkins University](https://www.jhu.edu/) for funding to the [NIH HEAL Initiative](https://heal.nih.gov/). The list of funded projects can be found at <https://heal.nih.gov/funding/awarded>. A benefit of using this data is that it doesn't require much cleaning as Twitter text might. For the purposes of this demonstration I won't do any data cleaning. The objective will be to find the optimal number of topics to which to attribute the abstracts. I'll be using the `tidyverse` and `tidytext` packages for wrangling, the `stm` package for modeling, the `janitor` package for cleaning column names (but I won't load it because I'm only going to use it once), the `knitr` package for table formatting, and the `pacman` package to load my packages (also only used once).

```{r}
#| label: load-packages
pacman::p_load(tidyverse, tidytext, stm, knitr, gt)
```

## Downloading the abstract data

I'll first download the data from the [NIH's Funded Projects page](%22https://heal.nih.gov/funding/awarded/export?combine=johns%20hopkins&_format=csv%22) and filter for John's Hopkins. The code for this is below, though, I will not run it because the data change as proposals are submitted. Instead I'll download the filtered data to my hard drive and the ensuing analysis will be based on the data available as of this writing (January 25, 2024).

```{r}
#| label: declare-data-dir
#| include: false

data_dir <- "./topic-modeling-data"

```

```{r}
#| label: show-how-download
#| eval: false

# Not run
read_csv(
  "https://heal.nih.gov/funding/awarded/export?combine=johns%20hopkins&_format=csv"
) |>
  write_csv(file.path(data_dir, "jh-data.csv"))

```

Now I'll read that data in and just look at the column names to start familiarizing myself with the data

```{r}
#| label: glimpse-data

jh_data <- read_csv(file.path(data_dir, "jh-data.csv"))
glimpse(jh_data)
```

It looks like there are `r nrow(jh_data)` columns in the data. I'm going to clean up the column names and select only the Project \# (to use as an identifier), Project Title, and Summary.

```{r}
#| label: clean-data

jh_clean <- jh_data |>
  janitor::clean_names() |>
  select(project_number, project_title, summary)

head(jh_clean, 3) |> gt::gt() # kable(booktabs = TRUE)
```

## Tidy the data

In this next step I'm going to tidy the data and remove stop words. Then I'll take a look at word frequencies and TF-IDF metrics to get an idea of what I might expect from my model.

I'm first going to use the `tidytext::unnest_tokens()` function to tokenize the data by single word then remove stop words from the text. I'll also get word frequencies in this one step. Also, I'm going to drop the project title for the time being, but I'll bring it back later.

```{r}
#| label: tidy-data

jh_tidy <- jh_clean |>
  mutate(summary = str_c(project_title, summary, sep = " ")) |>
  select(-project_title) |>
  unnest_tokens(word, summary, token = "words") |>
  anti_join(stop_words, by = "word") |>
  count(project_number, word)

jh_tidy |> arrange(-n) |> head(10) |> gt()
```

The words "pain" is one of the most frequently occurring word in documents 1RF1NS113883-01 and 5U54DA049110-04 while the words "pain" and "sleep" seem to be 2 of the most frequent across documents. Let's see how these words do in terms of how important they are in distinguishing a document from other documents.

I'm going to add TF-IDF metrics using the `tidytext::bind_tf_idf()` function.

```{r}
#| label: add-tf-idf

jh_tfidf <- jh_tidy |> bind_tf_idf(word, project_number, n)

jh_tfidf |>
  arrange(-tf_idf) |>
  head(15) |>
  gt()
```

It's very strange that 2 documents have the term "summary" as the only term in the summary. This is indicated by a term frequency (tf) of 1. Let's take a look at the original text for these documents.

```{r}
#| label: check-summaries

jh_clean |>
  filter(
    project_number %in% c("1R61HL156248-01", "3U24TR001609-04S1")
  ) |>
  select(project_number, summary) |>
  gt()
```

There was no summary for either of these documents! The word "summary" had a really high TF-IDF because it was the only word ("no" is a stop word and was dropped) in these documents and it likely isn't used very frequently in other documents. This is one of the benefits of doing this kind of check before going into modeling. Interestingly, though, neither "pain" nor "stress" are in the terms with the top 15 TF-IDF values. I imagine it's because these words occur in a lot of the documents and, therefore, would not be very useful in distinguishing one project from another. So I'll drop the two project with no summaries and go into the modeling.

```{r}
#| label: drop-no_summary

jh_tidy <- jh_tidy |>
  filter(
    !project_number %in% c("1R61HL156248-01", "3U24TR001609-04S1")
  )
```

## Modeling

To be able to perform text mining of any kind, including structural topic modeling, I first need to put my data into a sparse matrix. For this I'll use the `tidytext::cast_sparse()` function.

```{r}
#| label: make-sparse-data

jh_sparse <- jh_tidy |> cast_sparse(project_number, word, n)

dim(jh_sparse)
```

I now have a matrix of `r dim(jh_sparse)[1]` and `r dim(jh_sparse)[2]` columns. There is now one column for each distinct word in my corpus of data. I'm going to create a structural model with 3 topics (number chosen arbitrarily) just to get an idea of how I might evaluate a model. I'm going to use the `stm::stm()` function and use the LDA algorithm to generate my model.

```{r run-lda-3}
#| output: false
#| cache: true

jh_lda_3 <- jh_sparse |>
  stm(K = 3, init.type = "LDA", seed = 212, verbose = FALSE)
```

Now I want to see what some of the most frequently occurring words are within each topic. For this I'm going to simply use the `tidy` method from `stm` on the model. By default, the tidy method extracts values of "beta" which tells us the probability that a word comes from a given topic.

```{r}
#| label: lda_3-top-terms

jh_lda_3 |>
  tidy() |> 
  group_by(topic) |>
  slice_max(beta, n = 10, with_ties = FALSE) |>
  ungroup() |>
  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  labs(y = "Beta", x = NULL) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  theme_bw()
```

The word "pain" is in the top 2 most frequently occurring words in each topic and the word "opioid" occurs in the top 10 words of 2 of the topics. It looks like the model's not doing a great job of separating topics based on this.

Another interesting metric to look at is "gamma" which is the probability that a given document is related to a given topic and this is also provided by our model output, which we can access by specifying the `matrix = "gamma"` argument in our `tidy()` function.

```{r}
#| label: lda_3-topic-probs

jh_lda_3 |>
  tidy(matrix = "gamma") |>
  mutate(topic = str_c("Topic", topic, sep = " ") |> factor()) |>
  ggplot(aes(document, gamma, fill = topic)) +
  geom_col(show.legend = FALSE) +
  scale_x_continuous(breaks = 1:nrow(distinct(jh_tidy, project_number))) +
  facet_wrap(~ topic, scales = "free", ncol = 1) +
  labs(x = "Document", y = "Gamma") +
  theme_bw()
```

It looks like each document except for Document 16 has a high probability of coming from only 1 topic. This is pretty good considering what the beta values showed.

Next I'll see if there's a better number of topics to separate these documents into.

## Choosing a number of topics

First I'm going to build a set of LDA models with values of K (number of topics) ranging from 2 to 25. I'm also going to create a hold-out data set to calculate the heldout-likelihood metric later. This is somewhat akin to using a cross-validation holdout set in other machine learning methodologies.

```{r}
#| label: tune-models
#| output: false
#| cache: true

jh_tune_models <- tibble(k = 2:24) |>
  mutate(
    lda_mod = map(
      k,
      \(x) stm(
        jh_sparse,
        K = x,
        init.type = "LDA",
        seed = 212,
        verbose = FALSE
      )
    )
  )

heldout <- make.heldout(jh_sparse)
```

Now I'm going to extract the metrics that I want to use to evaluate my model. I'm going to focus on the approach of find the value of K with high held-out likelihood and semantic coherence metrics, but I'm also interested in looking at the residual and exclusivity metrics, so I'll keep those as well.

```{r}
#| label: get-tune-results

jh_tune_results <- jh_tune_models |>
  mutate(
    exclusivity = map(lda_mod, exclusivity),
    semantic_coherence = map(
      lda_mod,
      semanticCoherence,
      documents = jh_sparse,
      M = 10
    ),
    eval_heldout = map(lda_mod, eval.heldout, heldout$missing),
    residual = map(lda_mod, checkResiduals, jh_sparse)
  )
```

```{r}
#| label: plot-tune-results

jh_tune_results |>
  transmute(
    k,
    Residuals = map_dbl(residual, "dispersion"),
    `Semantic coherence` = map_dbl(semantic_coherence, mean),
    `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout"),
    Exclusivity = map_dbl(exclusivity, mean)
  ) |>
  pivot_longer(-k, names_to = "metrics", values_to = "value") |>
  ggplot(aes(k, value, color = metrics)) +
  geom_line(show.legend = FALSE) +
  labs(x = "Number of Topics", y = NULL) +
  facet_wrap(~ metrics, scales = "free_y") +
  theme_bw()
```

The highest held-out likelihood score comes from the model with `K = 17` and the highest semantic coherence comes from the model with `K = 20`, so there's a little bit of a trade-off. In this case I'm going to go with parsimony and choose the model with `K = 17`. This model also has relatively high exclusivity. Notably, exclusivity and the residual seem to flatten out and semantic coherence increases dramatically at `K > 10`.

Now I'm going to look at both the `K = 17` and `K = 20` models to see whether one is clearly better. I'm going to start by looking at how easily each model would predict each document to belong to a specific topic. For that, I'm going to look at the maximum values of gamma for each document. For comparison I'm going to show the same plot for the approach of maximizing semantic coherence (`K = 21`).

```{r}
#| label: compare-gamma

jh_tune_models |> 
  filter(k %in% c(17, 20)) |>
  mutate(
    max_gammas = map(
      lda_mod,
      \(x) tidy(x, matrix = "gamma") |>
        group_by(document) |>
        summarise(gamma = max(gamma), .groups = "drop")
    ),
    k_label = str_c("Number of Topics:", k, sep = " ") |> fct_reorder(k)
  ) |>
  select(-lda_mod) |>
  unnest(max_gammas) |>
  ggplot(aes(document, gamma, fill = factor(k))) +
  geom_col(show.legend = FALSE) +
  scale_x_continuous(breaks = 1:24) +
  labs(x = "Document", y = "Maximum Gamma") +
  facet_wrap(~ k_label, ncol = 1) +
  theme_bw()
```

It looks like both models yield relatively high probabilities that each document corresponds to one specific topic across the board with only documents 23 and 24 having maximum gammas below 0.75, but the model with `K = 17` generally performs a bit better.

For the purposes of visualization I'll look at the top 5 words defining the topics for the 17-topic model.

```{r}
#| label: lda_17-top-words

jh_tune_models |> 
  filter(k == 17) |>
  pluck("lda_mod", 1) |>
  tidy() |>
  group_by(topic) |>
  slice_max(beta, n = 5, with_ties = FALSE) |>
  ungroup() |>
  mutate(
    topic = str_c("Topic", topic, sep = " ") |>
      factor(levels = str_c("Topic", 1:17, sep = " "))
  ) |>
  ggplot(aes(reorder_within(term, beta, topic), beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_x_reordered() +
  labs(y = "Beta", x = NULL) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  theme_bw()
```

The word "pain" appears in the top 5 words of 7 topics; in further analysis I might consider making this a stop word along with words like "study" and "research". But topic 11 seems to be about joint issues given the prevalence of the words "osteoarthritis" and "knee"; topic 12 seems to be about demographics with words like "american" and "urban" in the top 5 words; and topic 17 seems to be clearly about digestion issues with words like "pain", "ibs", and "abdominal" in the top 5 words.

Finally, I'll look at the titles grouped by each project's maximum gamma value. You be the judge.

```{r}
#| label: show-final-gamma

jh_tune_models |> 
  filter(k == 17) |>
  pluck("lda_mod", 1) |>
  tidy(matrix = "gamma") |>
  nest(data = -document) |>
  mutate(
    data = map(data,\(x) slice_max(x, gamma, n = 1, with_ties = FALSE)),
    project_number = rownames(jh_sparse)
  ) |>
  unnest(data) |>
  left_join(
    jh_clean |> select(project_number, project_title),
    by = "project_number"
  ) |>
  select(topic, project_title, gamma) |>
  mutate(gamma = round(gamma, 3)) |>
  arrange(topic, gamma) |>
  gt()
```
