---
title: "Forecasting with {modeltime} - Part III"
subtitle: "Global vs. iterative modeling"
author: "Arcenis Rojas"
date: "2/16/2024"
bibliography: references.bib
fig-height: 6
fig-width: 8
categories:
  - Time Series
  - Forecasting
  - Machine Learning
---

In my previous post on this topic – [Forecasting with {modeltime} - Part II](../20240215-forecasting-with-modeltime-part-2/forecasting-with-modeltime-part-2.qmd) – I performed a basic analysis of the Case-Shiller HPI time series for four U.S. cities (Dallas, Detroit, NYC, and San Diego). In this post I'm going to use some of the conclusions from that analysis to build ARIMA models using both a "global" modeling process and an "iterative" modeling process using the [modeltime](https://business-science.github.io/modeltime/index.html) [@modeltime] and [tidymodels](https://www.tidymodels.org/) [@tidymodels] frameworks.

One of the distinguishing features of `modeltime` among other time series modeling frameworks is that it works very well with `tidymodels`. In fact, like `tidymodels` , `modeltime` serves as a platform for building consistent workflows using algorithms from other packages. `modeltime` builds on top of `tidymodels` to create a framework for dealing with the specific challenges of time series modelling and putting it all in a convenient, consistent API.

I'll also be using [tidyverse](https://www.tidyverse.org/) [@tidyverse] and [gt](https://gt.rstudio.com/) [@gt], per usual, and [timetk](https://business-science.github.io/timetk/) [@timetk]. `modeltime` and `timetk` are developed and maintained by the good people at [Business Science](https://www.business-science.io/) and `tidyverse`, `tidymodels`, and `gt` are developed and maintained by the generous folks at [Posit](https://posit.co/).

## Time Series Analysis Review

In the aforementioned time series analysis I found that the time series data for all four cities had a strong trend and a strong seasonal component. The trend was mostly muted by using a lag of 1 and the seasonal component of each series was generally muted by differencing twelve times, which makes sense given that the data are monthly (annual seasonal cycle). I was able to use these methods to greatly reduce the influence of non-stationary components according to the Augmented Dickey-Fuller test. The ACF and PACF plots led me to conclude to use an AR(3) and MA(5) ARIMA model. To deal with the seasonality I differenced the data twelve times. This will translate to a seasonal differencing component of (1) in the ARIMA model. In the previous post I wrote that I would be performing ARIMA rather than SARIMA... things change.

```{r}
#| label: load-packages

library(tidyverse)
library(timetk)
library(modeltime)
library(tidymodels)
library(gt)
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("lag", "dplyr")
tidymodels_prefer()

```

```{r}
#| label: import-data
#| include: false

econ_data <- read_rds("../_common-resources/econ-data.Rds")  |> 
  select(city, date, hpi)

source("../_common-resources/gt-bold-head.R")

theme_timetk <- read_rds("../_common-resources/theme-timetk.Rds") 

series_colors <- c("#2c3e50", "red")

```

## Differences between global and iterative modeling

Conventionally, time series models work by regressing the outcome – the most recent or future observation(s) – on previous observations of itself. We can refer to this as a "local" model because the variance in the data corresponds to only the single time series. A global model, on the other hand, pools the data to get a global variance and uses that information to make forecasts.

The primary advantages of a global model are speed and scalability. A second advantage is that each, individual forecasting model can "learn" from the history of the other time series that it's pooled with. The disadvantage is that each individual model will likely be less accurate than if it was fit on an individual basis.

The advantage of modeling time series locally and iteratively is that this process most likely yields the highest quality fit and accuracy for each, individual model. The disadvantage is that as the number of models grows, so do the computational burden and processing time.

In two sections below I'll demonstrate both processes with an ARIMA model using the parameters mentioned above and in the last section I'll compare the results from both.

## Building a global ARIMA model

As mentioned above, `modeltime` follows the Tidymodels convention. As such, it starts with partitioning data into training and testing data sets, continues with writing a recipe and model specification (and combining them in a `workflow` object), follows with fitting the model, and continues with model evaluation and selection.

::: callout-tip
For more information on the Tidymodels framework, please check out <https://www.tmwr.org/>.
:::

### Splitting data

For a global modeling process one can use the timetk::time_series_split() function which will create a list containing the data set, one vector of training data IDs and one of testing data IDs, and a 1-column data set containing all IDs.

The time_series_split() function uses 5 initial cross-validation periods by default with each one being progressively longer using a specified period of time as the assessment period. This assessment period is specified by the asses argument.

```{r}
#| label: splits-global

econ_splits_global <- econ_data |>
  time_series_split(
    assess = "2 year",
    cumulative = TRUE,
    date_var = date   
  )

str(econ_splits_global)

```

### Specifying a modeling workflow and fit the model

To specify the workflow I'll first write the recipe...

::: {.callout-caution title="Lagging/Differencing as a recipe step" collapse="true"}
One might be tempted to add lags and differences in the recipe, but this leads to a mess that's really difficult to get out of. It all starts with the fact that a lagged and/or differenced variable will have a different name and will create missing values. I found that it's best to handle these operations in the model specification.
:::

```{r}
#| label: rec-global

arima_rec_global <- recipe(hpi ~ date, data = training(econ_splits_global))

arima_rec_global |> summary() |> gt() |> gt_bold_head()

```

... then the model object.

The model specification below will be used for both processes.

```{r}
#| label: mod-spec

arima_spec <- arima_reg(         # <1>
  non_seasonal_ar = 3,           # <2>
  non_seasonal_ma = 5,           # <3>
  non_seasonal_differences = 1,  # <4>
  seasonal_differences = 1,      # <5>
  seasonal_period = 12           # <6>
) |>
  set_engine("arima")            # <7>

arima_spec

```

1.  Call the API for building ARIMA models
2.  Set the AR order (*p* in conventional notation)
3.  Set the MA order (*q* in conventional notation)
4.  Set the degree of non-seasonal differencing (*d* in conventional notation)
5.  Set the degree of seasonal differencing (*D* in conventional notation)
6.  Set the length of the seasonal period
7.  Set the modeling engine to `stats::arima()`

Now that those are complete I can put them together in a `workflow` object.

```{r}
#| label: workflow-global

arima_wflow_global <- workflow() |>   # <1>
  add_model(arima_spec) |>            # <2>
  add_recipe(arima_rec_global)        # <3>

arima_wflow_global

```

1.  Create a container for a `workflow` object
2.  Add the above model specification
3.  Add the recipe

The final step here is to fit the model

```{r}
#| label: fit-global
#| cache: true

global_fit_start <- proc.time()

arima_fit_global <- arima_wflow_global |>
  fit(training(econ_splits_global))

global_fit_end <- proc.time()

arima_fit_global

```

### Model evaluation

To deal with some of the characteristics specific to time series modeling, the `modeltime` framework uses conventions like the `mdl_time_tbl` (modeltime table) class to store a variety of elements of a time series model including accuracy metrics. This class is also used in model calibration and refitting for future forecasting. Here I'll write the modeltime table for the global process.

I'm showing the model time table using a `print()` method rather than as a `gt` table because the list in the second column (list) would render in a cumbersome way.

```{r}
#| label: mtt-global

arima_mtt_global <- modeltime_table(arima_fit_global)

arima_mtt_global

```

This table contains the model and some additional information about it. I'll now use this table to calibrate the global model to the individual time series with

```{r}
#| label: calibrate-global
#| cache: true

arima_calib_global <- arima_mtt_global |>
  modeltime_calibrate(new_data = testing(econ_splits_global), id = "city")

arima_calib_global

```

With the calibration done I can now look at the accuracy of the model both at a global level and at an individual model level.

Getting the accuracy is done by the same function: `modeltime_accuracy()`. Whether it outputs global or local model accuracy is determined by the `acc_by_id` argument.

#### Global model accuracy

```{r}
#| label: tbl-acc-global

arima_calib_global |>
  modeltime_accuracy(acc_by_id = FALSE) |>  
  table_modeltime_accuracy(.interactive = FALSE)

```

#### Local model accuracy

```{r}
#| label: tbl-acc-global-local
#| tbl-cap: "global model accuracy table"

arima_calib_global |>
  modeltime_accuracy(acc_by_id = TRUE) |>
  table_modeltime_accuracy(.interactive = FALSE)

```

For comparison you can jump ahead to see the [iterative model accuracy table @tbl-acc-iterative].

#### Accuracy plot

Below is a visual representation of how the models performed against the test data (the last 2 years).

```{r}
#| label: fig-acc-global
#| fig-cap: "global model accuracy plot"

arima_calib_global |>                            # <1>
  modeltime_forecast(                            # <2>
    new_data = testing(econ_splits_global),      # <3>
    actual_data = econ_data,                     # <4>
    conf_by_id = TRUE,                           # <5>
    keep_data = TRUE                             # <6>
  ) |>
  group_by(city) |>                              # <7>
  plot_modeltime_forecast(                       # <8>
    .interactive = FALSE,
    .title = "Forecast of Test Data - Global",
    .facet_ncol = 2
  ) +
  theme(plot.title = element_text(hjust = 0.5))

```

1.  Call up the object containing the calibrated models
2.  Call the `modeltime_forecast()` function
3.  Specify the new data to test forecasts against; in this case it's the testing split from the object created by `time_series_split()`
4.  Specify what to use as the actual data; in this case it's the original `econ_data` object
5.  Specify whether to generate confidence intervals by individual data series
6.  Specify whether to keep the columns from the actual data; I specified `TRUE` to keep the "city" column
7.  Group by city to generate the forecasts by city
8.  Plot the forecasts as a stationary plot

For comparison you can jump ahead to see the [iterative model accuracy plot @fig-acc-iterative].

### Forecasting the future

The first step in getting forecasts for the future (with respect to the available data) is to refit the model using all of the training data after calibrating the model. That is done with the `modeltime_refit()` function for a global process.

```{r}
#| label: refit-global
#| cache: true

global_refit_start <- proc.time()

arima_refit_global <- modeltime_refit(arima_calib_global, data = econ_data) 

global_refit_end <- proc.time()

arima_refit_global

```

The next step is to build a container for the data that has all the correct dates and ID columns. This is done with `timetk::future_frame()` .

::: {.callout-caution title="future_frame() with multiple series" collapse="true"}
`future_frame()` only generates one series per data set fed to it, so it's really important to group the data before executing the function if you have multiple series or you'll only get one series of future dates.
:::

```{r}
#| label: future-reframe-global
arima_future_global <- econ_data |>
  group_by(city) |>
  future_frame(.length_out = 12, .bind_data = FALSE, .date_var = date)

```

And finally one can generate a plot of the forecasts with `modeltime_forecast()` and plot the time series with `plot_modeltime_forecast()` . Better not forget to group by series!

```{r}
#| label: fig-forecast-future-global

forecast_future_global <- arima_refit_global |>
  modeltime_forecast(
    new_data = arima_future_global,
    actual_data = econ_data,
    conf_by_id = TRUE
  )

forecast_future_global |>
  group_by(city) |>
  plot_modeltime_forecast(
    .interactive = FALSE,
    .title = "1 Year Forecast into the Future - Global",
    .facet_ncol = 2
  ) +
  theme(plot.title = element_text(hjust = 0.5))

```

## Building an iterative ARIMA model

Now I'll go through an iterative modeling workflow with the same model parameters for comparison. A key difference in the workflow is that the iterative model requires the construction of a nested table.

In `modeltime` iterative forecasting is called ["Nested Forecasting"](https://business-science.github.io/modeltime/articles/nested-forecasting.html?q=nested#nested-forecasting).

### Splitting data

One of the big differences in building a nested model from building a global model in `modeltime` happens right at the beginning with splitting the data. With this method the future dates are added right up front with `extend_timeseries()` , the length of actual data is distinguished from that of future periods in `nest_timeseries()` and the train/test split is created with `split_nested_timeseries()` .

```{r}
#| label: splits-iterative

econ_splits_iterative <- econ_data |>
  extend_timeseries(
    .id_var = city,
    .date_var = date,
    .length_future = 12
  ) |>
  nest_timeseries(
    .id_var = city,
    .length_future = 12,
    .length_actual = 17 * 12
  ) |>
  split_nested_timeseries(.length_test = 24)

econ_splits_iterative

```

### Specifying a modeling workflow and fit the model

I'll use the same model specification as above, but the recipe has to be specified to use the new splits object.

```{r}
#| label: rec-iterative 

arima_rec_iterative <- recipe(
  hpi ~ date,
  extract_nested_train_split(econ_splits_iterative)
)

arima_rec_iterative |> summary() |> gt() |> gt_bold_head()

```

I'll also create a new workflow object that just updates the previous one with the new recipe.

```{r}
#| label: workflow-iterative

arima_wflow_iterative <- arima_wflow_global |>
  update_recipe(arima_rec_iterative)

arima_wflow_iterative

```

And now I'll fit the model to all four cities iteratively using `modeltime_nested_fit()` .

```{r}
#| label: fit-iterative
#| cache: true

iterative_fit_start <- proc.time()

arima_fit_iterative <- econ_splits_iterative |>
  modeltime_nested_fit(arima_wflow_iterative)

iterative_fit_end <- proc.time()

arima_fit_iterative

```

### Model evaluation

#### Local model accuracy

```{r}
#| label: tbl-acc-iterative
#| tbl-cap: "iterative model accuracy table"

arima_fit_iterative |>
  extract_nested_test_accuracy() |>
  table_modeltime_accuracy(.interactive = FALSE)

```

You can jump back to compare this with the [global model accuracy table @tbl-acc-global-local].

#### Accuracy plot

Below is a visual representation of how the models from the iterative modeling process performed against the test data (the last 2 years).

```{r}
#| label: fig-acc-iterative
#| fig-cap: "iterative model accuracy plot"

arima_fit_iterative |>
  extract_nested_test_forecast() |>
  group_by(city) |>
  plot_modeltime_forecast(
    .interactive = FALSE,
    .title = "Forecast of Test Data - Iterative",
    .facet_ncol = 2
  ) +
  theme(plot.title = element_text(hjust = 0.5))

```

You can jump back to compare this with the [global model accuracy plot @fig-acc-global].

### Forecasting the future

As with the global process above, the first step here will be to refit the model to the full training data set. Note that the iterative model process requires the use of `modeltime_nested_refit()` rather than `modeltime_refit()` .

::: {.callout-caution title="control_* functions" collapse="true"}
`modeltime` has separate `control_*` functions for different purposes such as this case in which `modeltime_refit()` has `control_refit()` and `modeltime_nested_refit()` has `control_nested_refit()`. Make sure to use the correct one.
:::

```{r}
#| label: refit-iterative
#| cache: true

iterative_refit_start <- proc.time()

arima_refit_iterative <- arima_fit_iterative |>
  modeltime_nested_refit(control = control_nested_refit(verbose = FALSE))

iterative_refit_end <- proc.time()

arima_refit_iterative

```

Since the future data container was already created in the splitting object, I can go right to forecasting the future here.

```{r}
#| label: fig-forecast-future-iterative 

forecast_future_iterative <- arima_refit_iterative |>
  extract_nested_future_forecast()

forecast_future_iterative |>
  group_by(city) |>
  plot_modeltime_forecast(
    .interactive = FALSE,
    .title = "1 Year Forecast into the Future - Global",
    .facet_ncol = 2
  ) +
  theme(plot.title = element_text(hjust = 0.5))

```

## Comparison

The two processes follow the same workflow, but have some key operational differences:

-   Different structures for the data split objects

-   The iterative workflow does not require calibration to the individual time series

-   Processing time

Below are two tables showing the processing times for fitting and refitting the global and iterative models.

```{r}
#| label: fig-fit-proc-times
#| fig-width: 6
#| fig-height: 4

global_fit_time <- (global_fit_end - global_fit_start) |>
  enframe(name = "metric") |>
  mutate(value = as.numeric(value), model_process = "Global Fit") |>
  drop_na()

iterative_fit_time <- (iterative_fit_end - iterative_fit_start) |>
  enframe(name = "metric") |>
  mutate(value = as.numeric(value), model_process = "Iterative Fit") |>
  drop_na()

bind_rows(global_fit_time, iterative_fit_time) |>
  ggplot(
    aes(x = metric, y = value, fill = model_process, group = model_process)
  ) +
  geom_col(position = "dodge") +
  labs(
    fill = NULL,
    x = NULL,
    y = "Time in seconds",
    title = "Comparison of Model Fit Processing Times"
  ) +
  scale_fill_manual(values = c("red", "blue")) +
  theme_timetk +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r}
#| label: fig-refit-proc-times
#| fig-width: 6
#| fig-height: 4

global_refit_time <- (global_refit_end - global_refit_start) |>
  enframe(name = "metric") |>
  mutate(value = as.numeric(value), model_process = "Global Refit") |>
  drop_na()

iterative_refit_time <- (iterative_refit_end - iterative_refit_start) |>
  enframe(name = "metric") |>
  mutate(value = as.numeric(value), model_process = "Iterative Refit") |>
  drop_na()

bind_rows(global_refit_time, iterative_refit_time) |>
  ggplot(
    aes(x = metric, y = value, fill = model_process, group = model_process)
  ) +
  geom_col(position = "dodge") +
  labs(
    fill = NULL,
    x = NULL,
    y = "Time in seconds",
    title = "Model Refit Processing Times"
  ) +
  scale_fill_manual(values = c("red", "blue")) +
  theme_timetk +
  theme(plot.title = element_text(hjust = 0.5))

```

Besides these operational differences, the processes yield different levels of accuracy due to differences in the data on which they are trained (or to which they are initially fit). It's also clear that the differences in models can yield some very big differences in forecasts. Global models have performed well in competitions that involved thousands of time series, so it's possible that the variance across the four time series in this analysis is too high to make global modeling worthwhile.

```{r}
#| label: show-both-forecasts
#| fig-width: 12
#| fig-height: 8

patchwork::wrap_plots(
  forecast_future_global |>
  group_by(city) |>
  plot_modeltime_forecast(
    .interactive = FALSE,
    .title = "1 Year Forecast into the Future - Global",
    .facet_ncol = 1
  ) +
  theme(plot.title = element_text(hjust = 0.5)),
  forecast_future_iterative |>
  group_by(city) |>
  plot_modeltime_forecast(
    .interactive = FALSE,
    .title = "1 Year Forecast into the Future - Iterative",
    .facet_ncol = 1
  ) +
  theme(plot.title = element_text(hjust = 0.5))
)

```

In the next post I'll run a global modeling process with a few different algorithms and specifications to get a few hundred models and find which ones run best.
